{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lB8F7bDO4azW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9239,
     "status": "ok",
     "timestamp": 1680817078775,
     "user": {
      "displayName": "Omar Al Akkad",
      "userId": "09097421262007916771"
     },
     "user_tz": 300
    },
    "id": "j3gEi7YUKDve"
   },
   "outputs": [],
   "source": [
    "#@title (Run) Part 1: Define required functions for Data processing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import Tensor\n",
    "import os\n",
    "import argparse\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]\n",
    "\n",
    "def balance_val_split(dataset, train_size=12500):\n",
    "\n",
    "    try:\n",
    "        targets = np.array(dataset.targets)\n",
    "    except:\n",
    "        targets = []  # create an empty list to store the targets\n",
    "        for data in dataset.datasets:\n",
    "            targets += data.targets  # concatenate the targets from each dataset into the list\n",
    "        targets = np.array(targets)\n",
    "    #targets = np.array(dataset.datasets.targets)\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(targets.shape[0]),\n",
    "        train_size=train_size,\n",
    "        stratify=targets\n",
    "    )\n",
    "    train_dataset = Subset(dataset, indices=train_indices)\n",
    "    # Get the data from the subset dataset\n",
    "    subset_data = [train_dataset[idx][0] for idx in range(len(train_dataset))]\n",
    "    subset_labels = [train_dataset[idx][1] for idx in range(len(train_dataset))]\n",
    "    # Create a dataset from the list of data and targets\n",
    "    train_dataset = MyDataset(subset_data, subset_labels)\n",
    "    \n",
    "    \n",
    "    val_dataset = Subset(dataset, indices=val_indices)\n",
    "    # Get the data from the subset dataset\n",
    "    subset_data = [val_dataset[idx][0] for idx in range(len(val_dataset))]\n",
    "    subset_labels = [val_dataset[idx][1] for idx in range(len(val_dataset))]\n",
    "    # Create a dataset from the list of data and targets\n",
    "    val_dataset = MyDataset(subset_data, subset_labels)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def count_label_frequency(target_train_dataset):\n",
    "\tfrom collections import Counter\n",
    "\ttarget_labels = []  # create an empty list to store the labels\n",
    "\n",
    "\tfor i in range(len(target_train_dataset)):\n",
    "\t\t\t_, label = target_train_dataset[i]  # extract the label for the i-th example in the subset\n",
    "\t\t\ttarget_labels.append(label)  # append the label to the 'subset_labels' list\n",
    "\n",
    "\n",
    "\treturn Counter(target_labels)\n",
    " \n",
    "\n",
    "\n",
    "def custom_transform(image: Tensor) -> Tensor:\n",
    "    import random\n",
    "    # randomly flip horizontally or vertically with 25% chance\n",
    "    if random.random() < 0.25:\n",
    "        image = RandomHorizontalFlip(p=1)(image)\n",
    "    elif random.random() < 0.5:\n",
    "        image = RandomVerticalFlip(p=1)(image)\n",
    "    \n",
    "    # randomly shift the image by 2 pixels to the left or right with 25% chance\n",
    "    if random.random() < 0.25:\n",
    "        image = RandomCrop((image.shape[-2], image.shape[-1] - 2), pad_if_needed=False)(image)\n",
    "    elif random.random() < 0.5:\n",
    "        image = RandomCrop((image.shape[-2], image.shape[-1] + 2), pad_if_needed=False)(image)\n",
    "        \n",
    "    # randomly shift the image by 2 pixels to the top or bottom with 25% chance\n",
    "    if random.random() < 0.25:\n",
    "        image = RandomCrop((image.shape[-2] - 2, image.shape[-1]), pad_if_needed=False)(image)\n",
    "    elif random.random() < 0.5:\n",
    "        image = RandomCrop((image.shape[-2] + 2, image.shape[-1]), pad_if_needed=False)(image)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1680817078989,
     "user": {
      "displayName": "Omar Al Akkad",
      "userId": "09097421262007916771"
     },
     "user_tz": 300
    },
    "id": "ypWLMvO54a10"
   },
   "outputs": [],
   "source": [
    "#@title (Run) Part 2: Define required functions for Data Training\n",
    "\n",
    "# Training\n",
    "def train(trainloader, epoch, batch_size=128, logfile = \"train.summary\"):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        #inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        if inputs.shape[0] != batch_size:\n",
    "          print(inputs.shape)\n",
    "          continue\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if batch_idx % 30 == 0:\n",
    "                print(batch_idx, len(trainloader), 'Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    print(len(trainloader), 'Epoch: %d | Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                     % (epoch, train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    f = open(logfile, \"a\")\n",
    "    f.write('Epoch: %d | Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)\\n'\n",
    "                     % (epoch, train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    f.close()\n",
    "\n",
    "def test(testloader, epoch, batch_size=128, logfile = \"train.summary\", save_modelpath = './DLA'):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            #inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            if batch_idx % 30 == 0:\n",
    "                print(batch_idx, len(testloader), 'Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    print(len(testloader), 'Epoch: %d | Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
    "                         % (epoch, test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    f = open(logfile, \"a\")\n",
    "    f.write('Epoch: %d | Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)\\n'\n",
    "                         % (epoch, test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    f.close()\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir(save_modelpath):\n",
    "            os.mkdir(save_modelpath)\n",
    "        torch.save(state, save_modelpath+'/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "def draw_training_summary(filepath = 'target_train_DCA-BiLSTM.summary'):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        results_summary = f.read()\n",
    "\n",
    "    train_epoch = []\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_epoch = []\n",
    "    test_loss=[]\n",
    "    test_acc=[]\n",
    "    for line in results_summary.split(\"\\n\"):\n",
    "        try:\n",
    "            r_epoch = line.split('|')[0].strip().split(' ')[1]\n",
    "            r_loss = line.split('|')[1].strip().split(' ')[2].replace('%','')\n",
    "            r_acc = line.split('|')[2].strip().split(' ')[2].replace('%','')\n",
    "            if 'Train' in line:\n",
    "                train_epoch.append(int(r_epoch))\n",
    "                train_loss.append(float(r_loss))\n",
    "                train_acc.append(float(r_acc))\n",
    "            if 'Test' in line:\n",
    "                test_epoch.append(int(r_epoch))\n",
    "                test_loss.append(float(r_loss))\n",
    "                test_acc.append(float(r_acc))\n",
    "        except:\n",
    "            print(line)\n",
    "\n",
    "    # Create a new figure and plot the data\n",
    "    plt.figure(figsize=(12,4))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_acc, label='Train')\n",
    "    plt.plot(test_acc, label='Test')\n",
    "    plt.axhline(y=np.max(test_acc), color='r', linestyle='--')\n",
    "    # Add text for the horizontal line\n",
    "    plt.text(test_epoch[-10], np.max(test_acc)*1.05, np.max(test_acc), color='r', fontsize=10)\n",
    "    # Customize the plot\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_loss, label='Train')\n",
    "    plt.plot(test_loss, label='Test')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1680817078989,
     "user": {
      "displayName": "Omar Al Akkad",
      "userId": "09097421262007916771"
     },
     "user_tz": 300
    },
    "id": "YxpJfRc4-FvE"
   },
   "outputs": [],
   "source": [
    "#@title (Run) Part 3: Prepare Cifar10 dataset for target and shadow model\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "def create_cifar_dataset_torch(batch_size=128, target_train_size = 15000, target_test_size= 15000, shadow_train_size = 15000, shadow_test_size= 15000):\n",
    "\n",
    "  # Data\n",
    "  print('==> Preparing data..')\n",
    "\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "      transforms.ToTensor()\n",
    "  ])\n",
    "  \n",
    "  cifar_trainset = torchvision.datasets.CIFAR10(\n",
    "      root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "  cifar_testset = torchvision.datasets.CIFAR10(\n",
    "      root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "  cifar_dataset = torch.utils.data.ConcatDataset([cifar_trainset, cifar_testset])\n",
    "\n",
    "\n",
    "  #target_train_size = int(0.25 * len(cifar_dataset)) # 15000\n",
    "  remain_size = len(cifar_dataset) - target_train_size\n",
    "  target_train_dataset, remain_dataset = torch.utils.data.random_split(cifar_dataset, [target_train_size, remain_size])\n",
    "\n",
    "  #target_test_size = int(0.25 * len(cifar_dataset)) # 15000\n",
    "  remain_size = len(remain_dataset) - target_test_size\n",
    "  target_test_dataset, remain_dataset = torch.utils.data.random_split(remain_dataset, [target_test_size, remain_size])\n",
    "\n",
    "  #target_test_dataset, remain_dataset = balance_val_split(remain_dataset, train_size=target_test_size)\n",
    "\n",
    "\n",
    "  #shadow_train_size = int(0.25 * len(cifar_dataset)) # 15000\n",
    "  remain_size = len(remain_dataset) - shadow_train_size\n",
    "  shadow_train_dataset, shadow_test_dataset = torch.utils.data.random_split(remain_dataset, [shadow_train_size, remain_size])\n",
    "  #shadow_train_dataset, shadow_test_dataset = balance_val_split(remain_dataset, train_size=shadow_train_size)\n",
    "\n",
    "  print(\"Setting target_train_dataset size to \",len(target_train_dataset), count_label_frequency(target_train_dataset))\n",
    "  print(\"Setting target_test_dataset size to \",len(target_test_dataset), count_label_frequency(target_test_dataset))\n",
    "  print(\"Setting shadow_train_dataset size to \",len(shadow_train_dataset), count_label_frequency(shadow_train_dataset))\n",
    "  print(\"Setting shadow_test_dataset size to \",len(shadow_test_dataset), count_label_frequency(shadow_test_dataset))\n",
    "  #print(\"Setting testset size to \",len(testset))\n",
    "\n",
    "\n",
    "\n",
    "  '''\n",
    "  transform_train = transforms.Compose([\n",
    "      transforms.RandomCrop(32, padding=4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "  ])\n",
    "  '''\n",
    "\n",
    "\n",
    "\n",
    "  transform_train = transforms.Compose([\n",
    "      transforms.RandomCrop(32, padding=4),\n",
    "      custom_transform,\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "  ])\n",
    "\n",
    "  transform_test = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "  ])\n",
    "\n",
    "  # apply the data augmentation transformations to the subset\n",
    "  target_train_dataset.dataset.transform = transform_train\n",
    "  # Load the transformed subset using a DataLoader\n",
    "  target_trainloader = DataLoader(target_train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "\n",
    "  target_test_dataset.dataset.transform = transform_test\n",
    "  # Load the transformed subset using a DataLoader\n",
    "  target_testloader = DataLoader(target_test_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "\n",
    "  # apply the data augmentation transformations to the subset\n",
    "  shadow_train_dataset.dataset.transform = transform_train\n",
    "  # Load the transformed subset using a DataLoader\n",
    "  shadow_trainloader = DataLoader(shadow_train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "\n",
    "  shadow_test_dataset.dataset.transform = transform_test\n",
    "  # Load the transformed subset using a DataLoader\n",
    "  shadow_testloader = DataLoader(shadow_test_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "  return target_trainloader, target_testloader, shadow_train_dataset, shadow_testloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdCuGYMA8LQq"
   },
   "source": [
    "## Part 4: Define deep learning methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1714,
     "status": "ok",
     "timestamp": 1680817080701,
     "user": {
      "displayName": "Omar Al Akkad",
      "userId": "09097421262007916771"
     },
     "user_tz": 300
    },
    "id": "f8-5klg13qch"
   },
   "outputs": [],
   "source": [
    "#@title (Run) Part 4.1: Define required functions for ReNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#on hopper: pip install --upgrade torch==2.0.0+cu118 torchvision==0.15.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "\n",
    "\n",
    "# renet with one layer\n",
    "class ReNet(nn.Module):\n",
    "\tdef __init__(self, receptive_filter_size, hidden_size, batch_size, image_patches_height, image_patches_width):\n",
    "\n",
    "\t\tsuper(ReNet, self).__init__()\n",
    "\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.receptive_filter_size = receptive_filter_size\n",
    "\t\tself.input_size1 = receptive_filter_size * receptive_filter_size * 3\n",
    "\t\tself.input_size2 = hidden_size * 2\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\n",
    "\t\t# vertical rnns\n",
    "\t\tself.rnn1 = nn.LSTM(self.input_size1, self.hidden_size, dropout = 0.2)\n",
    "\t\tself.rnn2 = nn.LSTM(self.input_size1, self.hidden_size, dropout = 0.2)\n",
    "\n",
    "\t\t# horizontal rnns\n",
    "\t\tself.rnn3 = nn.LSTM(self.input_size2, self.hidden_size, dropout = 0.2)\n",
    "\t\tself.rnn4 = nn.LSTM(self.input_size2, self.hidden_size, dropout = 0.2)\n",
    "\n",
    "\t\tself.initHidden()\n",
    "\n",
    "\t\tfeature_map_dim = int(image_patches_height*image_patches_height*hidden_size*2)\n",
    "\t\t#self.conv1 = nn.Conv2d(hidden_size*2, 2, 3,padding=1)#[1,640,8,8]->[1,1,8,8]\n",
    "\t\t#self.UpsamplingBilinear2d=nn.UpsamplingBilinear2d(size=(32,32), scale_factor=None)\n",
    "\t\tself.dense = nn.Linear(feature_map_dim, 1024) #4096\n",
    "\t\tself.fc = nn.Linear(1024, 10)\n",
    "\n",
    "\t\tself.log_softmax = nn.LogSoftmax()\n",
    "\n",
    "\tdef initHidden(self):\n",
    "\t\tself.hidden = (Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()), Variable(torch.zeros(1, self.batch_size, self.hidden_size)).cuda())\n",
    "\n",
    "\n",
    "\tdef get_image_patches(self, X, receptive_filter_size):\n",
    "\t\t\"\"\"\n",
    "\t\tcreates image patches based on the dimension of a receptive filter\n",
    "\t\t\"\"\"\n",
    "\t\timage_patches = []\n",
    "\n",
    "\t\t_, X_channel, X_height, X_width= X.size()\n",
    "\n",
    "\n",
    "\t\tfor i in range(0, X_height, receptive_filter_size):\n",
    "\t\t\tfor j in range(0, X_width, receptive_filter_size):\n",
    "\t\t\t\tX_patch = X[:, :, i: i + receptive_filter_size, j : j + receptive_filter_size]\n",
    "\t\t\t\timage_patches.append(X_patch)\n",
    "\n",
    "\t\timage_patches_height = (X_height // receptive_filter_size)\n",
    "\t\timage_patches_width = (X_width // receptive_filter_size)\n",
    "\n",
    "\n",
    "\t\timage_patches = torch.stack(image_patches)\n",
    "\t\timage_patches = image_patches.permute(1, 0, 2, 3, 4)\n",
    "\t\t#print(\"image_patches: \",image_patches.shape)\n",
    "\n",
    "\t\timage_patches = image_patches.contiguous().view(-1, image_patches_height, image_patches_height, receptive_filter_size * receptive_filter_size * X_channel)\n",
    "\n",
    "\t\treturn image_patches\n",
    "\n",
    "\n",
    "\n",
    "\tdef get_vertical_rnn_inputs(self, image_patches, forward):\n",
    "\t\t\"\"\"\n",
    "\t\tcreates vertical rnn inputs in dimensions \n",
    "\t\t(num_patches, batch_size, rnn_input_feature_dim)\n",
    "\t\tnum_patches: image_patches_height * image_patches_width\n",
    "\t\t\"\"\"\n",
    "\t\tvertical_rnn_inputs = []\n",
    "\t\t_, image_patches_height, image_patches_width, feature_dim = image_patches.size()\n",
    "\n",
    "\t\tif forward:\n",
    "\t\t\tfor i in range(image_patches_height):\n",
    "\t\t\t\tfor j in range(image_patches_width):\n",
    "\t\t\t\t\tvertical_rnn_inputs.append(image_patches[:, j, i, :])\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tfor i in range(image_patches_height-1, -1, -1):\n",
    "\t\t\t\tfor j in range(image_patches_width-1, -1, -1):\n",
    "\t\t\t\t\tvertical_rnn_inputs.append(image_patches[:, j, i, :])\n",
    "\n",
    "\t\tvertical_rnn_inputs = torch.stack(vertical_rnn_inputs)\n",
    "\n",
    "\n",
    "\t\treturn vertical_rnn_inputs\n",
    "\n",
    "\n",
    "\n",
    "\tdef get_horizontal_rnn_inputs(self, vertical_feature_map, image_patches_height, image_patches_width, forward):\n",
    "\t\t\"\"\"\n",
    "\t\tcreates vertical rnn inputs in dimensions \n",
    "\t\t(num_patches, batch_size, rnn_input_feature_dim)\n",
    "\t\tnum_patches: image_patches_height * image_patches_width\n",
    "\t\t\"\"\"\n",
    "\t\thorizontal_rnn_inputs = []\n",
    "\n",
    "\t\tif forward:\n",
    "\t\t\tfor i in range(image_patches_height):\n",
    "\t\t\t\tfor j in range(image_patches_width):\n",
    "\t\t\t\t\thorizontal_rnn_inputs.append(vertical_feature_map[:, i, j, :])\n",
    "\t\telse:\n",
    "\t\t\tfor i in range(image_patches_height-1, -1, -1):\n",
    "\t\t\t\tfor j in range(image_patches_width -1, -1, -1):\n",
    "\t\t\t\t\thorizontal_rnn_inputs.append(vertical_feature_map[:, i, j, :])\n",
    "\t\t\n",
    "\t\thorizontal_rnn_inputs = torch.stack(horizontal_rnn_inputs)\n",
    "\n",
    "\t\treturn horizontal_rnn_inputs\n",
    "\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\n",
    "\t\t\"\"\"ReNet \"\"\"\n",
    "\n",
    "\t\t# divide input input image to image patches\n",
    "\t\timage_patches = self.get_image_patches(X, self.receptive_filter_size)\n",
    "\t\t_, image_patches_height, image_patches_width, feature_dim = image_patches.size()\n",
    "\n",
    "\t\t# process vertical rnn inputs\n",
    "\t\tvertical_rnn_inputs_fw = self.get_vertical_rnn_inputs(image_patches, forward=True)\n",
    "\t\tvertical_rnn_inputs_rev = self.get_vertical_rnn_inputs(image_patches, forward=False)\n",
    "    \n",
    "\t\t#print(\"vertical_rnn_inputs_fw: \",vertical_rnn_inputs_fw.shape)\n",
    "\t\t#print(\"vertical_rnn_inputs_rev: \",vertical_rnn_inputs_rev.shape)\n",
    "\t\t# extract vertical hidden states\n",
    "\t\tvertical_forward_hidden, vertical_forward_cell = self.rnn1(vertical_rnn_inputs_fw, self.hidden)\n",
    "\t\tvertical_reverse_hidden, vertical_reverse_cell = self.rnn2(vertical_rnn_inputs_rev, self.hidden)\n",
    "\n",
    "\t\t# create vertical feature map\n",
    "\t\tvertical_feature_map = torch.cat((vertical_forward_hidden, vertical_reverse_hidden), 2)\n",
    "\t\tvertical_feature_map =  vertical_feature_map.permute(1, 0, 2)\n",
    "\n",
    "\t\t# reshape vertical feature map to (batch size, image_patches_height, image_patches_width, hidden_size * 2)\n",
    "\t\tvertical_feature_map = vertical_feature_map.contiguous().view(-1, image_patches_width, image_patches_height, self.hidden_size * 2)\n",
    "\t\tvertical_feature_map.permute(0, 2, 1, 3)\n",
    "\n",
    "\t\t# process horizontal rnn inputs\n",
    "\t\thorizontal_rnn_inputs_fw = self.get_horizontal_rnn_inputs(vertical_feature_map, image_patches_height, image_patches_width, forward=True)\n",
    "\t\thorizontal_rnn_inputs_rev = self.get_horizontal_rnn_inputs(vertical_feature_map, image_patches_height, image_patches_width, forward=False)\n",
    "\n",
    "\t\t#print(\"horizontal_rnn_inputs_fw1: \",horizontal_rnn_inputs_fw.shape)\n",
    "\t\t#print(\"horizontal_rnn_inputs_rev1: \",horizontal_rnn_inputs_rev.shape)\n",
    "\t\t# extract horizontal hidden states\n",
    "\t\thorizontal_forward_hidden, horizontal_forward_cell = self.rnn3(horizontal_rnn_inputs_fw, self.hidden)\n",
    "\t\thorizontal_reverse_hidden, horizontal_reverse_cell = self.rnn4(horizontal_rnn_inputs_rev, self.hidden)\n",
    "\n",
    "\t\t# create horiztonal feature map[64,1,320]\n",
    "\t\t#print(\"horizontal_forward_hidden2: \",horizontal_forward_hidden.shape)\n",
    "\t\t#print(\"horizontal_reverse_hidden2: \",horizontal_reverse_hidden.shape)\n",
    "\t\thorizontal_feature_map = torch.cat((horizontal_forward_hidden, horizontal_reverse_hidden), 2)\n",
    "\t\thorizontal_feature_map =  horizontal_feature_map.permute(1, 0, 2)\n",
    "\n",
    "\t\t# flatten[1,64,640]\n",
    "\t\toutput = horizontal_feature_map.contiguous().view(-1, image_patches_height , image_patches_width , self.hidden_size * 2)\n",
    "\t\toutput = output.view(output.size(0), -1)\n",
    "\t\t#output=output.permute(0,3,1,2)#[1,640,8,8]\n",
    "\t\t#conv1=self.conv1(output)\n",
    "\t\t#Upsampling=self.UpsamplingBilinear2d(conv1)\n",
    "\t\t# dense layer\n",
    "\t\toutput = F.relu(self.dense(output))\n",
    "\t\t \n",
    "\t\t# fully connected layer\n",
    "\t\tlogits = self.fc(output)\n",
    "\n",
    "\t\t# log softmax\n",
    "\t\t#logits = self.log_softmax(Upsampling)\n",
    "\n",
    "\t\treturn logits\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    s = '%s' % (asMinutes(s))\n",
    "    return s\n",
    "\n",
    "\n",
    "'''Some helper functions for PyTorch, including:\n",
    "    - get_mean_and_std: calculate the mean and std value of dataset.\n",
    "    - msr_init: net parameter initialization.\n",
    "    - progress_bar: progress bar mimic xlua.progress.\n",
    "'''\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "def get_mean_and_std(dataset):\n",
    "    '''Compute the mean and std value of dataset.'''\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    print('==> Computing mean and std..')\n",
    "    for inputs, targets in dataloader:\n",
    "        for i in range(3):\n",
    "            mean[i] += inputs[:,i,:,:].mean()\n",
    "            std[i] += inputs[:,i,:,:].std()\n",
    "    mean.div_(len(dataset))\n",
    "    std.div_(len(dataset))\n",
    "    return mean, std\n",
    "\n",
    "def init_params(net):\n",
    "    '''Init layer parameters.'''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal(m.weight, mode='fan_out')\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant(m.weight, 1)\n",
    "            init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal(m.weight, std=1e-3)\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1680817080701,
     "user": {
      "displayName": "Omar Al Akkad",
      "userId": "09097421262007916771"
     },
     "user_tz": 300
    },
    "id": "pbWUyHPaErza"
   },
   "outputs": [],
   "source": [
    "#@title (Run) Part 4.2: Define required functions for DLA & DLA+RNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class Root(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1):\n",
    "        super(Root, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride=1, padding=(kernel_size - 1) // 2, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = torch.cat(xs, 1)\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Tree(nn.Module):\n",
    "    def __init__(self, block, in_channels, out_channels, level=1, stride=1):\n",
    "        super(Tree, self).__init__()\n",
    "        self.level = level\n",
    "        if level == 1:\n",
    "            self.root = Root(2*out_channels, out_channels)\n",
    "            self.left_node = block(in_channels, out_channels, stride=stride)\n",
    "            self.right_node = block(out_channels, out_channels, stride=1)\n",
    "        else:\n",
    "            self.root = Root((level+2)*out_channels, out_channels)\n",
    "            for i in reversed(range(1, level)):\n",
    "                subtree = Tree(block, in_channels, out_channels,\n",
    "                               level=i, stride=stride)\n",
    "                self.__setattr__('level_%d' % i, subtree)\n",
    "            self.prev_root = block(in_channels, out_channels, stride=stride)\n",
    "            self.left_node = block(out_channels, out_channels, stride=1)\n",
    "            self.right_node = block(out_channels, out_channels, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = [self.prev_root(x)] if self.level > 1 else []\n",
    "        for i in reversed(range(1, self.level)):\n",
    "            level_i = self.__getattr__('level_%d' % i)\n",
    "            x = level_i(x)\n",
    "            xs.append(x)\n",
    "        x = self.left_node(x)\n",
    "        xs.append(x)\n",
    "        x = self.right_node(x)\n",
    "        xs.append(x)\n",
    "        out = self.root(xs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DLA(nn.Module):\n",
    "    def __init__(self, block=BasicBlock, num_classes=10, enable_RNN='LSTM'):\n",
    "        super(DLA, self).__init__()\n",
    "        \n",
    "        self.enable_RNN = enable_RNN\n",
    "        if enable_RNN not in ['None', 'LSTM', 'Bi-LSTM']:\n",
    "          raise Exception(\"enable_RNN only supports one of ['None', 'LSTM', 'Bi-LSTM']\")\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer3 = Tree(block,  32,  64, level=1, stride=1)\n",
    "        self.layer4 = Tree(block,  64, 128, level=2, stride=2)\n",
    "        self.layer5 = Tree(block, 128, 256, level=2, stride=2)\n",
    "        self.layer6 = Tree(block, 256, 512, level=1, stride=2)\n",
    "        self.linear = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        if enable_RNN == 'None':\n",
    "          self.linear = nn.Linear(512, num_classes)\n",
    "        elif enable_RNN == 'LSTM':\n",
    "          self.rnn = nn.LSTM(512, 1024, dropout = 0.6)\n",
    "          self.linear = nn.Linear(1024, num_classes)\n",
    "        elif enable_RNN == 'Bi-LSTM':\n",
    "          self.rnn = nn.LSTM(512, 2048, 1, dropout = 0.3, batch_first=True, bidirectional=True)\n",
    "          self.linear = nn.Linear(4096, num_classes)\n",
    "        else:\n",
    "          raise Exception(\"enable_RNN only supports one of ['None', 'LSTM', 'Bi-LSTM']\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "\n",
    "        if self.enable_RNN == 'None':\n",
    "          out = out.view(out.size(0), -1)\n",
    "          out = self.linear(out)\n",
    "        else:\n",
    "          # add LSTM\n",
    "          #print(out.shape)\n",
    "          out = out.view(out.size(0), 1,  -1)\n",
    "          out,_ = self.rnn(out)\n",
    "          out = out.view(out.size(0), -1)\n",
    "          out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1680817080702,
     "user": {
      "displayName": "Omar Al Akkad",
      "userId": "09097421262007916771"
     },
     "user_tz": 300
    },
    "id": "PyL-Bm2dnrIk"
   },
   "outputs": [],
   "source": [
    "#@title (Run) Part 4.3: Define required functions for VGG\n",
    "\n",
    "'''VGG11/13/16/19 in Pytorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, num_classes=10, enable_RNN='LSTM'):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.enable_RNN = enable_RNN\n",
    "        if enable_RNN not in ['None', 'LSTM', 'Bi-LSTM']:\n",
    "          raise Exception(\"enable_RNN only supports one of ['None', 'LSTM', 'Bi-LSTM']\")\n",
    "        \n",
    "        \n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        \n",
    "        \n",
    "        if enable_RNN == 'None':\n",
    "          self.linear = nn.Linear(512, num_classes)\n",
    "        elif enable_RNN == 'LSTM':\n",
    "          self.rnn = nn.LSTM(512, 1024, dropout = 0.6)\n",
    "          self.linear = nn.Linear(1024, num_classes)\n",
    "        elif enable_RNN == 'Bi-LSTM':\n",
    "          self.rnn = nn.LSTM(512, 2048, 1, dropout = 0.3, batch_first=True, bidirectional=True)\n",
    "          self.linear = nn.Linear(4096, num_classes)\n",
    "        else:\n",
    "          raise Exception(\"enable_RNN only supports one of ['None', 'LSTM', 'Bi-LSTM']\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        \n",
    "        if self.enable_RNN == 'None':\n",
    "          out = out.view(out.size(0), -1)\n",
    "          out = self.linear(out)\n",
    "        else:\n",
    "          # add LSTM\n",
    "          #print(out.shape)\n",
    "          out = out.view(out.size(0), 1,  -1)\n",
    "          out,_ = self.rnn(out)\n",
    "          out = out.view(out.size(0), -1)\n",
    "          out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1680817080894,
     "user": {
      "displayName": "Omar Al Akkad",
      "userId": "09097421262007916771"
     },
     "user_tz": 300
    },
    "id": "TJchcdCLnrIl"
   },
   "outputs": [],
   "source": [
    "#@title (Run) Part 4.4: Define required functions for ResNet\n",
    "\n",
    "'''\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, enable_RNN='LSTM'):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.enable_RNN = enable_RNN\n",
    "        if enable_RNN not in ['None', 'LSTM', 'Bi-LSTM']:\n",
    "          raise Exception(\"enable_RNN only supports one of ['None', 'LSTM', 'Bi-LSTM']\")\n",
    "        \n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        #self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if enable_RNN == 'None':\n",
    "          self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "        elif enable_RNN == 'LSTM':\n",
    "          self.rnn = nn.LSTM(512*block.expansion, 1024, dropout = 0.6)\n",
    "          self.linear = nn.Linear(1024, num_classes)\n",
    "        elif enable_RNN == 'Bi-LSTM':\n",
    "          self.rnn = nn.LSTM(512*block.expansion, 2048, 1, dropout = 0.3, batch_first=True, bidirectional=True)\n",
    "          self.linear = nn.Linear(4096, num_classes)\n",
    "        else:\n",
    "          raise Exception(\"enable_RNN only supports one of ['None', 'LSTM', 'Bi-LSTM']\")\n",
    "\n",
    "        \n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        \n",
    "        if self.enable_RNN == 'None':\n",
    "          out = out.view(out.size(0), -1)\n",
    "          out = self.linear(out)\n",
    "        else:\n",
    "          # add LSTM\n",
    "          #print(out.shape)\n",
    "          out = out.view(out.size(0), 1,  -1)\n",
    "          out,_ = self.rnn(out)\n",
    "          out = out.view(out.size(0), -1)\n",
    "          out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1680817080895,
     "user": {
      "displayName": "Omar Al Akkad",
      "userId": "09097421262007916771"
     },
     "user_tz": 300
    },
    "id": "MXLaKqNMnrIl"
   },
   "outputs": [],
   "source": [
    "#@title (Run) Part 4.5: Define required functions for DenseNet\n",
    "\n",
    "\n",
    "'''DenseNet in PyTorch.'''\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat([out,x], 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10, enable_RNN='LSTM'):\n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.enable_RNN = enable_RNN\n",
    "        if enable_RNN not in ['None', 'LSTM', 'Bi-LSTM']:\n",
    "          raise Exception(\"enable_RNN only supports one of ['None', 'LSTM', 'Bi-LSTM']\")\n",
    "        \n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2*growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3]*growth_rate\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "        #self.linear = nn.Linear(num_planes, num_classes)\n",
    "        \n",
    "        \n",
    "        if enable_RNN == 'None':\n",
    "          self.linear = nn.Linear(num_planes, num_classes)\n",
    "        elif enable_RNN == 'LSTM':\n",
    "          self.rnn = nn.LSTM(num_planes, 1024, dropout = 0.6)\n",
    "          self.linear = nn.Linear(1024, num_classes)\n",
    "        elif enable_RNN == 'Bi-LSTM':\n",
    "          self.rnn = nn.LSTM(num_planes, 2048, 1, dropout = 0.3, batch_first=True, bidirectional=True)\n",
    "          self.linear = nn.Linear(4096, num_classes)\n",
    "        else:\n",
    "          raise Exception(\"enable_RNN only supports one of ['None', 'LSTM', 'Bi-LSTM']\")\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        #out = out.view(out.size(0), -1)\n",
    "        #out = self.linear(out)\n",
    "        \n",
    "        if self.enable_RNN == 'None':\n",
    "          out = out.view(out.size(0), -1)\n",
    "          out = self.linear(out)\n",
    "        else:\n",
    "          # add LSTM\n",
    "          #print(out.shape)\n",
    "          out = out.view(out.size(0), 1,  -1)\n",
    "          out,_ = self.rnn(out)\n",
    "          out = out.view(out.size(0), -1)\n",
    "          out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def DenseNet121():\n",
    "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n",
    "\n",
    "def DenseNet169():\n",
    "    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n",
    "\n",
    "def DenseNet201():\n",
    "    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n",
    "\n",
    "def DenseNet161():\n",
    "    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n",
    "\n",
    "def densenet_cifar():\n",
    "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDLHoqByF4_7"
   },
   "source": [
    "## Part 5: Setup Target model for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxiCAbzuGY2U"
   },
   "source": [
    "### Method 1: ReNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1250,
     "status": "ok",
     "timestamp": 1680817132024,
     "user": {
      "displayName": "Omar Al Akkad",
      "userId": "09097421262007916771"
     },
     "user_tz": 300
    },
    "id": "GxU9m5y1Er2A",
    "outputId": "205a8ca7-3879-4125-9a93-e4f191c3c645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model for ReNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training model from scratch..\n",
      "Total trained parameters:  45364234\n",
      "ReNet(\n",
      "  (rnn1): LSTM(48, 320, dropout=0.2)\n",
      "  (rnn2): LSTM(48, 320, dropout=0.2)\n",
      "  (rnn3): LSTM(640, 320, dropout=0.2)\n",
      "  (rnn4): LSTM(640, 320, dropout=0.2)\n",
      "  (dense): Linear(in_features=40960, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'ReNet'  #@param {type:\"string\"}\n",
    "save_model_folder = './ReNet_models/'   #@param {type:\"string\"}\n",
    "batch_size = 128  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "if method_name == 'ReNet':\n",
    "  # Model\n",
    "  print('==> Building model for ' + method_name)\n",
    "\n",
    "  receptive_filter_size = 4\n",
    "  hidden_size = 320\n",
    "  image_size_w = 32\n",
    "  image_size_h = 32\n",
    "  \n",
    "\n",
    "  net = ReNet(receptive_filter_size, hidden_size, batch_size, image_size_w/receptive_filter_size, image_size_h/receptive_filter_size)\n",
    "  net.cuda()\n",
    "\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "  if load_pretrain_weight:\n",
    "    try:\n",
    "        # Load checkpoint.\n",
    "        print('==> Resuming from checkpoint..')\n",
    "        checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    except:\n",
    "        print('!!! Error: no checkpoint directory found!')\n",
    "  else:\n",
    "    print('==> Training model from scratch..')\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                        momentum=0.9, weight_decay=5e-4)\n",
    "  #optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "  pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "  print(\"Total trained parameters: \",pytorch_total_params)\n",
    "\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOtDNYaqM7MY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10616,
     "status": "ok",
     "timestamp": 1680817190336,
     "user": {
      "displayName": "Omar Al Akkad",
      "userId": "09097421262007916771"
     },
     "user_tz": 300
    },
    "id": "bJcvqWHL-wUF",
    "outputId": "045e68d7-f083-4161-f3bf-1deb977394d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Setting target_train_dataset size to  15000 Counter({8: 1573, 5: 1564, 9: 1503, 1: 1499, 3: 1488, 4: 1486, 2: 1481, 6: 1475, 0: 1468, 7: 1463})\n",
      "Setting target_test_dataset size to  15000 Counter({0: 1560, 9: 1555, 4: 1540, 7: 1509, 3: 1503, 1: 1486, 5: 1469, 8: 1465, 6: 1460, 2: 1453})\n",
      "Setting shadow_train_dataset size to  15000 Counter({6: 1565, 7: 1540, 2: 1523, 0: 1504, 4: 1501, 8: 1487, 9: 1481, 3: 1475, 1: 1464, 5: 1460})\n",
      "Setting shadow_test_dataset size to  15000 Counter({1: 1551, 2: 1543, 3: 1534, 5: 1507, 6: 1500, 7: 1488, 8: 1475, 4: 1473, 0: 1468, 9: 1461})\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.1.2: Setup Target and Shadow datasets for ReNet Training\n",
    "\n",
    "target_train_size = 15000 #@param {type:\"integer\"}\n",
    "target_test_size= 15000 #@param {type:\"integer\"}\n",
    "shadow_train_size = 15000  #@param {type:\"integer\"} \n",
    "shadow_test_size= 15000 #@param {type:\"integer\"}\n",
    "\n",
    "# create dataset for renet \n",
    "print('==> Preparing dataset..')\n",
    "target_trainloader, target_testloader, shadow_train_dataset, shadow_testloader = create_cifar_dataset_torch(batch_size=batch_size, target_train_size = target_train_size, target_test_size= target_test_size, shadow_train_size = shadow_train_size, shadow_test_size= shadow_test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTWT7NrGD3tO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2546955,
     "status": "ok",
     "timestamp": 1680819768428,
     "user": {
      "displayName": "Omar Al Akkad",
      "userId": "09097421262007916771"
     },
     "user_tz": 300
    },
    "id": "r5MvupwJBDUR",
    "outputId": "d4b24550-b0bf-46d0-d632-76997f91329b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 117 Train Loss: 2.303 | Train Acc: 11.719% (15/128)\n",
      "30 117 Train Loss: 2.300 | Train Acc: 12.147% (482/3968)\n",
      "60 117 Train Loss: 2.295 | Train Acc: 12.795% (999/7808)\n",
      "90 117 Train Loss: 2.289 | Train Acc: 14.363% (1673/11648)\n",
      "117 Epoch: 0 | Train Loss: 2.279 | Train Acc: 15.331% (2296/14976)\n",
      "0 117 Test Loss: 2.198 | Test Acc: 24.219% (31/128)\n",
      "30 117 Test Loss: 2.219 | Test Acc: 20.010% (794/3968)\n",
      "60 117 Test Loss: 2.218 | Test Acc: 21.119% (1649/7808)\n",
      "90 117 Test Loss: 2.218 | Test Acc: 21.145% (2463/11648)\n",
      "117 Epoch: 0 | Test Loss: 2.219 | Test Acc: 20.967% (3140/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "0 117 Train Loss: 2.203 | Train Acc: 25.781% (33/128)\n",
      "30 117 Train Loss: 2.184 | Train Acc: 21.472% (852/3968)\n",
      "60 117 Train Loss: 2.153 | Train Acc: 21.568% (1684/7808)\n",
      "90 117 Train Loss: 2.135 | Train Acc: 21.738% (2532/11648)\n",
      "117 Epoch: 1 | Train Loss: 2.125 | Train Acc: 21.982% (3292/14976)\n",
      "0 117 Test Loss: 2.126 | Test Acc: 14.062% (18/128)\n",
      "30 117 Test Loss: 2.111 | Test Acc: 20.010% (794/3968)\n",
      "60 117 Test Loss: 2.112 | Test Acc: 20.543% (1604/7808)\n",
      "90 117 Test Loss: 2.112 | Test Acc: 20.407% (2377/11648)\n",
      "117 Epoch: 1 | Test Loss: 2.107 | Test Acc: 20.713% (3102/14976)\n",
      "\n",
      "Epoch: 2\n",
      "0 117 Train Loss: 2.087 | Train Acc: 23.438% (30/128)\n",
      "30 117 Train Loss: 2.078 | Train Acc: 23.891% (948/3968)\n",
      "60 117 Train Loss: 2.067 | Train Acc: 23.809% (1859/7808)\n",
      "90 117 Train Loss: 2.063 | Train Acc: 23.901% (2784/11648)\n",
      "117 Epoch: 2 | Train Loss: 2.047 | Train Acc: 24.593% (3683/14976)\n",
      "0 117 Test Loss: 1.988 | Test Acc: 28.125% (36/128)\n",
      "30 117 Test Loss: 2.001 | Test Acc: 26.058% (1034/3968)\n",
      "60 117 Test Loss: 2.004 | Test Acc: 26.601% (2077/7808)\n",
      "90 117 Test Loss: 2.006 | Test Acc: 26.528% (3090/11648)\n",
      "117 Epoch: 2 | Test Loss: 2.006 | Test Acc: 26.376% (3950/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "0 117 Train Loss: 2.014 | Train Acc: 26.562% (34/128)\n",
      "30 117 Train Loss: 1.976 | Train Acc: 27.974% (1110/3968)\n",
      "60 117 Train Loss: 1.987 | Train Acc: 26.972% (2106/7808)\n",
      "90 117 Train Loss: 1.991 | Train Acc: 26.863% (3129/11648)\n",
      "117 Epoch: 3 | Train Loss: 1.992 | Train Acc: 26.950% (4036/14976)\n",
      "0 117 Test Loss: 2.026 | Test Acc: 32.812% (42/128)\n",
      "30 117 Test Loss: 1.958 | Test Acc: 28.906% (1147/3968)\n",
      "60 117 Test Loss: 1.956 | Test Acc: 29.713% (2320/7808)\n",
      "90 117 Test Loss: 1.959 | Test Acc: 29.481% (3434/11648)\n",
      "117 Epoch: 3 | Test Loss: 1.960 | Test Acc: 29.434% (4408/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "0 117 Train Loss: 1.944 | Train Acc: 24.219% (31/128)\n",
      "30 117 Train Loss: 1.944 | Train Acc: 28.579% (1134/3968)\n",
      "60 117 Train Loss: 1.941 | Train Acc: 29.034% (2267/7808)\n",
      "90 117 Train Loss: 1.936 | Train Acc: 29.481% (3434/11648)\n",
      "117 Epoch: 4 | Train Loss: 1.932 | Train Acc: 29.688% (4446/14976)\n",
      "0 117 Test Loss: 1.960 | Test Acc: 28.125% (36/128)\n",
      "30 117 Test Loss: 1.913 | Test Acc: 30.721% (1219/3968)\n",
      "60 117 Test Loss: 1.908 | Test Acc: 30.776% (2403/7808)\n",
      "90 117 Test Loss: 1.907 | Test Acc: 31.018% (3613/11648)\n",
      "117 Epoch: 4 | Test Loss: 1.905 | Test Acc: 31.263% (4682/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      "0 117 Train Loss: 1.843 | Train Acc: 36.719% (47/128)\n",
      "30 117 Train Loss: 1.862 | Train Acc: 32.812% (1302/3968)\n",
      "60 117 Train Loss: 1.871 | Train Acc: 32.864% (2566/7808)\n",
      "90 117 Train Loss: 1.876 | Train Acc: 32.503% (3786/11648)\n",
      "117 Epoch: 5 | Train Loss: 1.876 | Train Acc: 32.525% (4871/14976)\n",
      "0 117 Test Loss: 1.823 | Test Acc: 32.031% (41/128)\n",
      "30 117 Test Loss: 1.859 | Test Acc: 32.888% (1305/3968)\n",
      "60 117 Test Loss: 1.855 | Test Acc: 33.197% (2592/7808)\n",
      "90 117 Test Loss: 1.859 | Test Acc: 32.795% (3820/11648)\n",
      "117 Epoch: 5 | Test Loss: 1.862 | Test Acc: 33.046% (4949/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      "0 117 Train Loss: 1.778 | Train Acc: 37.500% (48/128)\n",
      "30 117 Train Loss: 1.842 | Train Acc: 33.014% (1310/3968)\n",
      "60 117 Train Loss: 1.843 | Train Acc: 33.466% (2613/7808)\n",
      "90 117 Train Loss: 1.832 | Train Acc: 33.877% (3946/11648)\n",
      "117 Epoch: 6 | Train Loss: 1.828 | Train Acc: 34.282% (5134/14976)\n",
      "0 117 Test Loss: 1.882 | Test Acc: 34.375% (44/128)\n",
      "30 117 Test Loss: 1.807 | Test Acc: 34.753% (1379/3968)\n",
      "60 117 Test Loss: 1.810 | Test Acc: 35.079% (2739/7808)\n",
      "90 117 Test Loss: 1.815 | Test Acc: 34.985% (4075/11648)\n",
      "117 Epoch: 6 | Test Loss: 1.811 | Test Acc: 35.250% (5279/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      "0 117 Train Loss: 1.816 | Train Acc: 32.812% (42/128)\n",
      "30 117 Train Loss: 1.802 | Train Acc: 35.862% (1423/3968)\n",
      "60 117 Train Loss: 1.795 | Train Acc: 35.438% (2767/7808)\n",
      "90 117 Train Loss: 1.787 | Train Acc: 35.877% (4179/11648)\n",
      "117 Epoch: 7 | Train Loss: 1.776 | Train Acc: 36.452% (5459/14976)\n",
      "0 117 Test Loss: 1.683 | Test Acc: 37.500% (48/128)\n",
      "30 117 Test Loss: 1.776 | Test Acc: 36.643% (1454/3968)\n",
      "60 117 Test Loss: 1.775 | Test Acc: 36.949% (2885/7808)\n",
      "90 117 Test Loss: 1.774 | Test Acc: 36.873% (4295/11648)\n",
      "117 Epoch: 7 | Test Loss: 1.777 | Test Acc: 36.532% (5471/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      "0 117 Train Loss: 1.677 | Train Acc: 41.406% (53/128)\n",
      "30 117 Train Loss: 1.745 | Train Acc: 37.500% (1488/3968)\n",
      "60 117 Train Loss: 1.735 | Train Acc: 38.076% (2973/7808)\n",
      "90 117 Train Loss: 1.730 | Train Acc: 38.616% (4498/11648)\n",
      "117 Epoch: 8 | Train Loss: 1.722 | Train Acc: 38.835% (5816/14976)\n",
      "0 117 Test Loss: 1.754 | Test Acc: 35.938% (46/128)\n",
      "30 117 Test Loss: 1.747 | Test Acc: 37.676% (1495/3968)\n",
      "60 117 Test Loss: 1.728 | Test Acc: 38.461% (3003/7808)\n",
      "90 117 Test Loss: 1.722 | Test Acc: 38.642% (4501/11648)\n",
      "117 Epoch: 8 | Test Loss: 1.722 | Test Acc: 38.749% (5803/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      "0 117 Train Loss: 1.654 | Train Acc: 39.062% (50/128)\n",
      "30 117 Train Loss: 1.673 | Train Acc: 40.197% (1595/3968)\n",
      "60 117 Train Loss: 1.682 | Train Acc: 40.177% (3137/7808)\n",
      "90 117 Train Loss: 1.679 | Train Acc: 40.204% (4683/11648)\n",
      "117 Epoch: 9 | Train Loss: 1.684 | Train Acc: 40.004% (5991/14976)\n",
      "0 117 Test Loss: 1.717 | Test Acc: 32.031% (41/128)\n",
      "30 117 Test Loss: 1.685 | Test Acc: 39.819% (1580/3968)\n",
      "60 117 Test Loss: 1.694 | Test Acc: 39.319% (3070/7808)\n",
      "90 117 Test Loss: 1.703 | Test Acc: 39.414% (4591/11648)\n",
      "117 Epoch: 9 | Test Loss: 1.707 | Test Acc: 39.149% (5863/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 10\n",
      "0 117 Train Loss: 1.754 | Train Acc: 37.500% (48/128)\n",
      "30 117 Train Loss: 1.657 | Train Acc: 41.482% (1646/3968)\n",
      "60 117 Train Loss: 1.652 | Train Acc: 41.150% (3213/7808)\n",
      "90 117 Train Loss: 1.648 | Train Acc: 41.303% (4811/11648)\n",
      "117 Epoch: 10 | Train Loss: 1.646 | Train Acc: 41.446% (6207/14976)\n",
      "0 117 Test Loss: 1.679 | Test Acc: 33.594% (43/128)\n",
      "30 117 Test Loss: 1.664 | Test Acc: 40.348% (1601/3968)\n",
      "60 117 Test Loss: 1.661 | Test Acc: 40.446% (3158/7808)\n",
      "90 117 Test Loss: 1.665 | Test Acc: 40.204% (4683/11648)\n",
      "117 Epoch: 10 | Test Loss: 1.666 | Test Acc: 40.097% (6005/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 11\n",
      "0 117 Train Loss: 1.711 | Train Acc: 39.062% (50/128)\n",
      "30 117 Train Loss: 1.609 | Train Acc: 42.944% (1704/3968)\n",
      "60 117 Train Loss: 1.615 | Train Acc: 42.700% (3334/7808)\n",
      "90 117 Train Loss: 1.614 | Train Acc: 42.883% (4995/11648)\n",
      "117 Epoch: 11 | Train Loss: 1.612 | Train Acc: 43.176% (6466/14976)\n",
      "0 117 Test Loss: 1.836 | Test Acc: 40.625% (52/128)\n",
      "30 117 Test Loss: 1.636 | Test Acc: 42.364% (1681/3968)\n",
      "60 117 Test Loss: 1.632 | Test Acc: 42.162% (3292/7808)\n",
      "90 117 Test Loss: 1.629 | Test Acc: 42.436% (4943/11648)\n",
      "117 Epoch: 11 | Test Loss: 1.629 | Test Acc: 42.214% (6322/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 12\n",
      "0 117 Train Loss: 1.727 | Train Acc: 41.406% (53/128)\n",
      "30 117 Train Loss: 1.580 | Train Acc: 44.128% (1751/3968)\n",
      "60 117 Train Loss: 1.579 | Train Acc: 44.365% (3464/7808)\n",
      "90 117 Train Loss: 1.580 | Train Acc: 44.205% (5149/11648)\n",
      "117 Epoch: 12 | Train Loss: 1.580 | Train Acc: 44.004% (6590/14976)\n",
      "0 117 Test Loss: 1.476 | Test Acc: 46.094% (59/128)\n",
      "30 117 Test Loss: 1.598 | Test Acc: 42.540% (1688/3968)\n",
      "60 117 Test Loss: 1.615 | Test Acc: 42.239% (3298/7808)\n",
      "90 117 Test Loss: 1.619 | Test Acc: 42.093% (4903/11648)\n",
      "117 Epoch: 12 | Test Loss: 1.623 | Test Acc: 42.041% (6296/14976)\n",
      "\n",
      "Epoch: 13\n",
      "0 117 Train Loss: 1.635 | Train Acc: 42.188% (54/128)\n",
      "30 117 Train Loss: 1.574 | Train Acc: 44.405% (1762/3968)\n",
      "60 117 Train Loss: 1.556 | Train Acc: 44.749% (3494/7808)\n",
      "90 117 Train Loss: 1.558 | Train Acc: 44.591% (5194/11648)\n",
      "117 Epoch: 13 | Train Loss: 1.555 | Train Acc: 44.798% (6709/14976)\n",
      "0 117 Test Loss: 1.515 | Test Acc: 43.750% (56/128)\n",
      "30 117 Test Loss: 1.594 | Test Acc: 43.221% (1715/3968)\n",
      "60 117 Test Loss: 1.595 | Test Acc: 43.788% (3419/7808)\n",
      "90 117 Test Loss: 1.600 | Test Acc: 43.467% (5063/11648)\n",
      "117 Epoch: 13 | Test Loss: 1.600 | Test Acc: 43.336% (6490/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 14\n",
      "0 117 Train Loss: 1.476 | Train Acc: 49.219% (63/128)\n",
      "30 117 Train Loss: 1.537 | Train Acc: 45.413% (1802/3968)\n",
      "60 117 Train Loss: 1.535 | Train Acc: 45.223% (3531/7808)\n",
      "90 117 Train Loss: 1.537 | Train Acc: 45.484% (5298/11648)\n",
      "117 Epoch: 14 | Train Loss: 1.528 | Train Acc: 46.040% (6895/14976)\n",
      "0 117 Test Loss: 1.598 | Test Acc: 42.188% (54/128)\n",
      "30 117 Test Loss: 1.588 | Test Acc: 43.196% (1714/3968)\n",
      "60 117 Test Loss: 1.580 | Test Acc: 44.147% (3447/7808)\n",
      "90 117 Test Loss: 1.594 | Test Acc: 43.415% (5057/11648)\n",
      "117 Epoch: 14 | Test Loss: 1.592 | Test Acc: 43.543% (6521/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 15\n",
      "0 117 Train Loss: 1.473 | Train Acc: 48.438% (62/128)\n",
      "30 117 Train Loss: 1.512 | Train Acc: 47.077% (1868/3968)\n",
      "60 117 Train Loss: 1.503 | Train Acc: 47.003% (3670/7808)\n",
      "90 117 Train Loss: 1.504 | Train Acc: 46.832% (5455/11648)\n",
      "117 Epoch: 15 | Train Loss: 1.503 | Train Acc: 46.802% (7009/14976)\n",
      "0 117 Test Loss: 1.631 | Test Acc: 40.625% (52/128)\n",
      "30 117 Test Loss: 1.544 | Test Acc: 44.708% (1774/3968)\n",
      "60 117 Test Loss: 1.546 | Test Acc: 44.928% (3508/7808)\n",
      "90 117 Test Loss: 1.554 | Test Acc: 44.900% (5230/11648)\n",
      "117 Epoch: 15 | Test Loss: 1.551 | Test Acc: 44.925% (6728/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 16\n",
      "0 117 Train Loss: 1.414 | Train Acc: 53.906% (69/128)\n",
      "30 117 Train Loss: 1.471 | Train Acc: 48.664% (1931/3968)\n",
      "60 117 Train Loss: 1.481 | Train Acc: 48.309% (3772/7808)\n",
      "90 117 Train Loss: 1.475 | Train Acc: 48.223% (5617/11648)\n",
      "117 Epoch: 16 | Train Loss: 1.470 | Train Acc: 48.458% (7257/14976)\n",
      "0 117 Test Loss: 1.413 | Test Acc: 50.000% (64/128)\n",
      "30 117 Test Loss: 1.562 | Test Acc: 45.111% (1790/3968)\n",
      "60 117 Test Loss: 1.557 | Test Acc: 44.967% (3511/7808)\n",
      "90 117 Test Loss: 1.548 | Test Acc: 45.312% (5278/11648)\n",
      "117 Epoch: 16 | Test Loss: 1.539 | Test Acc: 45.606% (6830/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 17\n",
      "0 117 Train Loss: 1.257 | Train Acc: 57.031% (73/128)\n",
      "30 117 Train Loss: 1.448 | Train Acc: 48.311% (1917/3968)\n",
      "60 117 Train Loss: 1.455 | Train Acc: 48.655% (3799/7808)\n",
      "90 117 Train Loss: 1.449 | Train Acc: 48.789% (5683/11648)\n",
      "117 Epoch: 17 | Train Loss: 1.451 | Train Acc: 48.745% (7300/14976)\n",
      "0 117 Test Loss: 1.518 | Test Acc: 43.750% (56/128)\n",
      "30 117 Test Loss: 1.522 | Test Acc: 45.943% (1823/3968)\n",
      "60 117 Test Loss: 1.523 | Test Acc: 45.838% (3579/7808)\n",
      "90 117 Test Loss: 1.523 | Test Acc: 45.853% (5341/11648)\n",
      "117 Epoch: 17 | Test Loss: 1.534 | Test Acc: 45.259% (6778/14976)\n",
      "\n",
      "Epoch: 18\n",
      "0 117 Train Loss: 1.345 | Train Acc: 50.000% (64/128)\n",
      "30 117 Train Loss: 1.432 | Train Acc: 49.874% (1979/3968)\n",
      "60 117 Train Loss: 1.420 | Train Acc: 50.269% (3925/7808)\n",
      "90 117 Train Loss: 1.430 | Train Acc: 49.760% (5796/11648)\n",
      "117 Epoch: 18 | Train Loss: 1.425 | Train Acc: 49.786% (7456/14976)\n",
      "0 117 Test Loss: 1.452 | Test Acc: 46.094% (59/128)\n",
      "30 117 Test Loss: 1.504 | Test Acc: 47.127% (1870/3968)\n",
      "60 117 Test Loss: 1.527 | Test Acc: 46.004% (3592/7808)\n",
      "90 117 Test Loss: 1.527 | Test Acc: 45.999% (5358/11648)\n",
      "117 Epoch: 18 | Test Loss: 1.522 | Test Acc: 46.067% (6899/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 19\n",
      "0 117 Train Loss: 1.512 | Train Acc: 43.750% (56/128)\n",
      "30 117 Train Loss: 1.392 | Train Acc: 50.932% (2021/3968)\n",
      "60 117 Train Loss: 1.401 | Train Acc: 50.256% (3924/7808)\n",
      "90 117 Train Loss: 1.397 | Train Acc: 50.601% (5894/11648)\n",
      "117 Epoch: 19 | Train Loss: 1.399 | Train Acc: 50.554% (7571/14976)\n",
      "0 117 Test Loss: 1.413 | Test Acc: 57.031% (73/128)\n",
      "30 117 Test Loss: 1.494 | Test Acc: 46.850% (1859/3968)\n",
      "60 117 Test Loss: 1.479 | Test Acc: 47.054% (3674/7808)\n",
      "90 117 Test Loss: 1.489 | Test Acc: 46.557% (5423/11648)\n",
      "117 Epoch: 19 | Test Loss: 1.492 | Test Acc: 46.802% (7009/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 20\n",
      "0 117 Train Loss: 1.221 | Train Acc: 56.250% (72/128)\n",
      "30 117 Train Loss: 1.352 | Train Acc: 51.588% (2047/3968)\n",
      "60 117 Train Loss: 1.365 | Train Acc: 51.524% (4023/7808)\n",
      "90 117 Train Loss: 1.368 | Train Acc: 51.674% (6019/11648)\n",
      "117 Epoch: 20 | Train Loss: 1.370 | Train Acc: 51.549% (7720/14976)\n",
      "0 117 Test Loss: 1.418 | Test Acc: 43.750% (56/128)\n",
      "30 117 Test Loss: 1.524 | Test Acc: 46.623% (1850/3968)\n",
      "60 117 Test Loss: 1.502 | Test Acc: 47.605% (3717/7808)\n",
      "90 117 Test Loss: 1.487 | Test Acc: 48.000% (5591/11648)\n",
      "117 Epoch: 20 | Test Loss: 1.487 | Test Acc: 47.623% (7132/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 21\n",
      "0 117 Train Loss: 1.504 | Train Acc: 51.562% (66/128)\n",
      "30 117 Train Loss: 1.326 | Train Acc: 53.377% (2118/3968)\n",
      "60 117 Train Loss: 1.340 | Train Acc: 52.433% (4094/7808)\n",
      "90 117 Train Loss: 1.348 | Train Acc: 52.035% (6061/11648)\n",
      "117 Epoch: 21 | Train Loss: 1.346 | Train Acc: 52.304% (7833/14976)\n",
      "0 117 Test Loss: 1.356 | Test Acc: 50.000% (64/128)\n",
      "30 117 Test Loss: 1.458 | Test Acc: 47.757% (1895/3968)\n",
      "60 117 Test Loss: 1.477 | Test Acc: 47.464% (3706/7808)\n",
      "90 117 Test Loss: 1.473 | Test Acc: 47.442% (5526/11648)\n",
      "117 Epoch: 21 | Test Loss: 1.476 | Test Acc: 47.289% (7082/14976)\n",
      "\n",
      "Epoch: 22\n",
      "0 117 Train Loss: 1.413 | Train Acc: 46.094% (59/128)\n",
      "30 117 Train Loss: 1.303 | Train Acc: 53.755% (2133/3968)\n",
      "60 117 Train Loss: 1.307 | Train Acc: 54.073% (4222/7808)\n",
      "90 117 Train Loss: 1.319 | Train Acc: 53.580% (6241/11648)\n",
      "117 Epoch: 22 | Train Loss: 1.319 | Train Acc: 53.673% (8038/14976)\n",
      "0 117 Test Loss: 1.625 | Test Acc: 44.531% (57/128)\n",
      "30 117 Test Loss: 1.474 | Test Acc: 47.656% (1891/3968)\n",
      "60 117 Test Loss: 1.467 | Test Acc: 47.720% (3726/7808)\n",
      "90 117 Test Loss: 1.462 | Test Acc: 48.068% (5599/11648)\n",
      "117 Epoch: 22 | Test Loss: 1.460 | Test Acc: 48.331% (7238/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 23\n",
      "0 117 Train Loss: 1.320 | Train Acc: 56.250% (72/128)\n",
      "30 117 Train Loss: 1.280 | Train Acc: 53.730% (2132/3968)\n",
      "60 117 Train Loss: 1.289 | Train Acc: 53.932% (4211/7808)\n",
      "90 117 Train Loss: 1.298 | Train Acc: 53.760% (6262/11648)\n",
      "117 Epoch: 23 | Train Loss: 1.300 | Train Acc: 53.779% (8054/14976)\n",
      "0 117 Test Loss: 1.270 | Test Acc: 48.438% (62/128)\n",
      "30 117 Test Loss: 1.468 | Test Acc: 48.463% (1923/3968)\n",
      "60 117 Test Loss: 1.483 | Test Acc: 47.554% (3713/7808)\n",
      "90 117 Test Loss: 1.469 | Test Acc: 47.991% (5590/11648)\n",
      "117 Epoch: 23 | Test Loss: 1.462 | Test Acc: 48.150% (7211/14976)\n",
      "\n",
      "Epoch: 24\n",
      "0 117 Train Loss: 1.352 | Train Acc: 50.781% (65/128)\n",
      "30 117 Train Loss: 1.244 | Train Acc: 55.922% (2219/3968)\n",
      "60 117 Train Loss: 1.250 | Train Acc: 55.622% (4343/7808)\n",
      "90 117 Train Loss: 1.263 | Train Acc: 55.185% (6428/11648)\n",
      "117 Epoch: 24 | Train Loss: 1.269 | Train Acc: 54.981% (8234/14976)\n",
      "0 117 Test Loss: 1.466 | Test Acc: 47.656% (61/128)\n",
      "30 117 Test Loss: 1.447 | Test Acc: 50.126% (1989/3968)\n",
      "60 117 Test Loss: 1.451 | Test Acc: 49.501% (3865/7808)\n",
      "90 117 Test Loss: 1.442 | Test Acc: 49.330% (5746/11648)\n",
      "117 Epoch: 24 | Test Loss: 1.441 | Test Acc: 49.299% (7383/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 25\n",
      "0 117 Train Loss: 1.242 | Train Acc: 58.594% (75/128)\n",
      "30 117 Train Loss: 1.233 | Train Acc: 56.981% (2261/3968)\n",
      "60 117 Train Loss: 1.242 | Train Acc: 56.365% (4401/7808)\n",
      "90 117 Train Loss: 1.249 | Train Acc: 56.044% (6528/11648)\n",
      "117 Epoch: 25 | Train Loss: 1.252 | Train Acc: 55.883% (8369/14976)\n",
      "0 117 Test Loss: 1.373 | Test Acc: 53.125% (68/128)\n",
      "30 117 Test Loss: 1.434 | Test Acc: 49.572% (1967/3968)\n",
      "60 117 Test Loss: 1.432 | Test Acc: 49.270% (3847/7808)\n",
      "90 117 Test Loss: 1.434 | Test Acc: 49.493% (5765/11648)\n",
      "117 Epoch: 25 | Test Loss: 1.437 | Test Acc: 49.446% (7405/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 26\n",
      "0 117 Train Loss: 1.216 | Train Acc: 55.469% (71/128)\n",
      "30 117 Train Loss: 1.241 | Train Acc: 55.973% (2221/3968)\n",
      "60 117 Train Loss: 1.226 | Train Acc: 56.570% (4417/7808)\n",
      "90 117 Train Loss: 1.220 | Train Acc: 56.276% (6555/11648)\n",
      "117 Epoch: 26 | Train Loss: 1.223 | Train Acc: 56.444% (8453/14976)\n",
      "0 117 Test Loss: 1.436 | Test Acc: 49.219% (63/128)\n",
      "30 117 Test Loss: 1.454 | Test Acc: 48.387% (1920/3968)\n",
      "60 117 Test Loss: 1.430 | Test Acc: 48.911% (3819/7808)\n",
      "90 117 Test Loss: 1.430 | Test Acc: 49.313% (5744/11648)\n",
      "117 Epoch: 26 | Test Loss: 1.430 | Test Acc: 49.299% (7383/14976)\n",
      "\n",
      "Epoch: 27\n",
      "0 117 Train Loss: 1.190 | Train Acc: 59.375% (76/128)\n",
      "30 117 Train Loss: 1.193 | Train Acc: 57.082% (2265/3968)\n",
      "60 117 Train Loss: 1.177 | Train Acc: 58.145% (4540/7808)\n",
      "90 117 Train Loss: 1.186 | Train Acc: 57.709% (6722/11648)\n",
      "117 Epoch: 27 | Train Loss: 1.187 | Train Acc: 57.999% (8686/14976)\n",
      "0 117 Test Loss: 1.297 | Test Acc: 53.125% (68/128)\n",
      "30 117 Test Loss: 1.429 | Test Acc: 49.420% (1961/3968)\n",
      "60 117 Test Loss: 1.424 | Test Acc: 50.192% (3919/7808)\n",
      "90 117 Test Loss: 1.428 | Test Acc: 50.094% (5835/11648)\n",
      "117 Epoch: 27 | Test Loss: 1.429 | Test Acc: 49.953% (7481/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 28\n",
      "0 117 Train Loss: 0.913 | Train Acc: 69.531% (89/128)\n",
      "30 117 Train Loss: 1.137 | Train Acc: 60.459% (2399/3968)\n",
      "60 117 Train Loss: 1.141 | Train Acc: 59.926% (4679/7808)\n",
      "90 117 Train Loss: 1.159 | Train Acc: 59.195% (6895/11648)\n",
      "117 Epoch: 28 | Train Loss: 1.165 | Train Acc: 58.914% (8823/14976)\n",
      "0 117 Test Loss: 1.386 | Test Acc: 54.688% (70/128)\n",
      "30 117 Test Loss: 1.391 | Test Acc: 50.857% (2018/3968)\n",
      "60 117 Test Loss: 1.403 | Test Acc: 50.717% (3960/7808)\n",
      "90 117 Test Loss: 1.413 | Test Acc: 50.223% (5850/11648)\n",
      "117 Epoch: 28 | Test Loss: 1.423 | Test Acc: 49.900% (7473/14976)\n",
      "\n",
      "Epoch: 29\n",
      "0 117 Train Loss: 1.064 | Train Acc: 60.938% (78/128)\n",
      "30 117 Train Loss: 1.135 | Train Acc: 58.997% (2341/3968)\n",
      "60 117 Train Loss: 1.132 | Train Acc: 59.734% (4664/7808)\n",
      "90 117 Train Loss: 1.141 | Train Acc: 59.392% (6918/11648)\n",
      "117 Epoch: 29 | Train Loss: 1.139 | Train Acc: 59.302% (8881/14976)\n",
      "0 117 Test Loss: 1.337 | Test Acc: 52.344% (67/128)\n",
      "30 117 Test Loss: 1.421 | Test Acc: 50.731% (2013/3968)\n",
      "60 117 Test Loss: 1.410 | Test Acc: 51.281% (4004/7808)\n",
      "90 117 Test Loss: 1.417 | Test Acc: 50.816% (5919/11648)\n",
      "117 Epoch: 29 | Test Loss: 1.416 | Test Acc: 50.881% (7620/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 30\n",
      "0 117 Train Loss: 0.973 | Train Acc: 65.625% (84/128)\n",
      "30 117 Train Loss: 1.084 | Train Acc: 62.021% (2461/3968)\n",
      "60 117 Train Loss: 1.074 | Train Acc: 61.988% (4840/7808)\n",
      "90 117 Train Loss: 1.095 | Train Acc: 61.041% (7110/11648)\n",
      "117 Epoch: 30 | Train Loss: 1.108 | Train Acc: 60.817% (9108/14976)\n",
      "0 117 Test Loss: 1.451 | Test Acc: 48.438% (62/128)\n",
      "30 117 Test Loss: 1.416 | Test Acc: 49.748% (1974/3968)\n",
      "60 117 Test Loss: 1.418 | Test Acc: 50.346% (3931/7808)\n",
      "90 117 Test Loss: 1.439 | Test Acc: 50.009% (5825/11648)\n",
      "117 Epoch: 30 | Test Loss: 1.443 | Test Acc: 49.893% (7472/14976)\n",
      "\n",
      "Epoch: 31\n",
      "0 117 Train Loss: 0.925 | Train Acc: 68.750% (88/128)\n",
      "30 117 Train Loss: 1.057 | Train Acc: 62.046% (2462/3968)\n",
      "60 117 Train Loss: 1.081 | Train Acc: 61.898% (4833/7808)\n",
      "90 117 Train Loss: 1.081 | Train Acc: 61.865% (7206/11648)\n",
      "117 Epoch: 31 | Train Loss: 1.081 | Train Acc: 61.672% (9236/14976)\n",
      "0 117 Test Loss: 1.473 | Test Acc: 47.656% (61/128)\n",
      "30 117 Test Loss: 1.416 | Test Acc: 50.529% (2005/3968)\n",
      "60 117 Test Loss: 1.425 | Test Acc: 50.346% (3931/7808)\n",
      "90 117 Test Loss: 1.423 | Test Acc: 50.386% (5869/11648)\n",
      "117 Epoch: 31 | Test Loss: 1.416 | Test Acc: 50.681% (7590/14976)\n",
      "\n",
      "Epoch: 32\n",
      "0 117 Train Loss: 0.941 | Train Acc: 65.625% (84/128)\n",
      "30 117 Train Loss: 1.003 | Train Acc: 64.718% (2568/3968)\n",
      "60 117 Train Loss: 1.033 | Train Acc: 63.665% (4971/7808)\n",
      "90 117 Train Loss: 1.042 | Train Acc: 63.316% (7375/11648)\n",
      "117 Epoch: 32 | Train Loss: 1.048 | Train Acc: 63.094% (9449/14976)\n",
      "0 117 Test Loss: 1.471 | Test Acc: 50.781% (65/128)\n",
      "30 117 Test Loss: 1.430 | Test Acc: 51.436% (2041/3968)\n",
      "60 117 Test Loss: 1.434 | Test Acc: 51.345% (4009/7808)\n",
      "90 117 Test Loss: 1.416 | Test Acc: 51.211% (5965/11648)\n",
      "117 Epoch: 32 | Test Loss: 1.430 | Test Acc: 50.942% (7629/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 33\n",
      "0 117 Train Loss: 0.996 | Train Acc: 67.969% (87/128)\n",
      "30 117 Train Loss: 0.995 | Train Acc: 65.045% (2581/3968)\n",
      "60 117 Train Loss: 0.990 | Train Acc: 64.972% (5073/7808)\n",
      "90 117 Train Loss: 1.012 | Train Acc: 64.234% (7482/11648)\n",
      "117 Epoch: 33 | Train Loss: 1.015 | Train Acc: 64.216% (9617/14976)\n",
      "0 117 Test Loss: 1.666 | Test Acc: 39.062% (50/128)\n",
      "30 117 Test Loss: 1.479 | Test Acc: 50.227% (1993/3968)\n",
      "60 117 Test Loss: 1.445 | Test Acc: 50.897% (3974/7808)\n",
      "90 117 Test Loss: 1.434 | Test Acc: 51.073% (5949/11648)\n",
      "117 Epoch: 33 | Test Loss: 1.421 | Test Acc: 51.469% (7708/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 34\n",
      "0 117 Train Loss: 0.978 | Train Acc: 65.625% (84/128)\n",
      "30 117 Train Loss: 0.946 | Train Acc: 67.011% (2659/3968)\n",
      "60 117 Train Loss: 0.957 | Train Acc: 66.637% (5203/7808)\n",
      "90 117 Train Loss: 0.963 | Train Acc: 66.054% (7694/11648)\n",
      "117 Epoch: 34 | Train Loss: 0.979 | Train Acc: 65.418% (9797/14976)\n",
      "0 117 Test Loss: 1.403 | Test Acc: 49.219% (63/128)\n",
      "30 117 Test Loss: 1.424 | Test Acc: 51.991% (2063/3968)\n",
      "60 117 Test Loss: 1.403 | Test Acc: 53.035% (4141/7808)\n",
      "90 117 Test Loss: 1.394 | Test Acc: 52.747% (6144/11648)\n",
      "117 Epoch: 34 | Test Loss: 1.398 | Test Acc: 52.491% (7861/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 35\n",
      "0 117 Train Loss: 0.855 | Train Acc: 67.969% (87/128)\n",
      "30 117 Train Loss: 0.923 | Train Acc: 67.112% (2663/3968)\n",
      "60 117 Train Loss: 0.945 | Train Acc: 66.586% (5199/7808)\n",
      "90 117 Train Loss: 0.956 | Train Acc: 65.659% (7648/11648)\n",
      "117 Epoch: 35 | Train Loss: 0.962 | Train Acc: 65.658% (9833/14976)\n",
      "0 117 Test Loss: 1.354 | Test Acc: 58.594% (75/128)\n",
      "30 117 Test Loss: 1.430 | Test Acc: 52.445% (2081/3968)\n",
      "60 117 Test Loss: 1.441 | Test Acc: 51.921% (4054/7808)\n",
      "90 117 Test Loss: 1.433 | Test Acc: 52.215% (6082/11648)\n",
      "117 Epoch: 35 | Test Loss: 1.430 | Test Acc: 52.097% (7802/14976)\n",
      "\n",
      "Epoch: 36\n",
      "0 117 Train Loss: 0.875 | Train Acc: 67.969% (87/128)\n",
      "30 117 Train Loss: 0.872 | Train Acc: 70.111% (2782/3968)\n",
      "60 117 Train Loss: 0.891 | Train Acc: 69.006% (5388/7808)\n",
      "90 117 Train Loss: 0.904 | Train Acc: 68.578% (7988/11648)\n",
      "117 Epoch: 36 | Train Loss: 0.914 | Train Acc: 68.082% (10196/14976)\n",
      "0 117 Test Loss: 1.472 | Test Acc: 44.531% (57/128)\n",
      "30 117 Test Loss: 1.437 | Test Acc: 52.067% (2066/3968)\n",
      "60 117 Test Loss: 1.420 | Test Acc: 52.741% (4118/7808)\n",
      "90 117 Test Loss: 1.423 | Test Acc: 52.455% (6110/11648)\n",
      "117 Epoch: 36 | Test Loss: 1.422 | Test Acc: 52.664% (7887/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 37\n",
      "0 117 Train Loss: 0.777 | Train Acc: 74.219% (95/128)\n",
      "30 117 Train Loss: 0.814 | Train Acc: 71.522% (2838/3968)\n",
      "60 117 Train Loss: 0.840 | Train Acc: 70.492% (5504/7808)\n",
      "90 117 Train Loss: 0.850 | Train Acc: 69.918% (8144/11648)\n",
      "117 Epoch: 37 | Train Loss: 0.860 | Train Acc: 69.618% (10426/14976)\n",
      "0 117 Test Loss: 1.557 | Test Acc: 49.219% (63/128)\n",
      "30 117 Test Loss: 1.504 | Test Acc: 52.016% (2064/3968)\n",
      "60 117 Test Loss: 1.494 | Test Acc: 51.780% (4043/7808)\n",
      "90 117 Test Loss: 1.492 | Test Acc: 51.683% (6020/11648)\n",
      "117 Epoch: 37 | Test Loss: 1.488 | Test Acc: 51.716% (7745/14976)\n",
      "\n",
      "Epoch: 38\n",
      "0 117 Train Loss: 0.695 | Train Acc: 75.781% (97/128)\n",
      "30 117 Train Loss: 0.775 | Train Acc: 73.034% (2898/3968)\n",
      "60 117 Train Loss: 0.803 | Train Acc: 71.849% (5610/7808)\n",
      "90 117 Train Loss: 0.811 | Train Acc: 71.386% (8315/11648)\n",
      "117 Epoch: 38 | Train Loss: 0.829 | Train Acc: 70.686% (10586/14976)\n",
      "0 117 Test Loss: 1.409 | Test Acc: 50.000% (64/128)\n",
      "30 117 Test Loss: 1.504 | Test Acc: 50.781% (2015/3968)\n",
      "60 117 Test Loss: 1.513 | Test Acc: 50.730% (3961/7808)\n",
      "90 117 Test Loss: 1.501 | Test Acc: 50.970% (5937/11648)\n",
      "117 Epoch: 38 | Test Loss: 1.492 | Test Acc: 51.209% (7669/14976)\n",
      "\n",
      "Epoch: 39\n",
      "0 117 Train Loss: 0.662 | Train Acc: 81.250% (104/128)\n",
      "30 117 Train Loss: 0.754 | Train Acc: 73.790% (2928/3968)\n",
      "60 117 Train Loss: 0.766 | Train Acc: 73.258% (5720/7808)\n",
      "90 117 Train Loss: 0.771 | Train Acc: 73.051% (8509/11648)\n",
      "117 Epoch: 39 | Train Loss: 0.783 | Train Acc: 72.449% (10850/14976)\n",
      "0 117 Test Loss: 1.431 | Test Acc: 51.562% (66/128)\n",
      "30 117 Test Loss: 1.539 | Test Acc: 51.260% (2034/3968)\n",
      "60 117 Test Loss: 1.549 | Test Acc: 50.871% (3972/7808)\n",
      "90 117 Test Loss: 1.529 | Test Acc: 51.185% (5962/11648)\n",
      "117 Epoch: 39 | Test Loss: 1.518 | Test Acc: 51.576% (7724/14976)\n",
      "\n",
      "Epoch: 40\n",
      "0 117 Train Loss: 0.628 | Train Acc: 75.781% (97/128)\n",
      "30 117 Train Loss: 0.698 | Train Acc: 76.361% (3030/3968)\n",
      "60 117 Train Loss: 0.721 | Train Acc: 75.282% (5878/7808)\n",
      "90 117 Train Loss: 0.733 | Train Acc: 74.596% (8689/11648)\n",
      "117 Epoch: 40 | Train Loss: 0.741 | Train Acc: 74.145% (11104/14976)\n",
      "0 117 Test Loss: 1.402 | Test Acc: 52.344% (67/128)\n",
      "30 117 Test Loss: 1.507 | Test Acc: 51.789% (2055/3968)\n",
      "60 117 Test Loss: 1.497 | Test Acc: 52.472% (4097/7808)\n",
      "90 117 Test Loss: 1.507 | Test Acc: 52.292% (6091/11648)\n",
      "117 Epoch: 40 | Test Loss: 1.510 | Test Acc: 52.230% (7822/14976)\n",
      "\n",
      "Epoch: 41\n",
      "0 117 Train Loss: 0.511 | Train Acc: 83.594% (107/128)\n",
      "30 117 Train Loss: 0.647 | Train Acc: 79.057% (3137/3968)\n",
      "60 117 Train Loss: 0.655 | Train Acc: 77.933% (6085/7808)\n",
      "90 117 Train Loss: 0.664 | Train Acc: 77.206% (8993/11648)\n",
      "117 Epoch: 41 | Train Loss: 0.678 | Train Acc: 76.562% (11466/14976)\n",
      "0 117 Test Loss: 1.586 | Test Acc: 48.438% (62/128)\n",
      "30 117 Test Loss: 1.553 | Test Acc: 52.293% (2075/3968)\n",
      "60 117 Test Loss: 1.559 | Test Acc: 52.331% (4086/7808)\n",
      "90 117 Test Loss: 1.557 | Test Acc: 52.644% (6132/11648)\n",
      "117 Epoch: 41 | Test Loss: 1.547 | Test Acc: 52.711% (7894/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 42\n",
      "0 117 Train Loss: 0.700 | Train Acc: 73.438% (94/128)\n",
      "30 117 Train Loss: 0.600 | Train Acc: 78.982% (3134/3968)\n",
      "60 117 Train Loss: 0.617 | Train Acc: 78.202% (6106/7808)\n",
      "90 117 Train Loss: 0.617 | Train Acc: 78.365% (9128/11648)\n",
      "117 Epoch: 42 | Train Loss: 0.625 | Train Acc: 78.058% (11690/14976)\n",
      "0 117 Test Loss: 1.803 | Test Acc: 53.125% (68/128)\n",
      "30 117 Test Loss: 1.641 | Test Acc: 51.613% (2048/3968)\n",
      "60 117 Test Loss: 1.635 | Test Acc: 51.345% (4009/7808)\n",
      "90 117 Test Loss: 1.636 | Test Acc: 51.331% (5979/11648)\n",
      "117 Epoch: 42 | Test Loss: 1.636 | Test Acc: 51.429% (7702/14976)\n",
      "\n",
      "Epoch: 43\n",
      "0 117 Train Loss: 0.417 | Train Acc: 89.062% (114/128)\n",
      "30 117 Train Loss: 0.524 | Train Acc: 82.334% (3267/3968)\n",
      "60 117 Train Loss: 0.547 | Train Acc: 81.532% (6366/7808)\n",
      "90 117 Train Loss: 0.567 | Train Acc: 80.623% (9391/11648)\n",
      "117 Epoch: 43 | Train Loss: 0.584 | Train Acc: 79.894% (11965/14976)\n",
      "0 117 Test Loss: 1.785 | Test Acc: 47.656% (61/128)\n",
      "30 117 Test Loss: 1.709 | Test Acc: 50.504% (2004/3968)\n",
      "60 117 Test Loss: 1.719 | Test Acc: 50.320% (3929/7808)\n",
      "90 117 Test Loss: 1.714 | Test Acc: 50.575% (5891/11648)\n",
      "117 Epoch: 43 | Test Loss: 1.709 | Test Acc: 50.541% (7569/14976)\n",
      "\n",
      "Epoch: 44\n",
      "0 117 Train Loss: 0.604 | Train Acc: 77.344% (99/128)\n",
      "30 117 Train Loss: 0.516 | Train Acc: 82.283% (3265/3968)\n",
      "60 117 Train Loss: 0.509 | Train Acc: 82.825% (6467/7808)\n",
      "90 117 Train Loss: 0.513 | Train Acc: 82.855% (9651/11648)\n",
      "117 Epoch: 44 | Train Loss: 0.523 | Train Acc: 82.252% (12318/14976)\n",
      "0 117 Test Loss: 1.267 | Test Acc: 64.062% (82/128)\n",
      "30 117 Test Loss: 1.676 | Test Acc: 51.915% (2060/3968)\n",
      "60 117 Test Loss: 1.681 | Test Acc: 51.908% (4053/7808)\n",
      "90 117 Test Loss: 1.685 | Test Acc: 51.932% (6049/11648)\n",
      "117 Epoch: 44 | Test Loss: 1.710 | Test Acc: 51.583% (7725/14976)\n",
      "\n",
      "Epoch: 45\n",
      "0 117 Train Loss: 0.439 | Train Acc: 82.812% (106/128)\n",
      "30 117 Train Loss: 0.426 | Train Acc: 86.442% (3430/3968)\n",
      "60 117 Train Loss: 0.444 | Train Acc: 85.336% (6663/7808)\n",
      "90 117 Train Loss: 0.459 | Train Acc: 84.761% (9873/11648)\n",
      "117 Epoch: 45 | Train Loss: 0.473 | Train Acc: 84.181% (12607/14976)\n",
      "0 117 Test Loss: 1.777 | Test Acc: 53.125% (68/128)\n",
      "30 117 Test Loss: 1.753 | Test Acc: 51.512% (2044/3968)\n",
      "60 117 Test Loss: 1.746 | Test Acc: 51.895% (4052/7808)\n",
      "90 117 Test Loss: 1.739 | Test Acc: 52.172% (6077/11648)\n",
      "117 Epoch: 45 | Test Loss: 1.744 | Test Acc: 52.257% (7826/14976)\n",
      "\n",
      "Epoch: 46\n",
      "0 117 Train Loss: 0.358 | Train Acc: 90.625% (116/128)\n",
      "30 117 Train Loss: 0.391 | Train Acc: 87.525% (3473/3968)\n",
      "60 117 Train Loss: 0.395 | Train Acc: 87.590% (6839/7808)\n",
      "90 117 Train Loss: 0.404 | Train Acc: 87.096% (10145/11648)\n",
      "117 Epoch: 46 | Train Loss: 0.412 | Train Acc: 86.619% (12972/14976)\n",
      "0 117 Test Loss: 1.725 | Test Acc: 53.906% (69/128)\n",
      "30 117 Test Loss: 1.821 | Test Acc: 51.890% (2059/3968)\n",
      "60 117 Test Loss: 1.870 | Test Acc: 51.729% (4039/7808)\n",
      "90 117 Test Loss: 1.841 | Test Acc: 52.112% (6070/11648)\n",
      "117 Epoch: 46 | Test Loss: 1.839 | Test Acc: 52.197% (7817/14976)\n",
      "\n",
      "Epoch: 47\n",
      "0 117 Train Loss: 0.294 | Train Acc: 92.188% (118/128)\n",
      "30 117 Train Loss: 0.328 | Train Acc: 89.718% (3560/3968)\n",
      "60 117 Train Loss: 0.358 | Train Acc: 88.397% (6902/7808)\n",
      "90 117 Train Loss: 0.364 | Train Acc: 88.264% (10281/11648)\n",
      "117 Epoch: 47 | Train Loss: 0.369 | Train Acc: 88.048% (13186/14976)\n",
      "0 117 Test Loss: 1.703 | Test Acc: 50.781% (65/128)\n",
      "30 117 Test Loss: 1.846 | Test Acc: 51.865% (2058/3968)\n",
      "60 117 Test Loss: 1.841 | Test Acc: 51.819% (4046/7808)\n",
      "90 117 Test Loss: 1.844 | Test Acc: 51.811% (6035/11648)\n",
      "117 Epoch: 47 | Test Loss: 1.861 | Test Acc: 51.616% (7730/14976)\n",
      "\n",
      "Epoch: 48\n",
      "0 117 Train Loss: 0.315 | Train Acc: 89.844% (115/128)\n",
      "30 117 Train Loss: 0.262 | Train Acc: 92.540% (3672/3968)\n",
      "60 117 Train Loss: 0.272 | Train Acc: 91.970% (7181/7808)\n",
      "90 117 Train Loss: 0.281 | Train Acc: 91.569% (10666/11648)\n",
      "117 Epoch: 48 | Train Loss: 0.289 | Train Acc: 91.219% (13661/14976)\n",
      "0 117 Test Loss: 3.008 | Test Acc: 39.844% (51/128)\n",
      "30 117 Test Loss: 1.990 | Test Acc: 51.033% (2025/3968)\n",
      "60 117 Test Loss: 1.979 | Test Acc: 50.871% (3972/7808)\n",
      "90 117 Test Loss: 1.950 | Test Acc: 51.133% (5956/11648)\n",
      "117 Epoch: 48 | Test Loss: 1.939 | Test Acc: 51.382% (7695/14976)\n",
      "\n",
      "Epoch: 49\n",
      "0 117 Train Loss: 0.196 | Train Acc: 94.531% (121/128)\n",
      "30 117 Train Loss: 0.242 | Train Acc: 93.196% (3698/3968)\n",
      "60 117 Train Loss: 0.250 | Train Acc: 92.866% (7251/7808)\n",
      "90 117 Train Loss: 0.258 | Train Acc: 92.368% (10759/11648)\n",
      "117 Epoch: 49 | Train Loss: 0.265 | Train Acc: 91.867% (13758/14976)\n",
      "0 117 Test Loss: 1.856 | Test Acc: 47.656% (61/128)\n",
      "30 117 Test Loss: 2.013 | Test Acc: 50.428% (2001/3968)\n",
      "60 117 Test Loss: 1.958 | Test Acc: 51.383% (4012/7808)\n",
      "90 117 Test Loss: 1.987 | Test Acc: 51.056% (5947/11648)\n",
      "117 Epoch: 49 | Test Loss: 1.984 | Test Acc: 51.302% (7683/14976)\n",
      "\n",
      "Epoch: 50\n",
      "0 117 Train Loss: 0.232 | Train Acc: 94.531% (121/128)\n",
      "30 117 Train Loss: 0.203 | Train Acc: 94.481% (3749/3968)\n",
      "60 117 Train Loss: 0.205 | Train Acc: 94.390% (7370/7808)\n",
      "90 117 Train Loss: 0.201 | Train Acc: 94.497% (11007/11648)\n",
      "117 Epoch: 50 | Train Loss: 0.209 | Train Acc: 94.124% (14096/14976)\n",
      "0 117 Test Loss: 2.060 | Test Acc: 47.656% (61/128)\n",
      "30 117 Test Loss: 2.115 | Test Acc: 49.924% (1981/3968)\n",
      "60 117 Test Loss: 2.114 | Test Acc: 50.333% (3930/7808)\n",
      "90 117 Test Loss: 2.103 | Test Acc: 50.704% (5906/11648)\n",
      "117 Epoch: 50 | Test Loss: 2.103 | Test Acc: 50.921% (7626/14976)\n",
      "\n",
      "Epoch: 51\n",
      "0 117 Train Loss: 0.177 | Train Acc: 93.750% (120/128)\n",
      "30 117 Train Loss: 0.178 | Train Acc: 95.086% (3773/3968)\n",
      "60 117 Train Loss: 0.169 | Train Acc: 95.633% (7467/7808)\n",
      "90 117 Train Loss: 0.170 | Train Acc: 95.613% (11137/11648)\n",
      "117 Epoch: 51 | Train Loss: 0.169 | Train Acc: 95.593% (14316/14976)\n",
      "0 117 Test Loss: 2.512 | Test Acc: 47.656% (61/128)\n",
      "30 117 Test Loss: 2.151 | Test Acc: 51.789% (2055/3968)\n",
      "60 117 Test Loss: 2.104 | Test Acc: 52.088% (4067/7808)\n",
      "90 117 Test Loss: 2.100 | Test Acc: 52.344% (6097/11648)\n",
      "117 Epoch: 51 | Test Loss: 2.112 | Test Acc: 52.297% (7832/14976)\n",
      "\n",
      "Epoch: 52\n",
      "0 117 Train Loss: 0.118 | Train Acc: 96.094% (123/128)\n",
      "30 117 Train Loss: 0.122 | Train Acc: 97.051% (3851/3968)\n",
      "60 117 Train Loss: 0.121 | Train Acc: 97.234% (7592/7808)\n",
      "90 117 Train Loss: 0.122 | Train Acc: 97.175% (11319/11648)\n",
      "117 Epoch: 52 | Train Loss: 0.126 | Train Acc: 96.968% (14522/14976)\n",
      "0 117 Test Loss: 1.703 | Test Acc: 56.250% (72/128)\n",
      "30 117 Test Loss: 2.199 | Test Acc: 51.058% (2026/3968)\n",
      "60 117 Test Loss: 2.164 | Test Acc: 52.510% (4100/7808)\n",
      "90 117 Test Loss: 2.154 | Test Acc: 52.601% (6127/11648)\n",
      "117 Epoch: 52 | Test Loss: 2.147 | Test Acc: 52.764% (7902/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 53\n",
      "0 117 Train Loss: 0.061 | Train Acc: 99.219% (127/128)\n",
      "30 117 Train Loss: 0.107 | Train Acc: 97.581% (3872/3968)\n",
      "60 117 Train Loss: 0.099 | Train Acc: 97.989% (7651/7808)\n",
      "90 117 Train Loss: 0.095 | Train Acc: 98.214% (11440/11648)\n",
      "117 Epoch: 53 | Train Loss: 0.094 | Train Acc: 98.224% (14710/14976)\n",
      "0 117 Test Loss: 2.375 | Test Acc: 53.125% (68/128)\n",
      "30 117 Test Loss: 2.087 | Test Acc: 53.175% (2110/3968)\n",
      "60 117 Test Loss: 2.132 | Test Acc: 53.240% (4157/7808)\n",
      "90 117 Test Loss: 2.150 | Test Acc: 53.091% (6184/11648)\n",
      "117 Epoch: 53 | Test Loss: 2.173 | Test Acc: 52.804% (7908/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 54\n",
      "0 117 Train Loss: 0.050 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.058 | Train Acc: 99.320% (3941/3968)\n",
      "60 117 Train Loss: 0.056 | Train Acc: 99.398% (7761/7808)\n",
      "90 117 Train Loss: 0.058 | Train Acc: 99.305% (11567/11648)\n",
      "117 Epoch: 54 | Train Loss: 0.059 | Train Acc: 99.239% (14862/14976)\n",
      "0 117 Test Loss: 1.918 | Test Acc: 62.500% (80/128)\n",
      "30 117 Test Loss: 2.267 | Test Acc: 52.117% (2068/3968)\n",
      "60 117 Test Loss: 2.292 | Test Acc: 51.960% (4057/7808)\n",
      "90 117 Test Loss: 2.262 | Test Acc: 52.344% (6097/11648)\n",
      "117 Epoch: 54 | Test Loss: 2.249 | Test Acc: 52.350% (7840/14976)\n",
      "\n",
      "Epoch: 55\n",
      "0 117 Train Loss: 0.049 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.043 | Train Acc: 99.723% (3957/3968)\n",
      "60 117 Train Loss: 0.041 | Train Acc: 99.821% (7794/7808)\n",
      "90 117 Train Loss: 0.040 | Train Acc: 99.828% (11628/11648)\n",
      "117 Epoch: 55 | Train Loss: 0.040 | Train Acc: 99.820% (14949/14976)\n",
      "0 117 Test Loss: 2.449 | Test Acc: 51.562% (66/128)\n",
      "30 117 Test Loss: 2.335 | Test Acc: 51.789% (2055/3968)\n",
      "60 117 Test Loss: 2.319 | Test Acc: 52.433% (4094/7808)\n",
      "90 117 Test Loss: 2.291 | Test Acc: 52.747% (6144/11648)\n",
      "117 Epoch: 55 | Test Loss: 2.272 | Test Acc: 53.018% (7940/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 56\n",
      "0 117 Train Loss: 0.034 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.027 | Train Acc: 99.975% (3967/3968)\n",
      "60 117 Train Loss: 0.027 | Train Acc: 99.936% (7803/7808)\n",
      "90 117 Train Loss: 0.027 | Train Acc: 99.957% (11643/11648)\n",
      "117 Epoch: 56 | Train Loss: 0.027 | Train Acc: 99.927% (14965/14976)\n",
      "0 117 Test Loss: 2.532 | Test Acc: 47.656% (61/128)\n",
      "30 117 Test Loss: 2.269 | Test Acc: 53.856% (2137/3968)\n",
      "60 117 Test Loss: 2.319 | Test Acc: 53.291% (4161/7808)\n",
      "90 117 Test Loss: 2.311 | Test Acc: 53.580% (6241/11648)\n",
      "117 Epoch: 56 | Test Loss: 2.311 | Test Acc: 53.218% (7970/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 57\n",
      "0 117 Train Loss: 0.023 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.021 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.021 | Train Acc: 99.974% (7806/7808)\n",
      "90 117 Train Loss: 0.021 | Train Acc: 99.983% (11646/11648)\n",
      "117 Epoch: 57 | Train Loss: 0.021 | Train Acc: 99.987% (14974/14976)\n",
      "0 117 Test Loss: 2.272 | Test Acc: 58.594% (75/128)\n",
      "30 117 Test Loss: 2.326 | Test Acc: 53.705% (2131/3968)\n",
      "60 117 Test Loss: 2.348 | Test Acc: 53.368% (4167/7808)\n",
      "90 117 Test Loss: 2.335 | Test Acc: 53.116% (6187/11648)\n",
      "117 Epoch: 57 | Test Loss: 2.327 | Test Acc: 53.292% (7981/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 58\n",
      "0 117 Train Loss: 0.016 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.017 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.017 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.017 | Train Acc: 99.991% (11647/11648)\n",
      "117 Epoch: 58 | Train Loss: 0.017 | Train Acc: 99.987% (14974/14976)\n",
      "0 117 Test Loss: 2.431 | Test Acc: 50.000% (64/128)\n",
      "30 117 Test Loss: 2.431 | Test Acc: 52.369% (2078/3968)\n",
      "60 117 Test Loss: 2.371 | Test Acc: 53.023% (4140/7808)\n",
      "90 117 Test Loss: 2.337 | Test Acc: 53.314% (6210/11648)\n",
      "117 Epoch: 58 | Test Loss: 2.338 | Test Acc: 53.446% (8004/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 59\n",
      "0 117 Train Loss: 0.012 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.014 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.014 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.014 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 59 | Train Loss: 0.015 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.327 | Test Acc: 56.250% (72/128)\n",
      "30 117 Test Loss: 2.382 | Test Acc: 53.982% (2142/3968)\n",
      "60 117 Test Loss: 2.373 | Test Acc: 53.714% (4194/7808)\n",
      "90 117 Test Loss: 2.383 | Test Acc: 53.056% (6180/11648)\n",
      "117 Epoch: 59 | Test Loss: 2.357 | Test Acc: 53.425% (8001/14976)\n",
      "\n",
      "Epoch: 60\n",
      "0 117 Train Loss: 0.012 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.012 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.012 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.013 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 60 | Train Loss: 0.013 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.938 | Test Acc: 46.875% (60/128)\n",
      "30 117 Test Loss: 2.336 | Test Acc: 53.453% (2121/3968)\n",
      "60 117 Test Loss: 2.344 | Test Acc: 53.112% (4147/7808)\n",
      "90 117 Test Loss: 2.370 | Test Acc: 53.400% (6220/11648)\n",
      "117 Epoch: 60 | Test Loss: 2.365 | Test Acc: 53.412% (7999/14976)\n",
      "\n",
      "Epoch: 61\n",
      "0 117 Train Loss: 0.012 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.012 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.012 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.012 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 61 | Train Loss: 0.012 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.245 | Test Acc: 57.812% (74/128)\n",
      "30 117 Test Loss: 2.314 | Test Acc: 54.410% (2159/3968)\n",
      "60 117 Test Loss: 2.364 | Test Acc: 54.367% (4245/7808)\n",
      "90 117 Test Loss: 2.398 | Test Acc: 53.674% (6252/11648)\n",
      "117 Epoch: 61 | Test Loss: 2.381 | Test Acc: 53.673% (8038/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 62\n",
      "0 117 Train Loss: 0.009 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.011 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.011 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.011 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 62 | Train Loss: 0.011 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.709 | Test Acc: 53.906% (69/128)\n",
      "30 117 Test Loss: 2.392 | Test Acc: 52.797% (2095/3968)\n",
      "60 117 Test Loss: 2.363 | Test Acc: 53.330% (4164/7808)\n",
      "90 117 Test Loss: 2.383 | Test Acc: 53.305% (6209/11648)\n",
      "117 Epoch: 62 | Test Loss: 2.381 | Test Acc: 53.446% (8004/14976)\n",
      "\n",
      "Epoch: 63\n",
      "0 117 Train Loss: 0.011 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.010 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.010 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.011 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 63 | Train Loss: 0.011 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.572 | Test Acc: 53.125% (68/128)\n",
      "30 117 Test Loss: 2.393 | Test Acc: 54.209% (2151/3968)\n",
      "60 117 Test Loss: 2.426 | Test Acc: 53.240% (4157/7808)\n",
      "90 117 Test Loss: 2.401 | Test Acc: 53.486% (6230/11648)\n",
      "117 Epoch: 63 | Test Loss: 2.394 | Test Acc: 53.606% (8028/14976)\n",
      "\n",
      "Epoch: 64\n",
      "0 117 Train Loss: 0.011 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.010 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.010 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.010 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 64 | Train Loss: 0.010 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.133 | Test Acc: 57.812% (74/128)\n",
      "30 117 Test Loss: 2.346 | Test Acc: 54.461% (2161/3968)\n",
      "60 117 Test Loss: 2.359 | Test Acc: 53.919% (4210/7808)\n",
      "90 117 Test Loss: 2.379 | Test Acc: 53.692% (6254/11648)\n",
      "117 Epoch: 64 | Test Loss: 2.402 | Test Acc: 53.552% (8020/14976)\n",
      "\n",
      "Epoch: 65\n",
      "0 117 Train Loss: 0.011 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.010 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.010 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.010 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 65 | Train Loss: 0.010 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.303 | Test Acc: 57.031% (73/128)\n",
      "30 117 Test Loss: 2.437 | Test Acc: 52.848% (2097/3968)\n",
      "60 117 Test Loss: 2.392 | Test Acc: 53.176% (4152/7808)\n",
      "90 117 Test Loss: 2.402 | Test Acc: 53.099% (6185/11648)\n",
      "117 Epoch: 65 | Test Loss: 2.407 | Test Acc: 53.339% (7988/14976)\n",
      "\n",
      "Epoch: 66\n",
      "0 117 Train Loss: 0.009 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.009 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.009 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.010 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 66 | Train Loss: 0.010 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.425 | Test Acc: 52.344% (67/128)\n",
      "30 117 Test Loss: 2.379 | Test Acc: 53.982% (2142/3968)\n",
      "60 117 Test Loss: 2.410 | Test Acc: 53.791% (4200/7808)\n",
      "90 117 Test Loss: 2.403 | Test Acc: 53.786% (6265/11648)\n",
      "117 Epoch: 66 | Test Loss: 2.407 | Test Acc: 53.706% (8043/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 67\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.009 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.009 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.009 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 67 | Train Loss: 0.009 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.384 | Test Acc: 53.906% (69/128)\n",
      "30 117 Test Loss: 2.378 | Test Acc: 53.125% (2108/3968)\n",
      "60 117 Test Loss: 2.416 | Test Acc: 53.010% (4139/7808)\n",
      "90 117 Test Loss: 2.420 | Test Acc: 53.374% (6217/11648)\n",
      "117 Epoch: 67 | Test Loss: 2.409 | Test Acc: 53.519% (8015/14976)\n",
      "\n",
      "Epoch: 68\n",
      "0 117 Train Loss: 0.010 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.009 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.009 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 68 | Train Loss: 0.009 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.687 | Test Acc: 50.000% (64/128)\n",
      "30 117 Test Loss: 2.404 | Test Acc: 53.629% (2128/3968)\n",
      "60 117 Test Loss: 2.365 | Test Acc: 53.945% (4212/7808)\n",
      "90 117 Test Loss: 2.376 | Test Acc: 53.606% (6244/11648)\n",
      "117 Epoch: 68 | Test Loss: 2.407 | Test Acc: 53.579% (8024/14976)\n",
      "\n",
      "Epoch: 69\n",
      "0 117 Train Loss: 0.008 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.009 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.009 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.009 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 69 | Train Loss: 0.009 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.113 | Test Acc: 59.375% (76/128)\n",
      "30 117 Test Loss: 2.470 | Test Acc: 52.545% (2085/3968)\n",
      "60 117 Test Loss: 2.453 | Test Acc: 52.843% (4126/7808)\n",
      "90 117 Test Loss: 2.427 | Test Acc: 53.297% (6208/11648)\n",
      "117 Epoch: 69 | Test Loss: 2.409 | Test Acc: 53.379% (7994/14976)\n",
      "\n",
      "Epoch: 70\n",
      "0 117 Train Loss: 0.008 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.009 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.009 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 70 | Train Loss: 0.009 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.796 | Test Acc: 53.125% (68/128)\n",
      "30 117 Test Loss: 2.371 | Test Acc: 54.410% (2159/3968)\n",
      "60 117 Test Loss: 2.404 | Test Acc: 53.996% (4216/7808)\n",
      "90 117 Test Loss: 2.409 | Test Acc: 53.692% (6254/11648)\n",
      "117 Epoch: 70 | Test Loss: 2.413 | Test Acc: 53.586% (8025/14976)\n",
      "\n",
      "Epoch: 71\n",
      "0 117 Train Loss: 0.008 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 71 | Train Loss: 0.009 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.746 | Test Acc: 48.438% (62/128)\n",
      "30 117 Test Loss: 2.496 | Test Acc: 52.898% (2099/3968)\n",
      "60 117 Test Loss: 2.455 | Test Acc: 53.253% (4158/7808)\n",
      "90 117 Test Loss: 2.430 | Test Acc: 53.434% (6224/11648)\n",
      "117 Epoch: 71 | Test Loss: 2.416 | Test Acc: 53.666% (8037/14976)\n",
      "\n",
      "Epoch: 72\n",
      "0 117 Train Loss: 0.009 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 72 | Train Loss: 0.009 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.434 | Test Acc: 52.344% (67/128)\n",
      "30 117 Test Loss: 2.431 | Test Acc: 53.402% (2119/3968)\n",
      "60 117 Test Loss: 2.416 | Test Acc: 53.227% (4156/7808)\n",
      "90 117 Test Loss: 2.403 | Test Acc: 53.383% (6218/11648)\n",
      "117 Epoch: 72 | Test Loss: 2.409 | Test Acc: 53.459% (8006/14976)\n",
      "\n",
      "Epoch: 73\n",
      "0 117 Train Loss: 0.009 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 73 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.701 | Test Acc: 49.219% (63/128)\n",
      "30 117 Test Loss: 2.429 | Test Acc: 53.427% (2120/3968)\n",
      "60 117 Test Loss: 2.410 | Test Acc: 53.304% (4162/7808)\n",
      "90 117 Test Loss: 2.409 | Test Acc: 53.546% (6237/11648)\n",
      "117 Epoch: 73 | Test Loss: 2.414 | Test Acc: 53.499% (8012/14976)\n",
      "\n",
      "Epoch: 74\n",
      "0 117 Train Loss: 0.008 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 74 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.513 | Test Acc: 54.688% (70/128)\n",
      "30 117 Test Loss: 2.483 | Test Acc: 52.218% (2072/3968)\n",
      "60 117 Test Loss: 2.453 | Test Acc: 53.432% (4172/7808)\n",
      "90 117 Test Loss: 2.424 | Test Acc: 53.657% (6250/11648)\n",
      "117 Epoch: 74 | Test Loss: 2.410 | Test Acc: 53.766% (8052/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 75\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 75 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.317 | Test Acc: 55.469% (71/128)\n",
      "30 117 Test Loss: 2.396 | Test Acc: 53.931% (2140/3968)\n",
      "60 117 Test Loss: 2.407 | Test Acc: 53.573% (4183/7808)\n",
      "90 117 Test Loss: 2.420 | Test Acc: 53.477% (6229/11648)\n",
      "117 Epoch: 75 | Test Loss: 2.409 | Test Acc: 53.466% (8007/14976)\n",
      "\n",
      "Epoch: 76\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 76 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.013 | Test Acc: 60.938% (78/128)\n",
      "30 117 Test Loss: 2.467 | Test Acc: 53.049% (2105/3968)\n",
      "60 117 Test Loss: 2.444 | Test Acc: 53.087% (4145/7808)\n",
      "90 117 Test Loss: 2.444 | Test Acc: 53.013% (6175/11648)\n",
      "117 Epoch: 76 | Test Loss: 2.411 | Test Acc: 53.372% (7993/14976)\n",
      "\n",
      "Epoch: 77\n",
      "0 117 Train Loss: 0.008 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 77 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.394 | Test Acc: 52.344% (67/128)\n",
      "30 117 Test Loss: 2.370 | Test Acc: 53.629% (2128/3968)\n",
      "60 117 Test Loss: 2.391 | Test Acc: 53.599% (4185/7808)\n",
      "90 117 Test Loss: 2.399 | Test Acc: 53.460% (6227/11648)\n",
      "117 Epoch: 77 | Test Loss: 2.409 | Test Acc: 53.539% (8018/14976)\n",
      "\n",
      "Epoch: 78\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 78 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 1.888 | Test Acc: 64.844% (83/128)\n",
      "30 117 Test Loss: 2.425 | Test Acc: 53.075% (2106/3968)\n",
      "60 117 Test Loss: 2.400 | Test Acc: 53.496% (4177/7808)\n",
      "90 117 Test Loss: 2.385 | Test Acc: 53.692% (6254/11648)\n",
      "117 Epoch: 78 | Test Loss: 2.408 | Test Acc: 53.479% (8009/14976)\n",
      "\n",
      "Epoch: 79\n",
      "0 117 Train Loss: 0.008 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 79 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.592 | Test Acc: 51.562% (66/128)\n",
      "30 117 Test Loss: 2.401 | Test Acc: 53.780% (2134/3968)\n",
      "60 117 Test Loss: 2.410 | Test Acc: 53.420% (4171/7808)\n",
      "90 117 Test Loss: 2.419 | Test Acc: 53.614% (6245/11648)\n",
      "117 Epoch: 79 | Test Loss: 2.407 | Test Acc: 53.653% (8035/14976)\n",
      "\n",
      "Epoch: 80\n",
      "0 117 Train Loss: 0.009 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 80 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.507 | Test Acc: 53.906% (69/128)\n",
      "30 117 Test Loss: 2.348 | Test Acc: 53.503% (2123/3968)\n",
      "60 117 Test Loss: 2.389 | Test Acc: 53.727% (4195/7808)\n",
      "90 117 Test Loss: 2.367 | Test Acc: 53.820% (6269/11648)\n",
      "117 Epoch: 80 | Test Loss: 2.400 | Test Acc: 53.459% (8006/14976)\n",
      "\n",
      "Epoch: 81\n",
      "0 117 Train Loss: 0.008 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 81 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.733 | Test Acc: 46.875% (60/128)\n",
      "30 117 Test Loss: 2.436 | Test Acc: 52.873% (2098/3968)\n",
      "60 117 Test Loss: 2.390 | Test Acc: 53.624% (4187/7808)\n",
      "90 117 Test Loss: 2.385 | Test Acc: 53.657% (6250/11648)\n",
      "117 Epoch: 81 | Test Loss: 2.397 | Test Acc: 53.686% (8040/14976)\n",
      "\n",
      "Epoch: 82\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 82 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.765 | Test Acc: 52.344% (67/128)\n",
      "30 117 Test Loss: 2.451 | Test Acc: 53.780% (2134/3968)\n",
      "60 117 Test Loss: 2.467 | Test Acc: 53.240% (4157/7808)\n",
      "90 117 Test Loss: 2.428 | Test Acc: 53.477% (6229/11648)\n",
      "117 Epoch: 82 | Test Loss: 2.398 | Test Acc: 53.546% (8019/14976)\n",
      "\n",
      "Epoch: 83\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.008 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 83 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.324 | Test Acc: 55.469% (71/128)\n",
      "30 117 Test Loss: 2.493 | Test Acc: 52.268% (2074/3968)\n",
      "60 117 Test Loss: 2.392 | Test Acc: 53.445% (4173/7808)\n",
      "90 117 Test Loss: 2.414 | Test Acc: 53.314% (6210/11648)\n",
      "117 Epoch: 83 | Test Loss: 2.393 | Test Acc: 53.486% (8010/14976)\n",
      "\n",
      "Epoch: 84\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 84 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.157 | Test Acc: 53.906% (69/128)\n",
      "30 117 Test Loss: 2.293 | Test Acc: 54.561% (2165/3968)\n",
      "60 117 Test Loss: 2.372 | Test Acc: 53.458% (4174/7808)\n",
      "90 117 Test Loss: 2.396 | Test Acc: 53.451% (6226/11648)\n",
      "117 Epoch: 84 | Test Loss: 2.392 | Test Acc: 53.546% (8019/14976)\n",
      "\n",
      "Epoch: 85\n",
      "0 117 Train Loss: 0.006 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.008 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 85 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.044 | Test Acc: 60.156% (77/128)\n",
      "30 117 Test Loss: 2.379 | Test Acc: 54.486% (2162/3968)\n",
      "60 117 Test Loss: 2.370 | Test Acc: 53.496% (4177/7808)\n",
      "90 117 Test Loss: 2.376 | Test Acc: 53.674% (6252/11648)\n",
      "117 Epoch: 85 | Test Loss: 2.391 | Test Acc: 53.439% (8003/14976)\n",
      "\n",
      "Epoch: 86\n",
      "0 117 Train Loss: 0.006 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.007 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 86 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.858 | Test Acc: 47.656% (61/128)\n",
      "30 117 Test Loss: 2.397 | Test Acc: 53.604% (2127/3968)\n",
      "60 117 Test Loss: 2.383 | Test Acc: 53.624% (4187/7808)\n",
      "90 117 Test Loss: 2.396 | Test Acc: 53.589% (6242/11648)\n",
      "117 Epoch: 86 | Test Loss: 2.392 | Test Acc: 53.539% (8018/14976)\n",
      "\n",
      "Epoch: 87\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.007 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 87 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.729 | Test Acc: 48.438% (62/128)\n",
      "30 117 Test Loss: 2.348 | Test Acc: 53.553% (2125/3968)\n",
      "60 117 Test Loss: 2.374 | Test Acc: 53.637% (4188/7808)\n",
      "90 117 Test Loss: 2.390 | Test Acc: 53.657% (6250/11648)\n",
      "117 Epoch: 87 | Test Loss: 2.394 | Test Acc: 53.579% (8024/14976)\n",
      "\n",
      "Epoch: 88\n",
      "0 117 Train Loss: 0.006 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 88 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.459 | Test Acc: 57.031% (73/128)\n",
      "30 117 Test Loss: 2.422 | Test Acc: 52.596% (2087/3968)\n",
      "60 117 Test Loss: 2.366 | Test Acc: 53.791% (4200/7808)\n",
      "90 117 Test Loss: 2.374 | Test Acc: 53.700% (6255/11648)\n",
      "117 Epoch: 88 | Test Loss: 2.390 | Test Acc: 53.579% (8024/14976)\n",
      "\n",
      "Epoch: 89\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.008 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 89 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.182 | Test Acc: 55.469% (71/128)\n",
      "30 117 Test Loss: 2.427 | Test Acc: 53.125% (2108/3968)\n",
      "60 117 Test Loss: 2.390 | Test Acc: 53.701% (4193/7808)\n",
      "90 117 Test Loss: 2.396 | Test Acc: 53.666% (6251/11648)\n",
      "117 Epoch: 89 | Test Loss: 2.384 | Test Acc: 53.519% (8015/14976)\n",
      "\n",
      "Epoch: 90\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.007 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 90 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.233 | Test Acc: 50.000% (64/128)\n",
      "30 117 Test Loss: 2.356 | Test Acc: 53.427% (2120/3968)\n",
      "60 117 Test Loss: 2.359 | Test Acc: 53.676% (4191/7808)\n",
      "90 117 Test Loss: 2.372 | Test Acc: 53.374% (6217/11648)\n",
      "117 Epoch: 90 | Test Loss: 2.382 | Test Acc: 53.459% (8006/14976)\n",
      "\n",
      "Epoch: 91\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.007 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 91 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 1.658 | Test Acc: 64.844% (83/128)\n",
      "30 117 Test Loss: 2.353 | Test Acc: 53.679% (2130/3968)\n",
      "60 117 Test Loss: 2.385 | Test Acc: 53.829% (4203/7808)\n",
      "90 117 Test Loss: 2.361 | Test Acc: 54.044% (6295/11648)\n",
      "117 Epoch: 91 | Test Loss: 2.382 | Test Acc: 53.552% (8020/14976)\n",
      "\n",
      "Epoch: 92\n",
      "0 117 Train Loss: 0.006 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.007 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 92 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.309 | Test Acc: 53.906% (69/128)\n",
      "30 117 Test Loss: 2.305 | Test Acc: 53.831% (2136/3968)\n",
      "60 117 Test Loss: 2.352 | Test Acc: 53.381% (4168/7808)\n",
      "90 117 Test Loss: 2.383 | Test Acc: 53.305% (6209/11648)\n",
      "117 Epoch: 92 | Test Loss: 2.379 | Test Acc: 53.459% (8006/14976)\n",
      "\n",
      "Epoch: 93\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.007 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 93 | Train Loss: 0.007 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.709 | Test Acc: 50.781% (65/128)\n",
      "30 117 Test Loss: 2.341 | Test Acc: 54.158% (2149/3968)\n",
      "60 117 Test Loss: 2.379 | Test Acc: 53.586% (4184/7808)\n",
      "90 117 Test Loss: 2.398 | Test Acc: 53.674% (6252/11648)\n",
      "117 Epoch: 93 | Test Loss: 2.378 | Test Acc: 53.486% (8010/14976)\n",
      "\n",
      "Epoch: 94\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.007 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 94 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.521 | Test Acc: 53.125% (68/128)\n",
      "30 117 Test Loss: 2.383 | Test Acc: 53.957% (2141/3968)\n",
      "60 117 Test Loss: 2.336 | Test Acc: 53.893% (4208/7808)\n",
      "90 117 Test Loss: 2.370 | Test Acc: 53.503% (6232/11648)\n",
      "117 Epoch: 94 | Test Loss: 2.380 | Test Acc: 53.579% (8024/14976)\n",
      "\n",
      "Epoch: 95\n",
      "0 117 Train Loss: 0.008 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.007 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 95 | Train Loss: 0.007 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.430 | Test Acc: 54.688% (70/128)\n",
      "30 117 Test Loss: 2.394 | Test Acc: 54.183% (2150/3968)\n",
      "60 117 Test Loss: 2.417 | Test Acc: 53.317% (4163/7808)\n",
      "90 117 Test Loss: 2.398 | Test Acc: 53.606% (6244/11648)\n",
      "117 Epoch: 95 | Test Loss: 2.377 | Test Acc: 53.579% (8024/14976)\n",
      "\n",
      "Epoch: 96\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.007 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 96 | Train Loss: 0.008 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 1.951 | Test Acc: 60.156% (77/128)\n",
      "30 117 Test Loss: 2.305 | Test Acc: 55.166% (2189/3968)\n",
      "60 117 Test Loss: 2.370 | Test Acc: 54.060% (4221/7808)\n",
      "90 117 Test Loss: 2.376 | Test Acc: 53.649% (6249/11648)\n",
      "117 Epoch: 96 | Test Loss: 2.370 | Test Acc: 53.619% (8030/14976)\n",
      "\n",
      "Epoch: 97\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.007 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 97 | Train Loss: 0.007 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.279 | Test Acc: 54.688% (70/128)\n",
      "30 117 Test Loss: 2.346 | Test Acc: 54.486% (2162/3968)\n",
      "60 117 Test Loss: 2.354 | Test Acc: 54.367% (4245/7808)\n",
      "90 117 Test Loss: 2.365 | Test Acc: 53.966% (6286/11648)\n",
      "117 Epoch: 97 | Test Loss: 2.370 | Test Acc: 53.726% (8046/14976)\n",
      "\n",
      "Epoch: 98\n",
      "0 117 Train Loss: 0.008 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.007 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 98 | Train Loss: 0.007 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.721 | Test Acc: 50.781% (65/128)\n",
      "30 117 Test Loss: 2.369 | Test Acc: 53.201% (2111/3968)\n",
      "60 117 Test Loss: 2.374 | Test Acc: 53.330% (4164/7808)\n",
      "90 117 Test Loss: 2.342 | Test Acc: 53.992% (6289/11648)\n",
      "117 Epoch: 98 | Test Loss: 2.367 | Test Acc: 53.706% (8043/14976)\n",
      "\n",
      "Epoch: 99\n",
      "0 117 Train Loss: 0.007 | Train Acc: 100.000% (128/128)\n",
      "30 117 Train Loss: 0.007 | Train Acc: 100.000% (3968/3968)\n",
      "60 117 Train Loss: 0.007 | Train Acc: 100.000% (7808/7808)\n",
      "90 117 Train Loss: 0.007 | Train Acc: 100.000% (11648/11648)\n",
      "117 Epoch: 99 | Train Loss: 0.007 | Train Acc: 100.000% (14976/14976)\n",
      "0 117 Test Loss: 2.349 | Test Acc: 57.812% (74/128)\n",
      "30 117 Test Loss: 2.422 | Test Acc: 53.478% (2122/3968)\n",
      "60 117 Test Loss: 2.401 | Test Acc: 53.599% (4185/7808)\n",
      "90 117 Test Loss: 2.374 | Test Acc: 53.700% (6255/11648)\n",
      "117 Epoch: 99 | Test Loss: 2.367 | Test Acc: 53.766% (8052/14976)\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.1.3: Start Target model training\n",
    "\n",
    "max_epoch = 100  #@param {type:\"integer\"}\n",
    "train_result_summary = 'target_train_ReNet.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(target_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(target_testloader, epoch, batch_size=batch_size, logfile = train_result_summary, save_modelpath = save_model_folder)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMAIZbEfGykV"
   },
   "source": [
    "## Method 2: DCA (with bi-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1pPHPJaOBDbn",
    "outputId": "dc0e5415-e981-4537-e0e2-3712979ece30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model for DLA-BiLSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/faculty/jhou4/Projects/RNA_folding/python_env_gpu1_2/lib/python3.8/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training model from scratch..\n",
      "Total trained parameters:  58303034\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DLA-BiLSTM'  #@param {type:\"string\"}\n",
    "save_model_folder = './DLA-BiLSTM_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DLA-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN6Js0lb04BO",
    "outputId": "d2d43644-7cba-4a0d-b674-d159618d60dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Setting target_train_dataset size to  15000 Counter({3: 1591, 5: 1536, 7: 1520, 8: 1519, 0: 1512, 4: 1502, 9: 1492, 1: 1456, 6: 1448, 2: 1424})\n",
      "Setting target_test_dataset size to  15000 Counter({6: 1543, 9: 1533, 2: 1527, 5: 1515, 7: 1507, 8: 1503, 4: 1495, 0: 1482, 3: 1461, 1: 1434})\n",
      "Setting shadow_train_dataset size to  15000 Counter({1: 1537, 6: 1535, 3: 1525, 0: 1514, 4: 1510, 7: 1493, 8: 1487, 9: 1478, 2: 1464, 5: 1457})\n",
      "Setting shadow_test_dataset size to  15000 Counter({2: 1585, 1: 1573, 9: 1497, 4: 1493, 0: 1492, 5: 1492, 8: 1491, 7: 1480, 6: 1474, 3: 1423})\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.2.2: Setup Target and Shadow datasets for DLA Training\n",
    "\n",
    "target_train_size = 15000 #@param {type:\"integer\"}\n",
    "target_test_size= 15000 #@param {type:\"integer\"}\n",
    "shadow_train_size = 15000  #@param {type:\"integer\"} \n",
    "shadow_test_size= 15000 #@param {type:\"integer\"}\n",
    "\n",
    "# create dataset for renet \n",
    "print('==> Preparing dataset..')\n",
    "target_trainloader, target_testloader, shadow_train_dataset, shadow_testloader = create_cifar_dataset_torch(batch_size=batch_size, target_train_size = target_train_size, target_test_size= target_train_size, shadow_train_size = target_train_size, shadow_test_size= target_train_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-Y5WkRwHofk",
    "outputId": "f1cfb872-7756-4079-d211-707ebefa5b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 234 Train Loss: 2.299 | Train Acc: 14.062% (9/64)\n",
      "30 234 Train Loss: 2.299 | Train Acc: 10.585% (210/1984)\n",
      "60 234 Train Loss: 2.279 | Train Acc: 13.909% (543/3904)\n",
      "90 234 Train Loss: 2.223 | Train Acc: 15.677% (913/5824)\n",
      "120 234 Train Loss: 2.156 | Train Acc: 17.149% (1328/7744)\n",
      "150 234 Train Loss: 2.103 | Train Acc: 18.584% (1796/9664)\n",
      "180 234 Train Loss: 2.055 | Train Acc: 19.993% (2316/11584)\n",
      "210 234 Train Loss: 2.019 | Train Acc: 21.134% (2854/13504)\n",
      "234 Epoch: 0 | Train Loss: 1.996 | Train Acc: 21.995% (3294/14976)\n",
      "0 234 Test Loss: 2.077 | Test Acc: 26.562% (17/64)\n",
      "30 234 Test Loss: 1.920 | Test Acc: 26.411% (524/1984)\n",
      "60 234 Test Loss: 1.897 | Test Acc: 26.895% (1050/3904)\n",
      "90 234 Test Loss: 1.893 | Test Acc: 27.249% (1587/5824)\n",
      "120 234 Test Loss: 1.889 | Test Acc: 27.402% (2122/7744)\n",
      "150 234 Test Loss: 1.883 | Test Acc: 27.514% (2659/9664)\n",
      "180 234 Test Loss: 1.884 | Test Acc: 27.150% (3145/11584)\n",
      "210 234 Test Loss: 1.884 | Test Acc: 27.333% (3691/13504)\n",
      "234 Epoch: 0 | Test Loss: 1.885 | Test Acc: 27.270% (4084/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "0 234 Train Loss: 1.745 | Train Acc: 34.375% (22/64)\n",
      "30 234 Train Loss: 1.713 | Train Acc: 32.409% (643/1984)\n",
      "60 234 Train Loss: 1.702 | Train Acc: 33.120% (1293/3904)\n",
      "90 234 Train Loss: 1.685 | Train Acc: 33.448% (1948/5824)\n",
      "120 234 Train Loss: 1.678 | Train Acc: 33.923% (2627/7744)\n",
      "150 234 Train Loss: 1.667 | Train Acc: 34.654% (3349/9664)\n",
      "180 234 Train Loss: 1.651 | Train Acc: 35.428% (4104/11584)\n",
      "210 234 Train Loss: 1.641 | Train Acc: 35.938% (4853/13504)\n",
      "234 Epoch: 1 | Train Loss: 1.629 | Train Acc: 36.765% (5506/14976)\n",
      "0 234 Test Loss: 1.559 | Test Acc: 40.625% (26/64)\n",
      "30 234 Test Loss: 1.546 | Test Acc: 43.196% (857/1984)\n",
      "60 234 Test Loss: 1.555 | Test Acc: 42.392% (1655/3904)\n",
      "90 234 Test Loss: 1.557 | Test Acc: 42.016% (2447/5824)\n",
      "120 234 Test Loss: 1.554 | Test Acc: 41.929% (3247/7744)\n",
      "150 234 Test Loss: 1.552 | Test Acc: 41.950% (4054/9664)\n",
      "180 234 Test Loss: 1.554 | Test Acc: 41.773% (4839/11584)\n",
      "210 234 Test Loss: 1.558 | Test Acc: 41.625% (5621/13504)\n",
      "234 Epoch: 1 | Test Loss: 1.560 | Test Acc: 41.513% (6217/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "0 234 Train Loss: 1.485 | Train Acc: 42.188% (27/64)\n",
      "30 234 Train Loss: 1.449 | Train Acc: 44.506% (883/1984)\n",
      "60 234 Train Loss: 1.446 | Train Acc: 45.569% (1779/3904)\n",
      "90 234 Train Loss: 1.442 | Train Acc: 45.484% (2649/5824)\n",
      "120 234 Train Loss: 1.409 | Train Acc: 47.146% (3651/7744)\n",
      "150 234 Train Loss: 1.401 | Train Acc: 47.734% (4613/9664)\n",
      "180 234 Train Loss: 1.385 | Train Acc: 48.489% (5617/11584)\n",
      "210 234 Train Loss: 1.367 | Train Acc: 49.385% (6669/13504)\n",
      "234 Epoch: 2 | Train Loss: 1.355 | Train Acc: 49.947% (7480/14976)\n",
      "0 234 Test Loss: 1.401 | Test Acc: 53.125% (34/64)\n",
      "30 234 Test Loss: 1.290 | Test Acc: 52.470% (1041/1984)\n",
      "60 234 Test Loss: 1.305 | Test Acc: 51.819% (2023/3904)\n",
      "90 234 Test Loss: 1.323 | Test Acc: 51.133% (2978/5824)\n",
      "120 234 Test Loss: 1.314 | Test Acc: 51.550% (3992/7744)\n",
      "150 234 Test Loss: 1.311 | Test Acc: 51.780% (5004/9664)\n",
      "180 234 Test Loss: 1.317 | Test Acc: 51.554% (5972/11584)\n",
      "210 234 Test Loss: 1.320 | Test Acc: 51.592% (6967/13504)\n",
      "234 Epoch: 2 | Test Loss: 1.317 | Test Acc: 51.562% (7722/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "0 234 Train Loss: 0.963 | Train Acc: 68.750% (44/64)\n",
      "30 234 Train Loss: 1.128 | Train Acc: 58.317% (1157/1984)\n",
      "60 234 Train Loss: 1.114 | Train Acc: 59.298% (2315/3904)\n",
      "90 234 Train Loss: 1.112 | Train Acc: 59.358% (3457/5824)\n",
      "120 234 Train Loss: 1.114 | Train Acc: 59.310% (4593/7744)\n",
      "150 234 Train Loss: 1.099 | Train Acc: 60.099% (5808/9664)\n",
      "180 234 Train Loss: 1.087 | Train Acc: 60.558% (7015/11584)\n",
      "210 234 Train Loss: 1.077 | Train Acc: 60.945% (8230/13504)\n",
      "234 Epoch: 3 | Train Loss: 1.075 | Train Acc: 60.924% (9124/14976)\n",
      "0 234 Test Loss: 1.177 | Test Acc: 67.188% (43/64)\n",
      "30 234 Test Loss: 1.363 | Test Acc: 51.865% (1029/1984)\n",
      "60 234 Test Loss: 1.356 | Test Acc: 52.049% (2032/3904)\n",
      "90 234 Test Loss: 1.346 | Test Acc: 51.889% (3022/5824)\n",
      "120 234 Test Loss: 1.334 | Test Acc: 52.131% (4037/7744)\n",
      "150 234 Test Loss: 1.341 | Test Acc: 51.904% (5016/9664)\n",
      "180 234 Test Loss: 1.346 | Test Acc: 51.856% (6007/11584)\n",
      "210 234 Test Loss: 1.345 | Test Acc: 51.903% (7009/13504)\n",
      "234 Epoch: 3 | Test Loss: 1.354 | Test Acc: 51.683% (7740/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "0 234 Train Loss: 0.994 | Train Acc: 64.062% (41/64)\n",
      "30 234 Train Loss: 0.949 | Train Acc: 66.331% (1316/1984)\n",
      "60 234 Train Loss: 0.931 | Train Acc: 67.008% (2616/3904)\n",
      "90 234 Train Loss: 0.929 | Train Acc: 67.479% (3930/5824)\n",
      "120 234 Train Loss: 0.933 | Train Acc: 67.110% (5197/7744)\n",
      "150 234 Train Loss: 0.923 | Train Acc: 67.446% (6518/9664)\n",
      "180 234 Train Loss: 0.913 | Train Acc: 67.524% (7822/11584)\n",
      "210 234 Train Loss: 0.909 | Train Acc: 67.536% (9120/13504)\n",
      "234 Epoch: 4 | Train Loss: 0.902 | Train Acc: 67.735% (10144/14976)\n",
      "0 234 Test Loss: 1.355 | Test Acc: 54.688% (35/64)\n",
      "30 234 Test Loss: 1.510 | Test Acc: 49.294% (978/1984)\n",
      "60 234 Test Loss: 1.525 | Test Acc: 49.949% (1950/3904)\n",
      "90 234 Test Loss: 1.494 | Test Acc: 50.086% (2917/5824)\n",
      "120 234 Test Loss: 1.483 | Test Acc: 50.181% (3886/7744)\n",
      "150 234 Test Loss: 1.481 | Test Acc: 50.114% (4843/9664)\n",
      "180 234 Test Loss: 1.473 | Test Acc: 50.129% (5807/11584)\n",
      "210 234 Test Loss: 1.471 | Test Acc: 50.378% (6803/13504)\n",
      "234 Epoch: 4 | Test Loss: 1.475 | Test Acc: 50.280% (7530/14976)\n",
      "\n",
      "Epoch: 5\n",
      "0 234 Train Loss: 1.067 | Train Acc: 62.500% (40/64)\n",
      "30 234 Train Loss: 0.760 | Train Acc: 73.034% (1449/1984)\n",
      "60 234 Train Loss: 0.746 | Train Acc: 73.028% (2851/3904)\n",
      "90 234 Train Loss: 0.750 | Train Acc: 72.940% (4248/5824)\n",
      "120 234 Train Loss: 0.762 | Train Acc: 72.275% (5597/7744)\n",
      "150 234 Train Loss: 0.764 | Train Acc: 72.279% (6985/9664)\n",
      "180 234 Train Loss: 0.763 | Train Acc: 72.376% (8384/11584)\n",
      "210 234 Train Loss: 0.759 | Train Acc: 72.556% (9798/13504)\n",
      "234 Epoch: 5 | Train Loss: 0.764 | Train Acc: 72.276% (10824/14976)\n",
      "0 234 Test Loss: 1.426 | Test Acc: 59.375% (38/64)\n",
      "30 234 Test Loss: 1.778 | Test Acc: 52.722% (1046/1984)\n",
      "60 234 Test Loss: 1.722 | Test Acc: 53.253% (2079/3904)\n",
      "90 234 Test Loss: 1.718 | Test Acc: 53.383% (3109/5824)\n",
      "120 234 Test Loss: 1.714 | Test Acc: 53.267% (4125/7744)\n",
      "150 234 Test Loss: 1.713 | Test Acc: 53.373% (5158/9664)\n",
      "180 234 Test Loss: 1.699 | Test Acc: 53.885% (6242/11584)\n",
      "210 234 Test Loss: 1.693 | Test Acc: 54.021% (7295/13504)\n",
      "234 Epoch: 5 | Test Loss: 1.696 | Test Acc: 53.973% (8083/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      "0 234 Train Loss: 0.637 | Train Acc: 79.688% (51/64)\n",
      "30 234 Train Loss: 0.587 | Train Acc: 80.091% (1589/1984)\n",
      "60 234 Train Loss: 0.587 | Train Acc: 79.611% (3108/3904)\n",
      "90 234 Train Loss: 0.587 | Train Acc: 79.499% (4630/5824)\n",
      "120 234 Train Loss: 0.601 | Train Acc: 79.068% (6123/7744)\n",
      "150 234 Train Loss: 0.612 | Train Acc: 78.611% (7597/9664)\n",
      "180 234 Train Loss: 0.623 | Train Acc: 77.978% (9033/11584)\n",
      "210 234 Train Loss: 0.627 | Train Acc: 77.821% (10509/13504)\n",
      "234 Epoch: 6 | Train Loss: 0.631 | Train Acc: 77.764% (11646/14976)\n",
      "0 234 Test Loss: 0.724 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.982 | Test Acc: 66.482% (1319/1984)\n",
      "60 234 Test Loss: 0.990 | Test Acc: 66.137% (2582/3904)\n",
      "90 234 Test Loss: 0.979 | Test Acc: 66.415% (3868/5824)\n",
      "120 234 Test Loss: 0.972 | Test Acc: 66.826% (5175/7744)\n",
      "150 234 Test Loss: 0.970 | Test Acc: 66.908% (6466/9664)\n",
      "180 234 Test Loss: 0.970 | Test Acc: 66.980% (7759/11584)\n",
      "210 234 Test Loss: 0.976 | Test Acc: 66.965% (9043/13504)\n",
      "234 Epoch: 6 | Test Loss: 0.973 | Test Acc: 67.134% (10054/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      "0 234 Train Loss: 0.593 | Train Acc: 78.125% (50/64)\n",
      "30 234 Train Loss: 0.461 | Train Acc: 83.518% (1657/1984)\n",
      "60 234 Train Loss: 0.442 | Train Acc: 84.554% (3301/3904)\n",
      "90 234 Train Loss: 0.448 | Train Acc: 84.323% (4911/5824)\n",
      "120 234 Train Loss: 0.442 | Train Acc: 84.607% (6552/7744)\n",
      "150 234 Train Loss: 0.452 | Train Acc: 84.323% (8149/9664)\n",
      "180 234 Train Loss: 0.465 | Train Acc: 83.710% (9697/11584)\n",
      "210 234 Train Loss: 0.479 | Train Acc: 83.161% (11230/13504)\n",
      "234 Epoch: 7 | Train Loss: 0.484 | Train Acc: 82.986% (12428/14976)\n",
      "0 234 Test Loss: 1.143 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.038 | Test Acc: 66.230% (1314/1984)\n",
      "60 234 Test Loss: 1.046 | Test Acc: 66.112% (2581/3904)\n",
      "90 234 Test Loss: 1.043 | Test Acc: 66.277% (3860/5824)\n",
      "120 234 Test Loss: 1.059 | Test Acc: 65.793% (5095/7744)\n",
      "150 234 Test Loss: 1.064 | Test Acc: 65.594% (6339/9664)\n",
      "180 234 Test Loss: 1.067 | Test Acc: 65.452% (7582/11584)\n",
      "210 234 Test Loss: 1.066 | Test Acc: 65.647% (8865/13504)\n",
      "234 Epoch: 7 | Test Loss: 1.063 | Test Acc: 65.558% (9818/14976)\n",
      "\n",
      "Epoch: 8\n",
      "0 234 Train Loss: 0.269 | Train Acc: 89.062% (57/64)\n",
      "30 234 Train Loss: 0.380 | Train Acc: 86.139% (1709/1984)\n",
      "60 234 Train Loss: 0.363 | Train Acc: 87.065% (3399/3904)\n",
      "90 234 Train Loss: 0.372 | Train Acc: 86.899% (5061/5824)\n",
      "120 234 Train Loss: 0.374 | Train Acc: 86.738% (6717/7744)\n",
      "150 234 Train Loss: 0.386 | Train Acc: 86.196% (8330/9664)\n",
      "180 234 Train Loss: 0.390 | Train Acc: 86.076% (9971/11584)\n",
      "210 234 Train Loss: 0.390 | Train Acc: 86.093% (11626/13504)\n",
      "234 Epoch: 8 | Train Loss: 0.398 | Train Acc: 85.844% (12856/14976)\n",
      "0 234 Test Loss: 0.639 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 1.181 | Test Acc: 64.466% (1279/1984)\n",
      "60 234 Test Loss: 1.168 | Test Acc: 65.241% (2547/3904)\n",
      "90 234 Test Loss: 1.170 | Test Acc: 65.007% (3786/5824)\n",
      "120 234 Test Loss: 1.163 | Test Acc: 64.954% (5030/7744)\n",
      "150 234 Test Loss: 1.155 | Test Acc: 65.201% (6301/9664)\n",
      "180 234 Test Loss: 1.152 | Test Acc: 65.116% (7543/11584)\n",
      "210 234 Test Loss: 1.151 | Test Acc: 64.966% (8773/13504)\n",
      "234 Epoch: 8 | Test Loss: 1.150 | Test Acc: 65.104% (9750/14976)\n",
      "\n",
      "Epoch: 9\n",
      "0 234 Train Loss: 0.441 | Train Acc: 82.812% (53/64)\n",
      "30 234 Train Loss: 0.313 | Train Acc: 89.012% (1766/1984)\n",
      "60 234 Train Loss: 0.286 | Train Acc: 90.574% (3536/3904)\n",
      "90 234 Train Loss: 0.278 | Train Acc: 90.848% (5291/5824)\n",
      "120 234 Train Loss: 0.271 | Train Acc: 90.961% (7044/7744)\n",
      "150 234 Train Loss: 0.285 | Train Acc: 90.304% (8727/9664)\n",
      "180 234 Train Loss: 0.293 | Train Acc: 90.003% (10426/11584)\n",
      "210 234 Train Loss: 0.305 | Train Acc: 89.559% (12094/13504)\n",
      "234 Epoch: 9 | Train Loss: 0.310 | Train Acc: 89.443% (13395/14976)\n",
      "0 234 Test Loss: 0.870 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.066 | Test Acc: 67.540% (1340/1984)\n",
      "60 234 Test Loss: 1.055 | Test Acc: 68.033% (2656/3904)\n",
      "90 234 Test Loss: 1.029 | Test Acc: 68.716% (4002/5824)\n",
      "120 234 Test Loss: 1.034 | Test Acc: 68.376% (5295/7744)\n",
      "150 234 Test Loss: 1.027 | Test Acc: 68.615% (6631/9664)\n",
      "180 234 Test Loss: 1.021 | Test Acc: 68.819% (7972/11584)\n",
      "210 234 Test Loss: 1.010 | Test Acc: 69.017% (9320/13504)\n",
      "234 Epoch: 9 | Test Loss: 1.012 | Test Acc: 68.884% (10316/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 10\n",
      "0 234 Train Loss: 0.174 | Train Acc: 93.750% (60/64)\n",
      "30 234 Train Loss: 0.214 | Train Acc: 93.498% (1855/1984)\n",
      "60 234 Train Loss: 0.204 | Train Acc: 93.801% (3662/3904)\n",
      "90 234 Train Loss: 0.213 | Train Acc: 93.355% (5437/5824)\n",
      "120 234 Train Loss: 0.215 | Train Acc: 93.156% (7214/7744)\n",
      "150 234 Train Loss: 0.220 | Train Acc: 92.891% (8977/9664)\n",
      "180 234 Train Loss: 0.226 | Train Acc: 92.593% (10726/11584)\n",
      "210 234 Train Loss: 0.230 | Train Acc: 92.461% (12486/13504)\n",
      "234 Epoch: 10 | Train Loss: 0.235 | Train Acc: 92.214% (13810/14976)\n",
      "0 234 Test Loss: 0.951 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 1.201 | Test Acc: 66.986% (1329/1984)\n",
      "60 234 Test Loss: 1.190 | Test Acc: 67.264% (2626/3904)\n",
      "90 234 Test Loss: 1.209 | Test Acc: 66.346% (3864/5824)\n",
      "120 234 Test Loss: 1.232 | Test Acc: 66.038% (5114/7744)\n",
      "150 234 Test Loss: 1.234 | Test Acc: 66.122% (6390/9664)\n",
      "180 234 Test Loss: 1.236 | Test Acc: 65.927% (7637/11584)\n",
      "210 234 Test Loss: 1.230 | Test Acc: 66.069% (8922/13504)\n",
      "234 Epoch: 10 | Test Loss: 1.227 | Test Acc: 66.166% (9909/14976)\n",
      "\n",
      "Epoch: 11\n",
      "0 234 Train Loss: 0.176 | Train Acc: 92.188% (59/64)\n",
      "30 234 Train Loss: 0.185 | Train Acc: 93.901% (1863/1984)\n",
      "60 234 Train Loss: 0.173 | Train Acc: 94.211% (3678/3904)\n",
      "90 234 Train Loss: 0.167 | Train Acc: 94.334% (5494/5824)\n",
      "120 234 Train Loss: 0.167 | Train Acc: 94.305% (7303/7744)\n",
      "150 234 Train Loss: 0.176 | Train Acc: 94.091% (9093/9664)\n",
      "180 234 Train Loss: 0.179 | Train Acc: 93.957% (10884/11584)\n",
      "210 234 Train Loss: 0.181 | Train Acc: 93.935% (12685/13504)\n",
      "234 Epoch: 11 | Train Loss: 0.184 | Train Acc: 93.743% (14039/14976)\n",
      "0 234 Test Loss: 1.658 | Test Acc: 57.812% (37/64)\n",
      "30 234 Test Loss: 1.219 | Test Acc: 67.540% (1340/1984)\n",
      "60 234 Test Loss: 1.191 | Test Acc: 67.879% (2650/3904)\n",
      "90 234 Test Loss: 1.176 | Test Acc: 68.338% (3980/5824)\n",
      "120 234 Test Loss: 1.186 | Test Acc: 68.221% (5283/7744)\n",
      "150 234 Test Loss: 1.168 | Test Acc: 68.615% (6631/9664)\n",
      "180 234 Test Loss: 1.167 | Test Acc: 68.621% (7949/11584)\n",
      "210 234 Test Loss: 1.174 | Test Acc: 68.350% (9230/13504)\n",
      "234 Epoch: 11 | Test Loss: 1.172 | Test Acc: 68.323% (10232/14976)\n",
      "\n",
      "Epoch: 12\n",
      "0 234 Train Loss: 0.046 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.135 | Train Acc: 95.464% (1894/1984)\n",
      "60 234 Train Loss: 0.135 | Train Acc: 95.312% (3721/3904)\n",
      "90 234 Train Loss: 0.138 | Train Acc: 95.158% (5542/5824)\n",
      "120 234 Train Loss: 0.133 | Train Acc: 95.416% (7389/7744)\n",
      "150 234 Train Loss: 0.139 | Train Acc: 95.219% (9202/9664)\n",
      "180 234 Train Loss: 0.144 | Train Acc: 94.984% (11003/11584)\n",
      "210 234 Train Loss: 0.150 | Train Acc: 94.876% (12812/13504)\n",
      "234 Epoch: 12 | Train Loss: 0.156 | Train Acc: 94.685% (14180/14976)\n",
      "0 234 Test Loss: 1.749 | Test Acc: 59.375% (38/64)\n",
      "30 234 Test Loss: 1.252 | Test Acc: 67.440% (1338/1984)\n",
      "60 234 Test Loss: 1.190 | Test Acc: 68.622% (2679/3904)\n",
      "90 234 Test Loss: 1.179 | Test Acc: 68.424% (3985/5824)\n",
      "120 234 Test Loss: 1.186 | Test Acc: 68.246% (5285/7744)\n",
      "150 234 Test Loss: 1.207 | Test Acc: 68.046% (6576/9664)\n",
      "180 234 Test Loss: 1.198 | Test Acc: 68.198% (7900/11584)\n",
      "210 234 Test Loss: 1.190 | Test Acc: 68.254% (9217/13504)\n",
      "234 Epoch: 12 | Test Loss: 1.192 | Test Acc: 68.222% (10217/14976)\n",
      "\n",
      "Epoch: 13\n",
      "0 234 Train Loss: 0.123 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.126 | Train Acc: 96.421% (1913/1984)\n",
      "60 234 Train Loss: 0.124 | Train Acc: 96.286% (3759/3904)\n",
      "90 234 Train Loss: 0.126 | Train Acc: 96.085% (5596/5824)\n",
      "120 234 Train Loss: 0.129 | Train Acc: 96.036% (7437/7744)\n",
      "150 234 Train Loss: 0.126 | Train Acc: 96.068% (9284/9664)\n",
      "180 234 Train Loss: 0.123 | Train Acc: 96.115% (11134/11584)\n",
      "210 234 Train Loss: 0.130 | Train Acc: 95.846% (12943/13504)\n",
      "234 Epoch: 13 | Train Loss: 0.133 | Train Acc: 95.720% (14335/14976)\n",
      "0 234 Test Loss: 1.546 | Test Acc: 64.062% (41/64)\n",
      "30 234 Test Loss: 1.123 | Test Acc: 71.169% (1412/1984)\n",
      "60 234 Test Loss: 1.124 | Test Acc: 70.927% (2769/3904)\n",
      "90 234 Test Loss: 1.121 | Test Acc: 71.205% (4147/5824)\n",
      "120 234 Test Loss: 1.123 | Test Acc: 71.152% (5510/7744)\n",
      "150 234 Test Loss: 1.124 | Test Acc: 71.016% (6863/9664)\n",
      "180 234 Test Loss: 1.120 | Test Acc: 70.977% (8222/11584)\n",
      "210 234 Test Loss: 1.113 | Test Acc: 71.031% (9592/13504)\n",
      "234 Epoch: 13 | Test Loss: 1.104 | Test Acc: 71.020% (10636/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 14\n",
      "0 234 Train Loss: 0.059 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.118 | Train Acc: 95.867% (1902/1984)\n",
      "60 234 Train Loss: 0.100 | Train Acc: 96.670% (3774/3904)\n",
      "90 234 Train Loss: 0.094 | Train Acc: 96.806% (5638/5824)\n",
      "120 234 Train Loss: 0.096 | Train Acc: 96.759% (7493/7744)\n",
      "150 234 Train Loss: 0.093 | Train Acc: 96.834% (9358/9664)\n",
      "180 234 Train Loss: 0.092 | Train Acc: 96.935% (11229/11584)\n",
      "210 234 Train Loss: 0.093 | Train Acc: 96.882% (13083/13504)\n",
      "234 Epoch: 14 | Train Loss: 0.094 | Train Acc: 96.875% (14508/14976)\n",
      "0 234 Test Loss: 0.965 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.088 | Test Acc: 69.808% (1385/1984)\n",
      "60 234 Test Loss: 1.136 | Test Acc: 69.160% (2700/3904)\n",
      "90 234 Test Loss: 1.090 | Test Acc: 70.192% (4088/5824)\n",
      "120 234 Test Loss: 1.107 | Test Acc: 70.196% (5436/7744)\n",
      "150 234 Test Loss: 1.109 | Test Acc: 70.095% (6774/9664)\n",
      "180 234 Test Loss: 1.111 | Test Acc: 70.140% (8125/11584)\n",
      "210 234 Test Loss: 1.108 | Test Acc: 70.209% (9481/13504)\n",
      "234 Epoch: 14 | Test Loss: 1.113 | Test Acc: 70.246% (10520/14976)\n",
      "\n",
      "Epoch: 15\n",
      "0 234 Train Loss: 0.031 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.082 | Train Acc: 97.228% (1929/1984)\n",
      "60 234 Train Loss: 0.090 | Train Acc: 97.182% (3794/3904)\n",
      "90 234 Train Loss: 0.089 | Train Acc: 97.150% (5658/5824)\n",
      "120 234 Train Loss: 0.088 | Train Acc: 97.211% (7528/7744)\n",
      "150 234 Train Loss: 0.084 | Train Acc: 97.320% (9405/9664)\n",
      "180 234 Train Loss: 0.082 | Train Acc: 97.298% (11271/11584)\n",
      "210 234 Train Loss: 0.082 | Train Acc: 97.334% (13144/13504)\n",
      "234 Epoch: 15 | Train Loss: 0.084 | Train Acc: 97.296% (14571/14976)\n",
      "0 234 Test Loss: 0.856 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 1.033 | Test Acc: 73.286% (1454/1984)\n",
      "60 234 Test Loss: 1.085 | Test Acc: 72.823% (2843/3904)\n",
      "90 234 Test Loss: 1.088 | Test Acc: 72.854% (4243/5824)\n",
      "120 234 Test Loss: 1.075 | Test Acc: 73.140% (5664/7744)\n",
      "150 234 Test Loss: 1.094 | Test Acc: 72.765% (7032/9664)\n",
      "180 234 Test Loss: 1.086 | Test Acc: 72.902% (8445/11584)\n",
      "210 234 Test Loss: 1.097 | Test Acc: 72.875% (9841/13504)\n",
      "234 Epoch: 15 | Test Loss: 1.098 | Test Acc: 72.776% (10899/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 16\n",
      "0 234 Train Loss: 0.025 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.069 | Train Acc: 98.185% (1948/1984)\n",
      "60 234 Train Loss: 0.066 | Train Acc: 98.105% (3830/3904)\n",
      "90 234 Train Loss: 0.065 | Train Acc: 98.025% (5709/5824)\n",
      "120 234 Train Loss: 0.062 | Train Acc: 98.128% (7599/7744)\n",
      "150 234 Train Loss: 0.059 | Train Acc: 98.179% (9488/9664)\n",
      "180 234 Train Loss: 0.059 | Train Acc: 98.161% (11371/11584)\n",
      "210 234 Train Loss: 0.060 | Train Acc: 98.119% (13250/13504)\n",
      "234 Epoch: 16 | Train Loss: 0.060 | Train Acc: 98.124% (14695/14976)\n",
      "0 234 Test Loss: 1.274 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 1.124 | Test Acc: 73.135% (1451/1984)\n",
      "60 234 Test Loss: 1.110 | Test Acc: 72.848% (2844/3904)\n",
      "90 234 Test Loss: 1.125 | Test Acc: 72.734% (4236/5824)\n",
      "120 234 Test Loss: 1.107 | Test Acc: 72.986% (5652/7744)\n",
      "150 234 Test Loss: 1.099 | Test Acc: 73.044% (7059/9664)\n",
      "180 234 Test Loss: 1.092 | Test Acc: 73.222% (8482/11584)\n",
      "210 234 Test Loss: 1.088 | Test Acc: 73.334% (9903/13504)\n",
      "234 Epoch: 16 | Test Loss: 1.093 | Test Acc: 73.137% (10953/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 17\n",
      "0 234 Train Loss: 0.063 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.044 | Train Acc: 98.841% (1961/1984)\n",
      "60 234 Train Loss: 0.047 | Train Acc: 98.617% (3850/3904)\n",
      "90 234 Train Loss: 0.055 | Train Acc: 98.231% (5721/5824)\n",
      "120 234 Train Loss: 0.059 | Train Acc: 97.998% (7589/7744)\n",
      "150 234 Train Loss: 0.060 | Train Acc: 97.993% (9470/9664)\n",
      "180 234 Train Loss: 0.061 | Train Acc: 97.980% (11350/11584)\n",
      "210 234 Train Loss: 0.062 | Train Acc: 97.941% (13226/13504)\n",
      "234 Epoch: 17 | Train Loss: 0.063 | Train Acc: 97.937% (14667/14976)\n",
      "0 234 Test Loss: 1.517 | Test Acc: 62.500% (40/64)\n",
      "30 234 Test Loss: 1.177 | Test Acc: 71.976% (1428/1984)\n",
      "60 234 Test Loss: 1.121 | Test Acc: 73.335% (2863/3904)\n",
      "90 234 Test Loss: 1.144 | Test Acc: 72.751% (4237/5824)\n",
      "120 234 Test Loss: 1.125 | Test Acc: 72.960% (5650/7744)\n",
      "150 234 Test Loss: 1.138 | Test Acc: 72.434% (7000/9664)\n",
      "180 234 Test Loss: 1.131 | Test Acc: 72.678% (8419/11584)\n",
      "210 234 Test Loss: 1.134 | Test Acc: 72.586% (9802/13504)\n",
      "234 Epoch: 17 | Test Loss: 1.127 | Test Acc: 72.763% (10897/14976)\n",
      "\n",
      "Epoch: 18\n",
      "0 234 Train Loss: 0.011 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.031 | Train Acc: 99.143% (1967/1984)\n",
      "60 234 Train Loss: 0.032 | Train Acc: 98.924% (3862/3904)\n",
      "90 234 Train Loss: 0.033 | Train Acc: 98.953% (5763/5824)\n",
      "120 234 Train Loss: 0.035 | Train Acc: 98.902% (7659/7744)\n",
      "150 234 Train Loss: 0.036 | Train Acc: 98.862% (9554/9664)\n",
      "180 234 Train Loss: 0.038 | Train Acc: 98.757% (11440/11584)\n",
      "210 234 Train Loss: 0.041 | Train Acc: 98.667% (13324/13504)\n",
      "234 Epoch: 18 | Train Loss: 0.043 | Train Acc: 98.604% (14767/14976)\n",
      "0 234 Test Loss: 0.754 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 1.040 | Test Acc: 73.085% (1450/1984)\n",
      "60 234 Test Loss: 1.082 | Test Acc: 72.848% (2844/3904)\n",
      "90 234 Test Loss: 1.084 | Test Acc: 73.197% (4263/5824)\n",
      "120 234 Test Loss: 1.084 | Test Acc: 73.412% (5685/7744)\n",
      "150 234 Test Loss: 1.070 | Test Acc: 73.665% (7119/9664)\n",
      "180 234 Test Loss: 1.072 | Test Acc: 73.593% (8525/11584)\n",
      "210 234 Test Loss: 1.077 | Test Acc: 73.556% (9933/13504)\n",
      "234 Epoch: 18 | Test Loss: 1.075 | Test Acc: 73.564% (11017/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 19\n",
      "0 234 Train Loss: 0.012 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.029 | Train Acc: 99.042% (1965/1984)\n",
      "60 234 Train Loss: 0.041 | Train Acc: 98.796% (3857/3904)\n",
      "90 234 Train Loss: 0.042 | Train Acc: 98.764% (5752/5824)\n",
      "120 234 Train Loss: 0.042 | Train Acc: 98.747% (7647/7744)\n",
      "150 234 Train Loss: 0.042 | Train Acc: 98.686% (9537/9664)\n",
      "180 234 Train Loss: 0.046 | Train Acc: 98.532% (11414/11584)\n",
      "210 234 Train Loss: 0.051 | Train Acc: 98.363% (13283/13504)\n",
      "234 Epoch: 19 | Train Loss: 0.053 | Train Acc: 98.244% (14713/14976)\n",
      "0 234 Test Loss: 0.945 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.244 | Test Acc: 69.153% (1372/1984)\n",
      "60 234 Test Loss: 1.187 | Test Acc: 70.108% (2737/3904)\n",
      "90 234 Test Loss: 1.196 | Test Acc: 70.347% (4097/5824)\n",
      "120 234 Test Loss: 1.187 | Test Acc: 70.532% (5462/7744)\n",
      "150 234 Test Loss: 1.187 | Test Acc: 70.716% (6834/9664)\n",
      "180 234 Test Loss: 1.200 | Test Acc: 70.658% (8185/11584)\n",
      "210 234 Test Loss: 1.194 | Test Acc: 70.920% (9577/13504)\n",
      "234 Epoch: 19 | Test Loss: 1.192 | Test Acc: 70.980% (10630/14976)\n",
      "\n",
      "Epoch: 20\n",
      "0 234 Train Loss: 0.125 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.069 | Train Acc: 97.984% (1944/1984)\n",
      "60 234 Train Loss: 0.062 | Train Acc: 98.181% (3833/3904)\n",
      "90 234 Train Loss: 0.055 | Train Acc: 98.386% (5730/5824)\n",
      "120 234 Train Loss: 0.052 | Train Acc: 98.476% (7626/7744)\n",
      "150 234 Train Loss: 0.051 | Train Acc: 98.479% (9517/9664)\n",
      "180 234 Train Loss: 0.051 | Train Acc: 98.463% (11406/11584)\n",
      "210 234 Train Loss: 0.051 | Train Acc: 98.408% (13289/13504)\n",
      "234 Epoch: 20 | Train Loss: 0.052 | Train Acc: 98.404% (14737/14976)\n",
      "0 234 Test Loss: 0.754 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 1.216 | Test Acc: 71.472% (1418/1984)\n",
      "60 234 Test Loss: 1.215 | Test Acc: 71.465% (2790/3904)\n",
      "90 234 Test Loss: 1.267 | Test Acc: 70.759% (4121/5824)\n",
      "120 234 Test Loss: 1.291 | Test Acc: 70.584% (5466/7744)\n",
      "150 234 Test Loss: 1.296 | Test Acc: 70.571% (6820/9664)\n",
      "180 234 Test Loss: 1.295 | Test Acc: 70.761% (8197/11584)\n",
      "210 234 Test Loss: 1.290 | Test Acc: 70.653% (9541/13504)\n",
      "234 Epoch: 20 | Test Loss: 1.283 | Test Acc: 70.807% (10604/14976)\n",
      "\n",
      "Epoch: 21\n",
      "0 234 Train Loss: 0.029 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.045 | Train Acc: 98.690% (1958/1984)\n",
      "60 234 Train Loss: 0.048 | Train Acc: 98.361% (3840/3904)\n",
      "90 234 Train Loss: 0.051 | Train Acc: 98.283% (5724/5824)\n",
      "120 234 Train Loss: 0.049 | Train Acc: 98.360% (7617/7744)\n",
      "150 234 Train Loss: 0.050 | Train Acc: 98.313% (9501/9664)\n",
      "180 234 Train Loss: 0.054 | Train Acc: 98.239% (11380/11584)\n",
      "210 234 Train Loss: 0.055 | Train Acc: 98.186% (13259/13504)\n",
      "234 Epoch: 21 | Train Loss: 0.058 | Train Acc: 98.110% (14693/14976)\n",
      "0 234 Test Loss: 1.156 | Test Acc: 71.875% (46/64)\n",
      "30 234 Test Loss: 1.250 | Test Acc: 70.817% (1405/1984)\n",
      "60 234 Test Loss: 1.250 | Test Acc: 70.825% (2765/3904)\n",
      "90 234 Test Loss: 1.244 | Test Acc: 70.604% (4112/5824)\n",
      "120 234 Test Loss: 1.243 | Test Acc: 70.519% (5461/7744)\n",
      "150 234 Test Loss: 1.213 | Test Acc: 70.902% (6852/9664)\n",
      "180 234 Test Loss: 1.222 | Test Acc: 70.718% (8192/11584)\n",
      "210 234 Test Loss: 1.210 | Test Acc: 70.912% (9576/13504)\n",
      "234 Epoch: 21 | Test Loss: 1.217 | Test Acc: 70.933% (10623/14976)\n",
      "\n",
      "Epoch: 22\n",
      "0 234 Train Loss: 0.123 | Train Acc: 95.312% (61/64)\n",
      "30 234 Train Loss: 0.068 | Train Acc: 97.681% (1938/1984)\n",
      "60 234 Train Loss: 0.058 | Train Acc: 97.900% (3822/3904)\n",
      "90 234 Train Loss: 0.055 | Train Acc: 98.043% (5710/5824)\n",
      "120 234 Train Loss: 0.050 | Train Acc: 98.295% (7612/7744)\n",
      "150 234 Train Loss: 0.049 | Train Acc: 98.344% (9504/9664)\n",
      "180 234 Train Loss: 0.049 | Train Acc: 98.420% (11401/11584)\n",
      "210 234 Train Loss: 0.050 | Train Acc: 98.363% (13283/13504)\n",
      "234 Epoch: 22 | Train Loss: 0.051 | Train Acc: 98.337% (14727/14976)\n",
      "0 234 Test Loss: 1.370 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 1.263 | Test Acc: 70.514% (1399/1984)\n",
      "60 234 Test Loss: 1.235 | Test Acc: 70.594% (2756/3904)\n",
      "90 234 Test Loss: 1.243 | Test Acc: 70.862% (4127/5824)\n",
      "120 234 Test Loss: 1.231 | Test Acc: 71.165% (5511/7744)\n",
      "150 234 Test Loss: 1.225 | Test Acc: 71.244% (6885/9664)\n",
      "180 234 Test Loss: 1.233 | Test Acc: 71.253% (8254/11584)\n",
      "210 234 Test Loss: 1.231 | Test Acc: 71.312% (9630/13504)\n",
      "234 Epoch: 22 | Test Loss: 1.231 | Test Acc: 71.374% (10689/14976)\n",
      "\n",
      "Epoch: 23\n",
      "0 234 Train Loss: 0.045 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.049 | Train Acc: 98.337% (1951/1984)\n",
      "60 234 Train Loss: 0.043 | Train Acc: 98.566% (3848/3904)\n",
      "90 234 Train Loss: 0.042 | Train Acc: 98.592% (5742/5824)\n",
      "120 234 Train Loss: 0.041 | Train Acc: 98.657% (7640/7744)\n",
      "150 234 Train Loss: 0.042 | Train Acc: 98.634% (9532/9664)\n",
      "180 234 Train Loss: 0.043 | Train Acc: 98.602% (11422/11584)\n",
      "210 234 Train Loss: 0.043 | Train Acc: 98.615% (13317/13504)\n",
      "234 Epoch: 23 | Train Loss: 0.044 | Train Acc: 98.591% (14765/14976)\n",
      "0 234 Test Loss: 1.168 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.197 | Test Acc: 72.026% (1429/1984)\n",
      "60 234 Test Loss: 1.163 | Test Acc: 71.952% (2809/3904)\n",
      "90 234 Test Loss: 1.155 | Test Acc: 72.459% (4220/5824)\n",
      "120 234 Test Loss: 1.148 | Test Acc: 72.546% (5618/7744)\n",
      "150 234 Test Loss: 1.139 | Test Acc: 72.537% (7010/9664)\n",
      "180 234 Test Loss: 1.146 | Test Acc: 72.272% (8372/11584)\n",
      "210 234 Test Loss: 1.139 | Test Acc: 72.334% (9768/13504)\n",
      "234 Epoch: 23 | Test Loss: 1.147 | Test Acc: 72.149% (10805/14976)\n",
      "\n",
      "Epoch: 24\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.036 | Train Acc: 98.740% (1959/1984)\n",
      "60 234 Train Loss: 0.035 | Train Acc: 98.847% (3859/3904)\n",
      "90 234 Train Loss: 0.034 | Train Acc: 98.867% (5758/5824)\n",
      "120 234 Train Loss: 0.034 | Train Acc: 98.928% (7661/7744)\n",
      "150 234 Train Loss: 0.032 | Train Acc: 99.038% (9571/9664)\n",
      "180 234 Train Loss: 0.032 | Train Acc: 99.033% (11472/11584)\n",
      "210 234 Train Loss: 0.032 | Train Acc: 99.037% (13374/13504)\n",
      "234 Epoch: 24 | Train Loss: 0.031 | Train Acc: 99.065% (14836/14976)\n",
      "0 234 Test Loss: 1.122 | Test Acc: 71.875% (46/64)\n",
      "30 234 Test Loss: 1.130 | Test Acc: 73.236% (1453/1984)\n",
      "60 234 Test Loss: 1.149 | Test Acc: 72.464% (2829/3904)\n",
      "90 234 Test Loss: 1.125 | Test Acc: 72.922% (4247/5824)\n",
      "120 234 Test Loss: 1.116 | Test Acc: 73.063% (5658/7744)\n",
      "150 234 Test Loss: 1.119 | Test Acc: 72.786% (7034/9664)\n",
      "180 234 Test Loss: 1.123 | Test Acc: 72.980% (8454/11584)\n",
      "210 234 Test Loss: 1.124 | Test Acc: 72.875% (9841/13504)\n",
      "234 Epoch: 24 | Test Loss: 1.129 | Test Acc: 72.796% (10902/14976)\n",
      "\n",
      "Epoch: 25\n",
      "0 234 Train Loss: 0.011 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.020 | Train Acc: 99.395% (1972/1984)\n",
      "60 234 Train Loss: 0.018 | Train Acc: 99.565% (3887/3904)\n",
      "90 234 Train Loss: 0.017 | Train Acc: 99.605% (5801/5824)\n",
      "120 234 Train Loss: 0.016 | Train Acc: 99.613% (7714/7744)\n",
      "150 234 Train Loss: 0.016 | Train Acc: 99.607% (9626/9664)\n",
      "180 234 Train Loss: 0.015 | Train Acc: 99.646% (11543/11584)\n",
      "210 234 Train Loss: 0.015 | Train Acc: 99.645% (13456/13504)\n",
      "234 Epoch: 25 | Train Loss: 0.015 | Train Acc: 99.633% (14921/14976)\n",
      "0 234 Test Loss: 0.775 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.979 | Test Acc: 76.663% (1521/1984)\n",
      "60 234 Test Loss: 0.984 | Test Acc: 75.897% (2963/3904)\n",
      "90 234 Test Loss: 1.013 | Test Acc: 75.206% (4380/5824)\n",
      "120 234 Test Loss: 1.019 | Test Acc: 75.181% (5822/7744)\n",
      "150 234 Test Loss: 1.024 | Test Acc: 74.897% (7238/9664)\n",
      "180 234 Test Loss: 1.025 | Test Acc: 75.035% (8692/11584)\n",
      "210 234 Test Loss: 1.018 | Test Acc: 75.141% (10147/13504)\n",
      "234 Epoch: 25 | Test Loss: 1.011 | Test Acc: 75.234% (11267/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 26\n",
      "0 234 Train Loss: 0.029 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.010 | Train Acc: 99.849% (1981/1984)\n",
      "60 234 Train Loss: 0.010 | Train Acc: 99.795% (3896/3904)\n",
      "90 234 Train Loss: 0.013 | Train Acc: 99.725% (5808/5824)\n",
      "120 234 Train Loss: 0.013 | Train Acc: 99.677% (7719/7744)\n",
      "150 234 Train Loss: 0.014 | Train Acc: 99.638% (9629/9664)\n",
      "180 234 Train Loss: 0.017 | Train Acc: 99.542% (11531/11584)\n",
      "210 234 Train Loss: 0.018 | Train Acc: 99.467% (13432/13504)\n",
      "234 Epoch: 26 | Train Loss: 0.020 | Train Acc: 99.392% (14885/14976)\n",
      "0 234 Test Loss: 0.874 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 1.043 | Test Acc: 74.950% (1487/1984)\n",
      "60 234 Test Loss: 1.063 | Test Acc: 74.974% (2927/3904)\n",
      "90 234 Test Loss: 1.082 | Test Acc: 74.966% (4366/5824)\n",
      "120 234 Test Loss: 1.071 | Test Acc: 74.961% (5805/7744)\n",
      "150 234 Test Loss: 1.070 | Test Acc: 74.886% (7237/9664)\n",
      "180 234 Test Loss: 1.068 | Test Acc: 74.914% (8678/11584)\n",
      "210 234 Test Loss: 1.069 | Test Acc: 74.682% (10085/13504)\n",
      "234 Epoch: 26 | Test Loss: 1.072 | Test Acc: 74.760% (11196/14976)\n",
      "\n",
      "Epoch: 27\n",
      "0 234 Train Loss: 0.011 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.017 | Train Acc: 99.597% (1976/1984)\n",
      "60 234 Train Loss: 0.018 | Train Acc: 99.539% (3886/3904)\n",
      "90 234 Train Loss: 0.018 | Train Acc: 99.536% (5797/5824)\n",
      "120 234 Train Loss: 0.018 | Train Acc: 99.535% (7708/7744)\n",
      "150 234 Train Loss: 0.018 | Train Acc: 99.524% (9618/9664)\n",
      "180 234 Train Loss: 0.018 | Train Acc: 99.525% (11529/11584)\n",
      "210 234 Train Loss: 0.018 | Train Acc: 99.526% (13440/13504)\n",
      "234 Epoch: 27 | Train Loss: 0.018 | Train Acc: 99.519% (14904/14976)\n",
      "0 234 Test Loss: 1.338 | Test Acc: 67.188% (43/64)\n",
      "30 234 Test Loss: 1.094 | Test Acc: 73.185% (1452/1984)\n",
      "60 234 Test Loss: 1.141 | Test Acc: 73.105% (2854/3904)\n",
      "90 234 Test Loss: 1.120 | Test Acc: 73.506% (4281/5824)\n",
      "120 234 Test Loss: 1.145 | Test Acc: 73.153% (5665/7744)\n",
      "150 234 Test Loss: 1.151 | Test Acc: 72.930% (7048/9664)\n",
      "180 234 Test Loss: 1.142 | Test Acc: 73.109% (8469/11584)\n",
      "210 234 Test Loss: 1.128 | Test Acc: 73.275% (9895/13504)\n",
      "234 Epoch: 27 | Test Loss: 1.126 | Test Acc: 73.304% (10978/14976)\n",
      "\n",
      "Epoch: 28\n",
      "0 234 Train Loss: 0.030 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.018 | Train Acc: 99.546% (1975/1984)\n",
      "60 234 Train Loss: 0.016 | Train Acc: 99.641% (3890/3904)\n",
      "90 234 Train Loss: 0.014 | Train Acc: 99.708% (5807/5824)\n",
      "120 234 Train Loss: 0.014 | Train Acc: 99.677% (7719/7744)\n",
      "150 234 Train Loss: 0.015 | Train Acc: 99.607% (9626/9664)\n",
      "180 234 Train Loss: 0.015 | Train Acc: 99.586% (11536/11584)\n",
      "210 234 Train Loss: 0.016 | Train Acc: 99.519% (13439/13504)\n",
      "234 Epoch: 28 | Train Loss: 0.017 | Train Acc: 99.499% (14901/14976)\n",
      "0 234 Test Loss: 0.927 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 1.095 | Test Acc: 74.748% (1483/1984)\n",
      "60 234 Test Loss: 1.060 | Test Acc: 74.923% (2925/3904)\n",
      "90 234 Test Loss: 1.045 | Test Acc: 74.948% (4365/5824)\n",
      "120 234 Test Loss: 1.061 | Test Acc: 74.626% (5779/7744)\n",
      "150 234 Test Loss: 1.053 | Test Acc: 74.783% (7227/9664)\n",
      "180 234 Test Loss: 1.052 | Test Acc: 74.888% (8675/11584)\n",
      "210 234 Test Loss: 1.058 | Test Acc: 74.822% (10104/13504)\n",
      "234 Epoch: 28 | Test Loss: 1.061 | Test Acc: 74.666% (11182/14976)\n",
      "\n",
      "Epoch: 29\n",
      "0 234 Train Loss: 0.007 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.011 | Train Acc: 99.899% (1982/1984)\n",
      "60 234 Train Loss: 0.012 | Train Acc: 99.872% (3899/3904)\n",
      "90 234 Train Loss: 0.012 | Train Acc: 99.811% (5813/5824)\n",
      "120 234 Train Loss: 0.013 | Train Acc: 99.768% (7726/7744)\n",
      "150 234 Train Loss: 0.014 | Train Acc: 99.700% (9635/9664)\n",
      "180 234 Train Loss: 0.016 | Train Acc: 99.637% (11542/11584)\n",
      "210 234 Train Loss: 0.017 | Train Acc: 99.593% (13449/13504)\n",
      "234 Epoch: 29 | Train Loss: 0.017 | Train Acc: 99.613% (14918/14976)\n",
      "0 234 Test Loss: 1.322 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.080 | Test Acc: 75.252% (1493/1984)\n",
      "60 234 Test Loss: 1.126 | Test Acc: 74.052% (2891/3904)\n",
      "90 234 Test Loss: 1.083 | Test Acc: 74.897% (4362/5824)\n",
      "120 234 Test Loss: 1.079 | Test Acc: 74.845% (5796/7744)\n",
      "150 234 Test Loss: 1.067 | Test Acc: 75.083% (7256/9664)\n",
      "180 234 Test Loss: 1.071 | Test Acc: 74.948% (8682/11584)\n",
      "210 234 Test Loss: 1.076 | Test Acc: 74.926% (10118/13504)\n",
      "234 Epoch: 29 | Test Loss: 1.080 | Test Acc: 74.840% (11208/14976)\n",
      "\n",
      "Epoch: 30\n",
      "0 234 Train Loss: 0.011 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.011 | Train Acc: 99.798% (1980/1984)\n",
      "60 234 Train Loss: 0.011 | Train Acc: 99.821% (3897/3904)\n",
      "90 234 Train Loss: 0.009 | Train Acc: 99.845% (5815/5824)\n",
      "120 234 Train Loss: 0.009 | Train Acc: 99.871% (7734/7744)\n",
      "150 234 Train Loss: 0.010 | Train Acc: 99.845% (9649/9664)\n",
      "180 234 Train Loss: 0.010 | Train Acc: 99.793% (11560/11584)\n",
      "210 234 Train Loss: 0.011 | Train Acc: 99.756% (13471/13504)\n",
      "234 Epoch: 30 | Train Loss: 0.012 | Train Acc: 99.733% (14936/14976)\n",
      "0 234 Test Loss: 1.268 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.111 | Test Acc: 72.681% (1442/1984)\n",
      "60 234 Test Loss: 1.108 | Test Acc: 73.335% (2863/3904)\n",
      "90 234 Test Loss: 1.112 | Test Acc: 73.455% (4278/5824)\n",
      "120 234 Test Loss: 1.101 | Test Acc: 73.851% (5719/7744)\n",
      "150 234 Test Loss: 1.114 | Test Acc: 73.644% (7117/9664)\n",
      "180 234 Test Loss: 1.117 | Test Acc: 73.567% (8522/11584)\n",
      "210 234 Test Loss: 1.120 | Test Acc: 73.674% (9949/13504)\n",
      "234 Epoch: 30 | Test Loss: 1.117 | Test Acc: 73.705% (11038/14976)\n",
      "\n",
      "Epoch: 31\n",
      "0 234 Train Loss: 0.005 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.008 | Train Acc: 99.849% (1981/1984)\n",
      "60 234 Train Loss: 0.011 | Train Acc: 99.744% (3894/3904)\n",
      "90 234 Train Loss: 0.013 | Train Acc: 99.691% (5806/5824)\n",
      "120 234 Train Loss: 0.012 | Train Acc: 99.677% (7719/7744)\n",
      "150 234 Train Loss: 0.014 | Train Acc: 99.607% (9626/9664)\n",
      "180 234 Train Loss: 0.014 | Train Acc: 99.612% (11539/11584)\n",
      "210 234 Train Loss: 0.014 | Train Acc: 99.615% (13452/13504)\n",
      "234 Epoch: 31 | Train Loss: 0.014 | Train Acc: 99.613% (14918/14976)\n",
      "0 234 Test Loss: 0.859 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 1.121 | Test Acc: 74.597% (1480/1984)\n",
      "60 234 Test Loss: 1.054 | Test Acc: 75.231% (2937/3904)\n",
      "90 234 Test Loss: 1.034 | Test Acc: 75.996% (4426/5824)\n",
      "120 234 Test Loss: 1.045 | Test Acc: 75.775% (5868/7744)\n",
      "150 234 Test Loss: 1.035 | Test Acc: 75.683% (7314/9664)\n",
      "180 234 Test Loss: 1.040 | Test Acc: 75.501% (8746/11584)\n",
      "210 234 Test Loss: 1.032 | Test Acc: 75.689% (10221/13504)\n",
      "234 Epoch: 31 | Test Loss: 1.044 | Test Acc: 75.467% (11302/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 32\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.011 | Train Acc: 99.698% (1978/1984)\n",
      "60 234 Train Loss: 0.014 | Train Acc: 99.590% (3888/3904)\n",
      "90 234 Train Loss: 0.014 | Train Acc: 99.571% (5799/5824)\n",
      "120 234 Train Loss: 0.015 | Train Acc: 99.535% (7708/7744)\n",
      "150 234 Train Loss: 0.018 | Train Acc: 99.452% (9611/9664)\n",
      "180 234 Train Loss: 0.019 | Train Acc: 99.404% (11515/11584)\n",
      "210 234 Train Loss: 0.022 | Train Acc: 99.282% (13407/13504)\n",
      "234 Epoch: 32 | Train Loss: 0.023 | Train Acc: 99.272% (14867/14976)\n",
      "0 234 Test Loss: 1.897 | Test Acc: 60.938% (39/64)\n",
      "30 234 Test Loss: 1.116 | Test Acc: 73.589% (1460/1984)\n",
      "60 234 Test Loss: 1.102 | Test Acc: 73.694% (2877/3904)\n",
      "90 234 Test Loss: 1.089 | Test Acc: 73.953% (4307/5824)\n",
      "120 234 Test Loss: 1.112 | Test Acc: 73.631% (5702/7744)\n",
      "150 234 Test Loss: 1.121 | Test Acc: 73.500% (7103/9664)\n",
      "180 234 Test Loss: 1.147 | Test Acc: 73.058% (8463/11584)\n",
      "210 234 Test Loss: 1.154 | Test Acc: 73.075% (9868/13504)\n",
      "234 Epoch: 32 | Test Loss: 1.144 | Test Acc: 73.137% (10953/14976)\n",
      "\n",
      "Epoch: 33\n",
      "0 234 Train Loss: 0.009 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.023 | Train Acc: 99.194% (1968/1984)\n",
      "60 234 Train Loss: 0.020 | Train Acc: 99.411% (3881/3904)\n",
      "90 234 Train Loss: 0.018 | Train Acc: 99.519% (5796/5824)\n",
      "120 234 Train Loss: 0.016 | Train Acc: 99.600% (7713/7744)\n",
      "150 234 Train Loss: 0.017 | Train Acc: 99.576% (9623/9664)\n",
      "180 234 Train Loss: 0.020 | Train Acc: 99.448% (11520/11584)\n",
      "210 234 Train Loss: 0.023 | Train Acc: 99.319% (13412/13504)\n",
      "234 Epoch: 33 | Train Loss: 0.025 | Train Acc: 99.272% (14867/14976)\n",
      "0 234 Test Loss: 1.901 | Test Acc: 57.812% (37/64)\n",
      "30 234 Test Loss: 1.114 | Test Acc: 72.530% (1439/1984)\n",
      "60 234 Test Loss: 1.126 | Test Acc: 72.951% (2848/3904)\n",
      "90 234 Test Loss: 1.113 | Test Acc: 73.025% (4253/5824)\n",
      "120 234 Test Loss: 1.134 | Test Acc: 72.818% (5639/7744)\n",
      "150 234 Test Loss: 1.115 | Test Acc: 73.375% (7091/9664)\n",
      "180 234 Test Loss: 1.137 | Test Acc: 73.196% (8479/11584)\n",
      "210 234 Test Loss: 1.151 | Test Acc: 72.956% (9852/13504)\n",
      "234 Epoch: 33 | Test Loss: 1.143 | Test Acc: 73.050% (10940/14976)\n",
      "\n",
      "Epoch: 34\n",
      "0 234 Train Loss: 0.033 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.021 | Train Acc: 99.345% (1971/1984)\n",
      "60 234 Train Loss: 0.023 | Train Acc: 99.308% (3877/3904)\n",
      "90 234 Train Loss: 0.024 | Train Acc: 99.245% (5780/5824)\n",
      "120 234 Train Loss: 0.022 | Train Acc: 99.329% (7692/7744)\n",
      "150 234 Train Loss: 0.024 | Train Acc: 99.214% (9588/9664)\n",
      "180 234 Train Loss: 0.024 | Train Acc: 99.258% (11498/11584)\n",
      "210 234 Train Loss: 0.026 | Train Acc: 99.208% (13397/13504)\n",
      "234 Epoch: 34 | Train Loss: 0.027 | Train Acc: 99.125% (14845/14976)\n",
      "0 234 Test Loss: 1.616 | Test Acc: 57.812% (37/64)\n",
      "30 234 Test Loss: 1.229 | Test Acc: 71.169% (1412/1984)\n",
      "60 234 Test Loss: 1.185 | Test Acc: 72.439% (2828/3904)\n",
      "90 234 Test Loss: 1.183 | Test Acc: 72.442% (4219/5824)\n",
      "120 234 Test Loss: 1.156 | Test Acc: 72.843% (5641/7744)\n",
      "150 234 Test Loss: 1.155 | Test Acc: 72.962% (7051/9664)\n",
      "180 234 Test Loss: 1.167 | Test Acc: 72.850% (8439/11584)\n",
      "210 234 Test Loss: 1.168 | Test Acc: 72.934% (9849/13504)\n",
      "234 Epoch: 34 | Test Loss: 1.167 | Test Acc: 72.937% (10923/14976)\n",
      "\n",
      "Epoch: 35\n",
      "0 234 Train Loss: 0.009 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.044 | Train Acc: 98.589% (1956/1984)\n",
      "60 234 Train Loss: 0.047 | Train Acc: 98.438% (3843/3904)\n",
      "90 234 Train Loss: 0.048 | Train Acc: 98.403% (5731/5824)\n",
      "120 234 Train Loss: 0.053 | Train Acc: 98.218% (7606/7744)\n",
      "150 234 Train Loss: 0.054 | Train Acc: 98.231% (9493/9664)\n",
      "180 234 Train Loss: 0.056 | Train Acc: 98.179% (11373/11584)\n",
      "210 234 Train Loss: 0.060 | Train Acc: 98.038% (13239/13504)\n",
      "234 Epoch: 35 | Train Loss: 0.064 | Train Acc: 97.930% (14666/14976)\n",
      "0 234 Test Loss: 0.810 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 1.293 | Test Acc: 68.397% (1357/1984)\n",
      "60 234 Test Loss: 1.316 | Test Acc: 68.724% (2683/3904)\n",
      "90 234 Test Loss: 1.322 | Test Acc: 68.355% (3981/5824)\n",
      "120 234 Test Loss: 1.319 | Test Acc: 68.427% (5299/7744)\n",
      "150 234 Test Loss: 1.307 | Test Acc: 68.615% (6631/9664)\n",
      "180 234 Test Loss: 1.307 | Test Acc: 68.612% (7948/11584)\n",
      "210 234 Test Loss: 1.303 | Test Acc: 68.728% (9281/13504)\n",
      "234 Epoch: 35 | Test Loss: 1.301 | Test Acc: 68.857% (10312/14976)\n",
      "\n",
      "Epoch: 36\n",
      "0 234 Train Loss: 0.075 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.079 | Train Acc: 97.429% (1933/1984)\n",
      "60 234 Train Loss: 0.069 | Train Acc: 97.772% (3817/3904)\n",
      "90 234 Train Loss: 0.068 | Train Acc: 97.819% (5697/5824)\n",
      "120 234 Train Loss: 0.065 | Train Acc: 97.908% (7582/7744)\n",
      "150 234 Train Loss: 0.064 | Train Acc: 97.889% (9460/9664)\n",
      "180 234 Train Loss: 0.062 | Train Acc: 97.963% (11348/11584)\n",
      "210 234 Train Loss: 0.062 | Train Acc: 97.971% (13230/13504)\n",
      "234 Epoch: 36 | Train Loss: 0.062 | Train Acc: 98.003% (14677/14976)\n",
      "0 234 Test Loss: 1.202 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 1.092 | Test Acc: 72.933% (1447/1984)\n",
      "60 234 Test Loss: 1.111 | Test Acc: 72.643% (2836/3904)\n",
      "90 234 Test Loss: 1.117 | Test Acc: 72.527% (4224/5824)\n",
      "120 234 Test Loss: 1.127 | Test Acc: 72.301% (5599/7744)\n",
      "150 234 Test Loss: 1.139 | Test Acc: 72.051% (6963/9664)\n",
      "180 234 Test Loss: 1.133 | Test Acc: 72.143% (8357/11584)\n",
      "210 234 Test Loss: 1.133 | Test Acc: 72.119% (9739/13504)\n",
      "234 Epoch: 36 | Test Loss: 1.138 | Test Acc: 72.009% (10784/14976)\n",
      "\n",
      "Epoch: 37\n",
      "0 234 Train Loss: 0.012 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.039 | Train Acc: 98.690% (1958/1984)\n",
      "60 234 Train Loss: 0.040 | Train Acc: 98.796% (3857/3904)\n",
      "90 234 Train Loss: 0.037 | Train Acc: 98.935% (5762/5824)\n",
      "120 234 Train Loss: 0.036 | Train Acc: 98.928% (7661/7744)\n",
      "150 234 Train Loss: 0.034 | Train Acc: 98.976% (9565/9664)\n",
      "180 234 Train Loss: 0.034 | Train Acc: 98.973% (11465/11584)\n",
      "210 234 Train Loss: 0.035 | Train Acc: 98.971% (13365/13504)\n",
      "234 Epoch: 37 | Train Loss: 0.035 | Train Acc: 98.965% (14821/14976)\n",
      "0 234 Test Loss: 1.352 | Test Acc: 64.062% (41/64)\n",
      "30 234 Test Loss: 1.042 | Test Acc: 74.597% (1480/1984)\n",
      "60 234 Test Loss: 1.085 | Test Acc: 74.103% (2893/3904)\n",
      "90 234 Test Loss: 1.081 | Test Acc: 74.056% (4313/5824)\n",
      "120 234 Test Loss: 1.078 | Test Acc: 73.980% (5729/7744)\n",
      "150 234 Test Loss: 1.090 | Test Acc: 73.800% (7132/9664)\n",
      "180 234 Test Loss: 1.086 | Test Acc: 73.800% (8549/11584)\n",
      "210 234 Test Loss: 1.089 | Test Acc: 73.586% (9937/13504)\n",
      "234 Epoch: 37 | Test Loss: 1.081 | Test Acc: 73.698% (11037/14976)\n",
      "\n",
      "Epoch: 38\n",
      "0 234 Train Loss: 0.042 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.019 | Train Acc: 99.496% (1974/1984)\n",
      "60 234 Train Loss: 0.019 | Train Acc: 99.539% (3886/3904)\n",
      "90 234 Train Loss: 0.020 | Train Acc: 99.451% (5792/5824)\n",
      "120 234 Train Loss: 0.020 | Train Acc: 99.445% (7701/7744)\n",
      "150 234 Train Loss: 0.020 | Train Acc: 99.483% (9614/9664)\n",
      "180 234 Train Loss: 0.021 | Train Acc: 99.422% (11517/11584)\n",
      "210 234 Train Loss: 0.021 | Train Acc: 99.437% (13428/13504)\n",
      "234 Epoch: 38 | Train Loss: 0.021 | Train Acc: 99.432% (14891/14976)\n",
      "0 234 Test Loss: 0.819 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 1.094 | Test Acc: 72.379% (1436/1984)\n",
      "60 234 Test Loss: 1.115 | Test Acc: 72.746% (2840/3904)\n",
      "90 234 Test Loss: 1.097 | Test Acc: 73.180% (4262/5824)\n",
      "120 234 Test Loss: 1.088 | Test Acc: 73.321% (5678/7744)\n",
      "150 234 Test Loss: 1.098 | Test Acc: 72.899% (7045/9664)\n",
      "180 234 Test Loss: 1.092 | Test Acc: 73.127% (8471/11584)\n",
      "210 234 Test Loss: 1.088 | Test Acc: 73.356% (9906/13504)\n",
      "234 Epoch: 38 | Test Loss: 1.084 | Test Acc: 73.364% (10987/14976)\n",
      "\n",
      "Epoch: 39\n",
      "0 234 Train Loss: 0.016 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.020 | Train Acc: 99.294% (1970/1984)\n",
      "60 234 Train Loss: 0.018 | Train Acc: 99.513% (3885/3904)\n",
      "90 234 Train Loss: 0.021 | Train Acc: 99.416% (5790/5824)\n",
      "120 234 Train Loss: 0.022 | Train Acc: 99.406% (7698/7744)\n",
      "150 234 Train Loss: 0.024 | Train Acc: 99.338% (9600/9664)\n",
      "180 234 Train Loss: 0.025 | Train Acc: 99.292% (11502/11584)\n",
      "210 234 Train Loss: 0.025 | Train Acc: 99.282% (13407/13504)\n",
      "234 Epoch: 39 | Train Loss: 0.026 | Train Acc: 99.239% (14862/14976)\n",
      "0 234 Test Loss: 1.723 | Test Acc: 62.500% (40/64)\n",
      "30 234 Test Loss: 1.071 | Test Acc: 73.740% (1463/1984)\n",
      "60 234 Test Loss: 1.145 | Test Acc: 72.464% (2829/3904)\n",
      "90 234 Test Loss: 1.126 | Test Acc: 72.854% (4243/5824)\n",
      "120 234 Test Loss: 1.135 | Test Acc: 72.921% (5647/7744)\n",
      "150 234 Test Loss: 1.141 | Test Acc: 72.734% (7029/9664)\n",
      "180 234 Test Loss: 1.133 | Test Acc: 72.954% (8451/11584)\n",
      "210 234 Test Loss: 1.133 | Test Acc: 72.964% (9853/13504)\n",
      "234 Epoch: 39 | Test Loss: 1.130 | Test Acc: 72.857% (10911/14976)\n",
      "\n",
      "Epoch: 40\n",
      "0 234 Train Loss: 0.063 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.020 | Train Acc: 99.496% (1974/1984)\n",
      "60 234 Train Loss: 0.019 | Train Acc: 99.462% (3883/3904)\n",
      "90 234 Train Loss: 0.022 | Train Acc: 99.313% (5784/5824)\n",
      "120 234 Train Loss: 0.021 | Train Acc: 99.341% (7693/7744)\n",
      "150 234 Train Loss: 0.021 | Train Acc: 99.389% (9605/9664)\n",
      "180 234 Train Loss: 0.020 | Train Acc: 99.430% (11518/11584)\n",
      "210 234 Train Loss: 0.021 | Train Acc: 99.408% (13424/13504)\n",
      "234 Epoch: 40 | Train Loss: 0.020 | Train Acc: 99.439% (14892/14976)\n",
      "0 234 Test Loss: 0.984 | Test Acc: 71.875% (46/64)\n",
      "30 234 Test Loss: 0.940 | Test Acc: 76.562% (1519/1984)\n",
      "60 234 Test Loss: 1.005 | Test Acc: 75.768% (2958/3904)\n",
      "90 234 Test Loss: 0.967 | Test Acc: 76.065% (4430/5824)\n",
      "120 234 Test Loss: 0.975 | Test Acc: 75.878% (5876/7744)\n",
      "150 234 Test Loss: 0.991 | Test Acc: 75.673% (7313/9664)\n",
      "180 234 Test Loss: 0.991 | Test Acc: 75.691% (8768/11584)\n",
      "210 234 Test Loss: 0.990 | Test Acc: 75.755% (10230/13504)\n",
      "234 Epoch: 40 | Test Loss: 0.989 | Test Acc: 75.708% (11338/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 41\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.012 | Train Acc: 99.798% (1980/1984)\n",
      "60 234 Train Loss: 0.012 | Train Acc: 99.718% (3893/3904)\n",
      "90 234 Train Loss: 0.013 | Train Acc: 99.725% (5808/5824)\n",
      "120 234 Train Loss: 0.013 | Train Acc: 99.703% (7721/7744)\n",
      "150 234 Train Loss: 0.013 | Train Acc: 99.669% (9632/9664)\n",
      "180 234 Train Loss: 0.015 | Train Acc: 99.594% (11537/11584)\n",
      "210 234 Train Loss: 0.016 | Train Acc: 99.548% (13443/13504)\n",
      "234 Epoch: 41 | Train Loss: 0.017 | Train Acc: 99.526% (14905/14976)\n",
      "0 234 Test Loss: 1.489 | Test Acc: 62.500% (40/64)\n",
      "30 234 Test Loss: 1.018 | Test Acc: 75.050% (1489/1984)\n",
      "60 234 Test Loss: 1.002 | Test Acc: 74.693% (2916/3904)\n",
      "90 234 Test Loss: 1.012 | Test Acc: 74.519% (4340/5824)\n",
      "120 234 Test Loss: 1.010 | Test Acc: 74.651% (5781/7744)\n",
      "150 234 Test Loss: 1.030 | Test Acc: 74.421% (7192/9664)\n",
      "180 234 Test Loss: 1.039 | Test Acc: 74.404% (8619/11584)\n",
      "210 234 Test Loss: 1.045 | Test Acc: 74.371% (10043/13504)\n",
      "234 Epoch: 41 | Test Loss: 1.030 | Test Acc: 74.633% (11177/14976)\n",
      "\n",
      "Epoch: 42\n",
      "0 234 Train Loss: 0.009 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.019 | Train Acc: 99.546% (1975/1984)\n",
      "60 234 Train Loss: 0.017 | Train Acc: 99.539% (3886/3904)\n",
      "90 234 Train Loss: 0.015 | Train Acc: 99.622% (5802/5824)\n",
      "120 234 Train Loss: 0.015 | Train Acc: 99.626% (7715/7744)\n",
      "150 234 Train Loss: 0.015 | Train Acc: 99.659% (9631/9664)\n",
      "180 234 Train Loss: 0.015 | Train Acc: 99.646% (11543/11584)\n",
      "210 234 Train Loss: 0.015 | Train Acc: 99.659% (13458/13504)\n",
      "234 Epoch: 42 | Train Loss: 0.015 | Train Acc: 99.659% (14925/14976)\n",
      "0 234 Test Loss: 0.917 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 0.971 | Test Acc: 76.714% (1522/1984)\n",
      "60 234 Test Loss: 1.007 | Test Acc: 75.768% (2958/3904)\n",
      "90 234 Test Loss: 1.024 | Test Acc: 75.584% (4402/5824)\n",
      "120 234 Test Loss: 1.029 | Test Acc: 75.594% (5854/7744)\n",
      "150 234 Test Loss: 1.033 | Test Acc: 75.341% (7281/9664)\n",
      "180 234 Test Loss: 1.019 | Test Acc: 75.319% (8725/11584)\n",
      "210 234 Test Loss: 1.029 | Test Acc: 75.059% (10136/13504)\n",
      "234 Epoch: 42 | Test Loss: 1.034 | Test Acc: 74.980% (11229/14976)\n",
      "\n",
      "Epoch: 43\n",
      "0 234 Train Loss: 0.006 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.014 | Train Acc: 99.647% (1977/1984)\n",
      "60 234 Train Loss: 0.018 | Train Acc: 99.488% (3884/3904)\n",
      "90 234 Train Loss: 0.018 | Train Acc: 99.416% (5790/5824)\n",
      "120 234 Train Loss: 0.017 | Train Acc: 99.483% (7704/7744)\n",
      "150 234 Train Loss: 0.016 | Train Acc: 99.545% (9620/9664)\n",
      "180 234 Train Loss: 0.017 | Train Acc: 99.508% (11527/11584)\n",
      "210 234 Train Loss: 0.019 | Train Acc: 99.452% (13430/13504)\n",
      "234 Epoch: 43 | Train Loss: 0.019 | Train Acc: 99.446% (14893/14976)\n",
      "0 234 Test Loss: 0.853 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 1.082 | Test Acc: 73.690% (1462/1984)\n",
      "60 234 Test Loss: 1.104 | Test Acc: 73.284% (2861/3904)\n",
      "90 234 Test Loss: 1.106 | Test Acc: 73.472% (4279/5824)\n",
      "120 234 Test Loss: 1.096 | Test Acc: 73.864% (5720/7744)\n",
      "150 234 Test Loss: 1.103 | Test Acc: 73.489% (7102/9664)\n",
      "180 234 Test Loss: 1.100 | Test Acc: 73.679% (8535/11584)\n",
      "210 234 Test Loss: 1.114 | Test Acc: 73.408% (9913/13504)\n",
      "234 Epoch: 43 | Test Loss: 1.119 | Test Acc: 73.304% (10978/14976)\n",
      "\n",
      "Epoch: 44\n",
      "0 234 Train Loss: 0.006 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.013 | Train Acc: 99.698% (1978/1984)\n",
      "60 234 Train Loss: 0.015 | Train Acc: 99.641% (3890/3904)\n",
      "90 234 Train Loss: 0.016 | Train Acc: 99.622% (5802/5824)\n",
      "120 234 Train Loss: 0.018 | Train Acc: 99.535% (7708/7744)\n",
      "150 234 Train Loss: 0.018 | Train Acc: 99.503% (9616/9664)\n",
      "180 234 Train Loss: 0.020 | Train Acc: 99.448% (11520/11584)\n",
      "210 234 Train Loss: 0.021 | Train Acc: 99.393% (13422/13504)\n",
      "234 Epoch: 44 | Train Loss: 0.022 | Train Acc: 99.366% (14881/14976)\n",
      "0 234 Test Loss: 1.154 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 1.140 | Test Acc: 73.438% (1457/1984)\n",
      "60 234 Test Loss: 1.127 | Test Acc: 73.719% (2878/3904)\n",
      "90 234 Test Loss: 1.118 | Test Acc: 73.729% (4294/5824)\n",
      "120 234 Test Loss: 1.126 | Test Acc: 73.657% (5704/7744)\n",
      "150 234 Test Loss: 1.129 | Test Acc: 73.551% (7108/9664)\n",
      "180 234 Test Loss: 1.117 | Test Acc: 73.817% (8551/11584)\n",
      "210 234 Test Loss: 1.118 | Test Acc: 73.741% (9958/13504)\n",
      "234 Epoch: 44 | Test Loss: 1.122 | Test Acc: 73.631% (11027/14976)\n",
      "\n",
      "Epoch: 45\n",
      "0 234 Train Loss: 0.050 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.037 | Train Acc: 98.790% (1960/1984)\n",
      "60 234 Train Loss: 0.029 | Train Acc: 99.129% (3870/3904)\n",
      "90 234 Train Loss: 0.026 | Train Acc: 99.245% (5780/5824)\n",
      "120 234 Train Loss: 0.023 | Train Acc: 99.316% (7691/7744)\n",
      "150 234 Train Loss: 0.022 | Train Acc: 99.358% (9602/9664)\n",
      "180 234 Train Loss: 0.022 | Train Acc: 99.387% (11513/11584)\n",
      "210 234 Train Loss: 0.023 | Train Acc: 99.371% (13419/13504)\n",
      "234 Epoch: 45 | Train Loss: 0.024 | Train Acc: 99.306% (14872/14976)\n",
      "0 234 Test Loss: 1.141 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 1.078 | Test Acc: 74.194% (1472/1984)\n",
      "60 234 Test Loss: 1.072 | Test Acc: 74.436% (2906/3904)\n",
      "90 234 Test Loss: 1.059 | Test Acc: 74.605% (4345/5824)\n",
      "120 234 Test Loss: 1.065 | Test Acc: 74.600% (5777/7744)\n",
      "150 234 Test Loss: 1.064 | Test Acc: 74.638% (7213/9664)\n",
      "180 234 Test Loss: 1.063 | Test Acc: 74.482% (8628/11584)\n",
      "210 234 Test Loss: 1.058 | Test Acc: 74.711% (10089/13504)\n",
      "234 Epoch: 45 | Test Loss: 1.057 | Test Acc: 74.793% (11201/14976)\n",
      "\n",
      "Epoch: 46\n",
      "0 234 Train Loss: 0.040 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.022 | Train Acc: 99.546% (1975/1984)\n",
      "60 234 Train Loss: 0.025 | Train Acc: 99.462% (3883/3904)\n",
      "90 234 Train Loss: 0.029 | Train Acc: 99.262% (5781/5824)\n",
      "120 234 Train Loss: 0.028 | Train Acc: 99.277% (7688/7744)\n",
      "150 234 Train Loss: 0.029 | Train Acc: 99.214% (9588/9664)\n",
      "180 234 Train Loss: 0.028 | Train Acc: 99.214% (11493/11584)\n",
      "210 234 Train Loss: 0.028 | Train Acc: 99.208% (13397/13504)\n",
      "234 Epoch: 46 | Train Loss: 0.028 | Train Acc: 99.199% (14856/14976)\n",
      "0 234 Test Loss: 0.838 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 1.015 | Test Acc: 73.589% (1460/1984)\n",
      "60 234 Test Loss: 1.039 | Test Acc: 74.283% (2900/3904)\n",
      "90 234 Test Loss: 1.058 | Test Acc: 74.176% (4320/5824)\n",
      "120 234 Test Loss: 1.056 | Test Acc: 74.251% (5750/7744)\n",
      "150 234 Test Loss: 1.073 | Test Acc: 73.934% (7145/9664)\n",
      "180 234 Test Loss: 1.081 | Test Acc: 73.774% (8546/11584)\n",
      "210 234 Test Loss: 1.069 | Test Acc: 74.008% (9994/13504)\n",
      "234 Epoch: 46 | Test Loss: 1.070 | Test Acc: 73.918% (11070/14976)\n",
      "\n",
      "Epoch: 47\n",
      "0 234 Train Loss: 0.008 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.024 | Train Acc: 99.244% (1969/1984)\n",
      "60 234 Train Loss: 0.025 | Train Acc: 99.206% (3873/3904)\n",
      "90 234 Train Loss: 0.029 | Train Acc: 99.107% (5772/5824)\n",
      "120 234 Train Loss: 0.030 | Train Acc: 99.032% (7669/7744)\n",
      "150 234 Train Loss: 0.029 | Train Acc: 99.089% (9576/9664)\n",
      "180 234 Train Loss: 0.028 | Train Acc: 99.119% (11482/11584)\n",
      "210 234 Train Loss: 0.028 | Train Acc: 99.082% (13380/13504)\n",
      "234 Epoch: 47 | Train Loss: 0.029 | Train Acc: 99.065% (14836/14976)\n",
      "0 234 Test Loss: 0.874 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 1.164 | Test Acc: 73.085% (1450/1984)\n",
      "60 234 Test Loss: 1.125 | Test Acc: 73.591% (2873/3904)\n",
      "90 234 Test Loss: 1.109 | Test Acc: 73.970% (4308/5824)\n",
      "120 234 Test Loss: 1.104 | Test Acc: 73.941% (5726/7744)\n",
      "150 234 Test Loss: 1.106 | Test Acc: 73.862% (7138/9664)\n",
      "180 234 Test Loss: 1.112 | Test Acc: 73.576% (8523/11584)\n",
      "210 234 Test Loss: 1.113 | Test Acc: 73.363% (9907/13504)\n",
      "234 Epoch: 47 | Test Loss: 1.113 | Test Acc: 73.304% (10978/14976)\n",
      "\n",
      "Epoch: 48\n",
      "0 234 Train Loss: 0.042 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.043 | Train Acc: 98.589% (1956/1984)\n",
      "60 234 Train Loss: 0.044 | Train Acc: 98.566% (3848/3904)\n",
      "90 234 Train Loss: 0.049 | Train Acc: 98.352% (5728/5824)\n",
      "120 234 Train Loss: 0.057 | Train Acc: 98.140% (7600/7744)\n",
      "150 234 Train Loss: 0.060 | Train Acc: 98.096% (9480/9664)\n",
      "180 234 Train Loss: 0.061 | Train Acc: 98.023% (11355/11584)\n",
      "210 234 Train Loss: 0.062 | Train Acc: 98.045% (13240/13504)\n",
      "234 Epoch: 48 | Train Loss: 0.061 | Train Acc: 98.090% (14690/14976)\n",
      "0 234 Test Loss: 0.915 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 1.124 | Test Acc: 70.665% (1402/1984)\n",
      "60 234 Test Loss: 1.078 | Test Acc: 71.926% (2808/3904)\n",
      "90 234 Test Loss: 1.126 | Test Acc: 71.137% (4143/5824)\n",
      "120 234 Test Loss: 1.118 | Test Acc: 71.488% (5536/7744)\n",
      "150 234 Test Loss: 1.114 | Test Acc: 71.471% (6907/9664)\n",
      "180 234 Test Loss: 1.116 | Test Acc: 71.400% (8271/11584)\n",
      "210 234 Test Loss: 1.116 | Test Acc: 71.594% (9668/13504)\n",
      "234 Epoch: 48 | Test Loss: 1.119 | Test Acc: 71.601% (10723/14976)\n",
      "\n",
      "Epoch: 49\n",
      "0 234 Train Loss: 0.042 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.050 | Train Acc: 98.337% (1951/1984)\n",
      "60 234 Train Loss: 0.051 | Train Acc: 98.309% (3838/3904)\n",
      "90 234 Train Loss: 0.048 | Train Acc: 98.541% (5739/5824)\n",
      "120 234 Train Loss: 0.045 | Train Acc: 98.631% (7638/7744)\n",
      "150 234 Train Loss: 0.044 | Train Acc: 98.675% (9536/9664)\n",
      "180 234 Train Loss: 0.043 | Train Acc: 98.731% (11437/11584)\n",
      "210 234 Train Loss: 0.041 | Train Acc: 98.823% (13345/13504)\n",
      "234 Epoch: 49 | Train Loss: 0.040 | Train Acc: 98.825% (14800/14976)\n",
      "0 234 Test Loss: 0.983 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 0.992 | Test Acc: 75.554% (1499/1984)\n",
      "60 234 Test Loss: 1.015 | Test Acc: 74.795% (2920/3904)\n",
      "90 234 Test Loss: 1.056 | Test Acc: 74.193% (4321/5824)\n",
      "120 234 Test Loss: 1.051 | Test Acc: 74.329% (5756/7744)\n",
      "150 234 Test Loss: 1.054 | Test Acc: 74.317% (7182/9664)\n",
      "180 234 Test Loss: 1.057 | Test Acc: 74.370% (8615/11584)\n",
      "210 234 Test Loss: 1.058 | Test Acc: 74.178% (10017/13504)\n",
      "234 Epoch: 49 | Test Loss: 1.060 | Test Acc: 74.072% (11093/14976)\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.2.3: Start Target model training\n",
    "\n",
    "max_epoch = 50  #@param {type:\"integer\"}\n",
    "train_result_summary = 'target_train_DCA-BiLSTM.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(target_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(target_testloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dgo9hPetJvNR",
    "outputId": "6efe1924-a3d1-4a97-f71d-4d2a08545054"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_training_summary(filepath = 'target_train_DCA-BiLSTM.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzWIh_S2JvZv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HH7wZAqCtzns"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBhaq_-_nrIp"
   },
   "source": [
    "## Method 3: VGG (with bi-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ns7kfygnrIp",
    "outputId": "dc0e5415-e981-4537-e0e2-3712979ece30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model for VGG-BiLSTM\n",
      "==> Training model from scratch..\n",
      "Total trained parameters:  51242762\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.3.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: VGG\n",
    "#@markdown * Option 2: VGG-LSTM\n",
    "#@markdown * Option 3: VGG-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'VGG-BiLSTM'  #@param {type:\"string\"}\n",
    "save_model_folder = './VGG-BiLSTM_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'VGG-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = VGG('VGG11', enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'VGG-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA('VGG11', enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'VGG':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA('VGG11', enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XgH78DF8nrIq",
    "outputId": "d2d43644-7cba-4a0d-b674-d159618d60dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Setting target_train_dataset size to  15000 Counter({0: 1538, 5: 1537, 9: 1533, 6: 1532, 3: 1504, 4: 1496, 2: 1487, 7: 1467, 1: 1459, 8: 1447})\n",
      "Setting target_test_dataset size to  15000 Counter({3: 1557, 4: 1534, 8: 1521, 7: 1519, 1: 1511, 5: 1492, 2: 1484, 9: 1482, 0: 1455, 6: 1445})\n",
      "Setting shadow_train_dataset size to  15000 Counter({1: 1542, 0: 1537, 8: 1523, 7: 1516, 2: 1515, 9: 1505, 5: 1503, 6: 1458, 4: 1456, 3: 1445})\n",
      "Setting shadow_test_dataset size to  15000 Counter({6: 1565, 2: 1514, 4: 1514, 8: 1509, 7: 1498, 3: 1494, 1: 1488, 9: 1480, 0: 1470, 5: 1468})\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.3.2: Setup Target and Shadow datasets for DLA Training\n",
    "\n",
    "target_train_size = 15000 #@param {type:\"integer\"}\n",
    "target_test_size= 15000 #@param {type:\"integer\"}\n",
    "shadow_train_size = 15000  #@param {type:\"integer\"} \n",
    "shadow_test_size= 15000 #@param {type:\"integer\"}\n",
    "\n",
    "# create dataset for renet \n",
    "print('==> Preparing dataset..')\n",
    "target_trainloader, target_testloader, shadow_train_dataset, shadow_testloader = create_cifar_dataset_torch(batch_size=batch_size, target_train_size = target_train_size, target_test_size= target_train_size, shadow_train_size = target_train_size, shadow_test_size= target_train_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-R7x4nGnrIq",
    "outputId": "f1cfb872-7756-4079-d211-707ebefa5b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 234 Train Loss: 2.303 | Train Acc: 9.375% (6/64)\n",
      "30 234 Train Loss: 2.136 | Train Acc: 22.278% (442/1984)\n",
      "60 234 Train Loss: 1.963 | Train Acc: 27.510% (1074/3904)\n",
      "90 234 Train Loss: 1.852 | Train Acc: 31.370% (1827/5824)\n",
      "120 234 Train Loss: 1.766 | Train Acc: 34.336% (2659/7744)\n",
      "150 234 Train Loss: 1.691 | Train Acc: 37.159% (3591/9664)\n",
      "180 234 Train Loss: 1.635 | Train Acc: 39.140% (4534/11584)\n",
      "210 234 Train Loss: 1.586 | Train Acc: 41.269% (5573/13504)\n",
      "234 Epoch: 0 | Train Loss: 1.552 | Train Acc: 42.581% (6377/14976)\n",
      "0 234 Test Loss: 1.499 | Test Acc: 51.562% (33/64)\n",
      "30 234 Test Loss: 1.701 | Test Acc: 42.742% (848/1984)\n",
      "60 234 Test Loss: 1.707 | Test Acc: 41.957% (1638/3904)\n",
      "90 234 Test Loss: 1.693 | Test Acc: 43.183% (2515/5824)\n",
      "120 234 Test Loss: 1.694 | Test Acc: 43.673% (3382/7744)\n",
      "150 234 Test Loss: 1.688 | Test Acc: 43.791% (4232/9664)\n",
      "180 234 Test Loss: 1.686 | Test Acc: 43.931% (5089/11584)\n",
      "210 234 Test Loss: 1.684 | Test Acc: 43.883% (5926/13504)\n",
      "234 Epoch: 0 | Test Loss: 1.688 | Test Acc: 43.610% (6531/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "0 234 Train Loss: 1.062 | Train Acc: 54.688% (35/64)\n",
      "30 234 Train Loss: 1.170 | Train Acc: 56.754% (1126/1984)\n",
      "60 234 Train Loss: 1.150 | Train Acc: 57.121% (2230/3904)\n",
      "90 234 Train Loss: 1.151 | Train Acc: 57.212% (3332/5824)\n",
      "120 234 Train Loss: 1.147 | Train Acc: 57.580% (4459/7744)\n",
      "150 234 Train Loss: 1.138 | Train Acc: 58.185% (5623/9664)\n",
      "180 234 Train Loss: 1.133 | Train Acc: 58.503% (6777/11584)\n",
      "210 234 Train Loss: 1.118 | Train Acc: 59.071% (7977/13504)\n",
      "234 Epoch: 1 | Train Loss: 1.104 | Train Acc: 59.615% (8928/14976)\n",
      "0 234 Test Loss: 1.421 | Test Acc: 50.000% (32/64)\n",
      "30 234 Test Loss: 1.286 | Test Acc: 55.746% (1106/1984)\n",
      "60 234 Test Loss: 1.303 | Test Acc: 54.329% (2121/3904)\n",
      "90 234 Test Loss: 1.315 | Test Acc: 53.932% (3141/5824)\n",
      "120 234 Test Loss: 1.314 | Test Acc: 53.951% (4178/7744)\n",
      "150 234 Test Loss: 1.312 | Test Acc: 54.025% (5221/9664)\n",
      "180 234 Test Loss: 1.305 | Test Acc: 54.472% (6310/11584)\n",
      "210 234 Test Loss: 1.297 | Test Acc: 54.688% (7385/13504)\n",
      "234 Epoch: 1 | Test Loss: 1.295 | Test Acc: 54.888% (8220/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "0 234 Train Loss: 0.899 | Train Acc: 60.938% (39/64)\n",
      "30 234 Train Loss: 0.832 | Train Acc: 70.413% (1397/1984)\n",
      "60 234 Train Loss: 0.842 | Train Acc: 69.826% (2726/3904)\n",
      "90 234 Train Loss: 0.863 | Train Acc: 69.128% (4026/5824)\n",
      "120 234 Train Loss: 0.868 | Train Acc: 69.034% (5346/7744)\n",
      "150 234 Train Loss: 0.861 | Train Acc: 69.257% (6693/9664)\n",
      "180 234 Train Loss: 0.855 | Train Acc: 69.596% (8062/11584)\n",
      "210 234 Train Loss: 0.851 | Train Acc: 69.868% (9435/13504)\n",
      "234 Epoch: 2 | Train Loss: 0.861 | Train Acc: 69.518% (10411/14976)\n",
      "0 234 Test Loss: 1.080 | Test Acc: 57.812% (37/64)\n",
      "30 234 Test Loss: 1.283 | Test Acc: 55.696% (1105/1984)\n",
      "60 234 Test Loss: 1.249 | Test Acc: 56.711% (2214/3904)\n",
      "90 234 Test Loss: 1.247 | Test Acc: 56.731% (3304/5824)\n",
      "120 234 Test Loss: 1.250 | Test Acc: 56.702% (4391/7744)\n",
      "150 234 Test Loss: 1.252 | Test Acc: 56.591% (5469/9664)\n",
      "180 234 Test Loss: 1.246 | Test Acc: 56.742% (6573/11584)\n",
      "210 234 Test Loss: 1.242 | Test Acc: 56.857% (7678/13504)\n",
      "234 Epoch: 2 | Test Loss: 1.240 | Test Acc: 57.051% (8544/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "0 234 Train Loss: 0.687 | Train Acc: 73.438% (47/64)\n",
      "30 234 Train Loss: 0.719 | Train Acc: 73.841% (1465/1984)\n",
      "60 234 Train Loss: 0.709 | Train Acc: 74.846% (2922/3904)\n",
      "90 234 Train Loss: 0.708 | Train Acc: 74.863% (4360/5824)\n",
      "120 234 Train Loss: 0.713 | Train Acc: 74.974% (5806/7744)\n",
      "150 234 Train Loss: 0.707 | Train Acc: 75.290% (7276/9664)\n",
      "180 234 Train Loss: 0.708 | Train Acc: 75.199% (8711/11584)\n",
      "210 234 Train Loss: 0.706 | Train Acc: 75.289% (10167/13504)\n",
      "234 Epoch: 3 | Train Loss: 0.705 | Train Acc: 75.347% (11284/14976)\n",
      "0 234 Test Loss: 0.972 | Test Acc: 65.625% (42/64)\n",
      "30 234 Test Loss: 1.006 | Test Acc: 66.129% (1312/1984)\n",
      "60 234 Test Loss: 0.992 | Test Acc: 66.393% (2592/3904)\n",
      "90 234 Test Loss: 0.997 | Test Acc: 66.655% (3882/5824)\n",
      "120 234 Test Loss: 0.998 | Test Acc: 66.723% (5167/7744)\n",
      "150 234 Test Loss: 0.994 | Test Acc: 66.639% (6440/9664)\n",
      "180 234 Test Loss: 0.996 | Test Acc: 66.695% (7726/11584)\n",
      "210 234 Test Loss: 0.992 | Test Acc: 66.884% (9032/13504)\n",
      "234 Epoch: 3 | Test Loss: 0.986 | Test Acc: 66.981% (10031/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "0 234 Train Loss: 0.444 | Train Acc: 84.375% (54/64)\n",
      "30 234 Train Loss: 0.495 | Train Acc: 83.518% (1657/1984)\n",
      "60 234 Train Loss: 0.495 | Train Acc: 82.838% (3234/3904)\n",
      "90 234 Train Loss: 0.503 | Train Acc: 82.383% (4798/5824)\n",
      "120 234 Train Loss: 0.523 | Train Acc: 81.689% (6326/7744)\n",
      "150 234 Train Loss: 0.539 | Train Acc: 81.105% (7838/9664)\n",
      "180 234 Train Loss: 0.541 | Train Acc: 81.112% (9396/11584)\n",
      "210 234 Train Loss: 0.544 | Train Acc: 80.946% (10931/13504)\n",
      "234 Epoch: 4 | Train Loss: 0.547 | Train Acc: 80.669% (12081/14976)\n",
      "0 234 Test Loss: 1.385 | Test Acc: 54.688% (35/64)\n",
      "30 234 Test Loss: 1.202 | Test Acc: 62.248% (1235/1984)\n",
      "60 234 Test Loss: 1.211 | Test Acc: 61.783% (2412/3904)\n",
      "90 234 Test Loss: 1.221 | Test Acc: 61.607% (3588/5824)\n",
      "120 234 Test Loss: 1.218 | Test Acc: 61.777% (4784/7744)\n",
      "150 234 Test Loss: 1.215 | Test Acc: 61.931% (5985/9664)\n",
      "180 234 Test Loss: 1.217 | Test Acc: 61.775% (7156/11584)\n",
      "210 234 Test Loss: 1.217 | Test Acc: 61.626% (8322/13504)\n",
      "234 Epoch: 4 | Test Loss: 1.218 | Test Acc: 61.632% (9230/14976)\n",
      "\n",
      "Epoch: 5\n",
      "0 234 Train Loss: 0.568 | Train Acc: 79.688% (51/64)\n",
      "30 234 Train Loss: 0.365 | Train Acc: 86.895% (1724/1984)\n",
      "60 234 Train Loss: 0.372 | Train Acc: 86.885% (3392/3904)\n",
      "90 234 Train Loss: 0.380 | Train Acc: 86.590% (5043/5824)\n",
      "120 234 Train Loss: 0.389 | Train Acc: 86.235% (6678/7744)\n",
      "150 234 Train Loss: 0.405 | Train Acc: 85.844% (8296/9664)\n",
      "180 234 Train Loss: 0.412 | Train Acc: 85.584% (9914/11584)\n",
      "210 234 Train Loss: 0.417 | Train Acc: 85.434% (11537/13504)\n",
      "234 Epoch: 5 | Train Loss: 0.426 | Train Acc: 85.136% (12750/14976)\n",
      "0 234 Test Loss: 1.215 | Test Acc: 64.062% (41/64)\n",
      "30 234 Test Loss: 1.135 | Test Acc: 66.532% (1320/1984)\n",
      "60 234 Test Loss: 1.157 | Test Acc: 66.035% (2578/3904)\n",
      "90 234 Test Loss: 1.169 | Test Acc: 65.625% (3822/5824)\n",
      "120 234 Test Loss: 1.170 | Test Acc: 65.677% (5086/7744)\n",
      "150 234 Test Loss: 1.177 | Test Acc: 65.635% (6343/9664)\n",
      "180 234 Test Loss: 1.166 | Test Acc: 65.832% (7626/11584)\n",
      "210 234 Test Loss: 1.162 | Test Acc: 65.743% (8878/13504)\n",
      "234 Epoch: 5 | Test Loss: 1.167 | Test Acc: 65.572% (9820/14976)\n",
      "\n",
      "Epoch: 6\n",
      "0 234 Train Loss: 0.354 | Train Acc: 89.062% (57/64)\n",
      "30 234 Train Loss: 0.278 | Train Acc: 91.331% (1812/1984)\n",
      "60 234 Train Loss: 0.276 | Train Acc: 91.163% (3559/3904)\n",
      "90 234 Train Loss: 0.276 | Train Acc: 91.106% (5306/5824)\n",
      "120 234 Train Loss: 0.301 | Train Acc: 90.121% (6979/7744)\n",
      "150 234 Train Loss: 0.313 | Train Acc: 89.570% (8656/9664)\n",
      "180 234 Train Loss: 0.313 | Train Acc: 89.451% (10362/11584)\n",
      "210 234 Train Loss: 0.314 | Train Acc: 89.307% (12060/13504)\n",
      "234 Epoch: 6 | Train Loss: 0.317 | Train Acc: 89.196% (13358/14976)\n",
      "0 234 Test Loss: 1.409 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.325 | Test Acc: 63.206% (1254/1984)\n",
      "60 234 Test Loss: 1.277 | Test Acc: 64.062% (2501/3904)\n",
      "90 234 Test Loss: 1.278 | Test Acc: 64.183% (3738/5824)\n",
      "120 234 Test Loss: 1.280 | Test Acc: 64.205% (4972/7744)\n",
      "150 234 Test Loss: 1.290 | Test Acc: 64.052% (6190/9664)\n",
      "180 234 Test Loss: 1.288 | Test Acc: 63.907% (7403/11584)\n",
      "210 234 Test Loss: 1.282 | Test Acc: 64.055% (8650/13504)\n",
      "234 Epoch: 6 | Test Loss: 1.277 | Test Acc: 64.196% (9614/14976)\n",
      "\n",
      "Epoch: 7\n",
      "0 234 Train Loss: 0.296 | Train Acc: 90.625% (58/64)\n",
      "30 234 Train Loss: 0.203 | Train Acc: 93.196% (1849/1984)\n",
      "60 234 Train Loss: 0.207 | Train Acc: 93.135% (3636/3904)\n",
      "90 234 Train Loss: 0.216 | Train Acc: 92.548% (5390/5824)\n",
      "120 234 Train Loss: 0.218 | Train Acc: 92.342% (7151/7744)\n",
      "150 234 Train Loss: 0.229 | Train Acc: 91.856% (8877/9664)\n",
      "180 234 Train Loss: 0.244 | Train Acc: 91.462% (10595/11584)\n",
      "210 234 Train Loss: 0.255 | Train Acc: 91.062% (12297/13504)\n",
      "234 Epoch: 7 | Train Loss: 0.260 | Train Acc: 90.852% (13606/14976)\n",
      "0 234 Test Loss: 0.728 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 1.065 | Test Acc: 70.464% (1398/1984)\n",
      "60 234 Test Loss: 1.088 | Test Acc: 70.159% (2739/3904)\n",
      "90 234 Test Loss: 1.086 | Test Acc: 69.866% (4069/5824)\n",
      "120 234 Test Loss: 1.080 | Test Acc: 69.809% (5406/7744)\n",
      "150 234 Test Loss: 1.068 | Test Acc: 69.899% (6755/9664)\n",
      "180 234 Test Loss: 1.073 | Test Acc: 69.613% (8064/11584)\n",
      "210 234 Test Loss: 1.073 | Test Acc: 69.594% (9398/13504)\n",
      "234 Epoch: 7 | Test Loss: 1.079 | Test Acc: 69.478% (10405/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      "0 234 Train Loss: 0.177 | Train Acc: 95.312% (61/64)\n",
      "30 234 Train Loss: 0.162 | Train Acc: 94.607% (1877/1984)\n",
      "60 234 Train Loss: 0.155 | Train Acc: 94.851% (3703/3904)\n",
      "90 234 Train Loss: 0.152 | Train Acc: 94.832% (5523/5824)\n",
      "120 234 Train Loss: 0.158 | Train Acc: 94.654% (7330/7744)\n",
      "150 234 Train Loss: 0.171 | Train Acc: 94.133% (9097/9664)\n",
      "180 234 Train Loss: 0.183 | Train Acc: 93.638% (10847/11584)\n",
      "210 234 Train Loss: 0.187 | Train Acc: 93.535% (12631/13504)\n",
      "234 Epoch: 8 | Train Loss: 0.188 | Train Acc: 93.483% (14000/14976)\n",
      "0 234 Test Loss: 1.547 | Test Acc: 65.625% (42/64)\n",
      "30 234 Test Loss: 1.119 | Test Acc: 70.363% (1396/1984)\n",
      "60 234 Test Loss: 1.130 | Test Acc: 69.442% (2711/3904)\n",
      "90 234 Test Loss: 1.104 | Test Acc: 69.986% (4076/5824)\n",
      "120 234 Test Loss: 1.108 | Test Acc: 70.093% (5428/7744)\n",
      "150 234 Test Loss: 1.120 | Test Acc: 69.754% (6741/9664)\n",
      "180 234 Test Loss: 1.101 | Test Acc: 70.131% (8124/11584)\n",
      "210 234 Test Loss: 1.094 | Test Acc: 70.320% (9496/13504)\n",
      "234 Epoch: 8 | Test Loss: 1.089 | Test Acc: 70.393% (10542/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      "0 234 Train Loss: 0.158 | Train Acc: 89.062% (57/64)\n",
      "30 234 Train Loss: 0.117 | Train Acc: 96.220% (1909/1984)\n",
      "60 234 Train Loss: 0.097 | Train Acc: 96.849% (3781/3904)\n",
      "90 234 Train Loss: 0.094 | Train Acc: 96.909% (5644/5824)\n",
      "120 234 Train Loss: 0.096 | Train Acc: 96.823% (7498/7744)\n",
      "150 234 Train Loss: 0.105 | Train Acc: 96.461% (9322/9664)\n",
      "180 234 Train Loss: 0.110 | Train Acc: 96.323% (11158/11584)\n",
      "210 234 Train Loss: 0.118 | Train Acc: 95.986% (12962/13504)\n",
      "234 Epoch: 9 | Train Loss: 0.124 | Train Acc: 95.706% (14333/14976)\n",
      "0 234 Test Loss: 1.056 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.234 | Test Acc: 70.312% (1395/1984)\n",
      "60 234 Test Loss: 1.235 | Test Acc: 70.415% (2749/3904)\n",
      "90 234 Test Loss: 1.223 | Test Acc: 70.484% (4105/5824)\n",
      "120 234 Test Loss: 1.218 | Test Acc: 70.558% (5464/7744)\n",
      "150 234 Test Loss: 1.219 | Test Acc: 70.447% (6808/9664)\n",
      "180 234 Test Loss: 1.220 | Test Acc: 70.399% (8155/11584)\n",
      "210 234 Test Loss: 1.234 | Test Acc: 70.120% (9469/13504)\n",
      "234 Epoch: 9 | Test Loss: 1.218 | Test Acc: 70.373% (10539/14976)\n",
      "\n",
      "Epoch: 10\n",
      "0 234 Train Loss: 0.071 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.105 | Train Acc: 96.623% (1917/1984)\n",
      "60 234 Train Loss: 0.104 | Train Acc: 96.414% (3764/3904)\n",
      "90 234 Train Loss: 0.114 | Train Acc: 96.051% (5594/5824)\n",
      "120 234 Train Loss: 0.115 | Train Acc: 96.049% (7438/7744)\n",
      "150 234 Train Loss: 0.112 | Train Acc: 96.161% (9293/9664)\n",
      "180 234 Train Loss: 0.111 | Train Acc: 96.141% (11137/11584)\n",
      "210 234 Train Loss: 0.113 | Train Acc: 95.942% (12956/13504)\n",
      "234 Epoch: 10 | Train Loss: 0.115 | Train Acc: 95.873% (14358/14976)\n",
      "0 234 Test Loss: 1.262 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.319 | Test Acc: 68.750% (1364/1984)\n",
      "60 234 Test Loss: 1.354 | Test Acc: 68.084% (2658/3904)\n",
      "90 234 Test Loss: 1.350 | Test Acc: 68.458% (3987/5824)\n",
      "120 234 Test Loss: 1.339 | Test Acc: 68.518% (5306/7744)\n",
      "150 234 Test Loss: 1.344 | Test Acc: 68.533% (6623/9664)\n",
      "180 234 Test Loss: 1.350 | Test Acc: 68.267% (7908/11584)\n",
      "210 234 Test Loss: 1.343 | Test Acc: 68.469% (9246/13504)\n",
      "234 Epoch: 10 | Test Loss: 1.346 | Test Acc: 68.563% (10268/14976)\n",
      "\n",
      "Epoch: 11\n",
      "0 234 Train Loss: 0.155 | Train Acc: 95.312% (61/64)\n",
      "30 234 Train Loss: 0.116 | Train Acc: 96.018% (1905/1984)\n",
      "60 234 Train Loss: 0.103 | Train Acc: 96.516% (3768/3904)\n",
      "90 234 Train Loss: 0.095 | Train Acc: 96.841% (5640/5824)\n",
      "120 234 Train Loss: 0.087 | Train Acc: 97.107% (7520/7744)\n",
      "150 234 Train Loss: 0.082 | Train Acc: 97.330% (9406/9664)\n",
      "180 234 Train Loss: 0.083 | Train Acc: 97.307% (11272/11584)\n",
      "210 234 Train Loss: 0.085 | Train Acc: 97.201% (13126/13504)\n",
      "234 Epoch: 11 | Train Loss: 0.090 | Train Acc: 96.989% (14525/14976)\n",
      "0 234 Test Loss: 1.269 | Test Acc: 62.500% (40/64)\n",
      "30 234 Test Loss: 1.282 | Test Acc: 67.994% (1349/1984)\n",
      "60 234 Test Loss: 1.237 | Test Acc: 69.288% (2705/3904)\n",
      "90 234 Test Loss: 1.221 | Test Acc: 69.797% (4065/5824)\n",
      "120 234 Test Loss: 1.222 | Test Acc: 69.861% (5410/7744)\n",
      "150 234 Test Loss: 1.260 | Test Acc: 69.557% (6722/9664)\n",
      "180 234 Test Loss: 1.277 | Test Acc: 69.449% (8045/11584)\n",
      "210 234 Test Loss: 1.279 | Test Acc: 69.491% (9384/13504)\n",
      "234 Epoch: 11 | Test Loss: 1.283 | Test Acc: 69.491% (10407/14976)\n",
      "\n",
      "Epoch: 12\n",
      "0 234 Train Loss: 0.138 | Train Acc: 95.312% (61/64)\n",
      "30 234 Train Loss: 0.077 | Train Acc: 97.127% (1927/1984)\n",
      "60 234 Train Loss: 0.074 | Train Acc: 97.080% (3790/3904)\n",
      "90 234 Train Loss: 0.080 | Train Acc: 97.098% (5655/5824)\n",
      "120 234 Train Loss: 0.077 | Train Acc: 97.237% (7530/7744)\n",
      "150 234 Train Loss: 0.074 | Train Acc: 97.382% (9411/9664)\n",
      "180 234 Train Loss: 0.075 | Train Acc: 97.402% (11283/11584)\n",
      "210 234 Train Loss: 0.075 | Train Acc: 97.416% (13155/13504)\n",
      "234 Epoch: 12 | Train Loss: 0.075 | Train Acc: 97.396% (14586/14976)\n",
      "0 234 Test Loss: 1.369 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.279 | Test Acc: 71.522% (1419/1984)\n",
      "60 234 Test Loss: 1.225 | Test Acc: 72.182% (2818/3904)\n",
      "90 234 Test Loss: 1.210 | Test Acc: 72.373% (4215/5824)\n",
      "120 234 Test Loss: 1.222 | Test Acc: 71.991% (5575/7744)\n",
      "150 234 Test Loss: 1.232 | Test Acc: 71.678% (6927/9664)\n",
      "180 234 Test Loss: 1.228 | Test Acc: 71.720% (8308/11584)\n",
      "210 234 Test Loss: 1.229 | Test Acc: 71.690% (9681/13504)\n",
      "234 Epoch: 12 | Test Loss: 1.225 | Test Acc: 71.815% (10755/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 13\n",
      "0 234 Train Loss: 0.018 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.062 | Train Acc: 97.984% (1944/1984)\n",
      "60 234 Train Loss: 0.059 | Train Acc: 98.002% (3826/3904)\n",
      "90 234 Train Loss: 0.061 | Train Acc: 97.974% (5706/5824)\n",
      "120 234 Train Loss: 0.063 | Train Acc: 97.869% (7579/7744)\n",
      "150 234 Train Loss: 0.062 | Train Acc: 97.910% (9462/9664)\n",
      "180 234 Train Loss: 0.061 | Train Acc: 97.954% (11347/11584)\n",
      "210 234 Train Loss: 0.060 | Train Acc: 98.023% (13237/13504)\n",
      "234 Epoch: 13 | Train Loss: 0.060 | Train Acc: 98.024% (14680/14976)\n",
      "0 234 Test Loss: 1.292 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.249 | Test Acc: 71.774% (1424/1984)\n",
      "60 234 Test Loss: 1.220 | Test Acc: 72.643% (2836/3904)\n",
      "90 234 Test Loss: 1.206 | Test Acc: 72.768% (4238/5824)\n",
      "120 234 Test Loss: 1.210 | Test Acc: 72.534% (5617/7744)\n",
      "150 234 Test Loss: 1.195 | Test Acc: 72.661% (7022/9664)\n",
      "180 234 Test Loss: 1.191 | Test Acc: 72.652% (8416/11584)\n",
      "210 234 Test Loss: 1.176 | Test Acc: 72.741% (9823/13504)\n",
      "234 Epoch: 13 | Test Loss: 1.171 | Test Acc: 72.776% (10899/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 14\n",
      "0 234 Train Loss: 0.022 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.050 | Train Acc: 98.236% (1949/1984)\n",
      "60 234 Train Loss: 0.058 | Train Acc: 97.925% (3823/3904)\n",
      "90 234 Train Loss: 0.051 | Train Acc: 98.249% (5722/5824)\n",
      "120 234 Train Loss: 0.048 | Train Acc: 98.360% (7617/7744)\n",
      "150 234 Train Loss: 0.048 | Train Acc: 98.386% (9508/9664)\n",
      "180 234 Train Loss: 0.049 | Train Acc: 98.325% (11390/11584)\n",
      "210 234 Train Loss: 0.053 | Train Acc: 98.282% (13272/13504)\n",
      "234 Epoch: 14 | Train Loss: 0.055 | Train Acc: 98.137% (14697/14976)\n",
      "0 234 Test Loss: 1.208 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 1.189 | Test Acc: 72.329% (1435/1984)\n",
      "60 234 Test Loss: 1.152 | Test Acc: 72.413% (2827/3904)\n",
      "90 234 Test Loss: 1.162 | Test Acc: 72.527% (4224/5824)\n",
      "120 234 Test Loss: 1.194 | Test Acc: 71.798% (5560/7744)\n",
      "150 234 Test Loss: 1.193 | Test Acc: 72.103% (6968/9664)\n",
      "180 234 Test Loss: 1.201 | Test Acc: 72.022% (8343/11584)\n",
      "210 234 Test Loss: 1.206 | Test Acc: 71.823% (9699/13504)\n",
      "234 Epoch: 14 | Test Loss: 1.210 | Test Acc: 71.855% (10761/14976)\n",
      "\n",
      "Epoch: 15\n",
      "0 234 Train Loss: 0.015 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.047 | Train Acc: 98.488% (1954/1984)\n",
      "60 234 Train Loss: 0.047 | Train Acc: 98.514% (3846/3904)\n",
      "90 234 Train Loss: 0.044 | Train Acc: 98.541% (5739/5824)\n",
      "120 234 Train Loss: 0.046 | Train Acc: 98.554% (7632/7744)\n",
      "150 234 Train Loss: 0.046 | Train Acc: 98.510% (9520/9664)\n",
      "180 234 Train Loss: 0.046 | Train Acc: 98.489% (11409/11584)\n",
      "210 234 Train Loss: 0.046 | Train Acc: 98.475% (13298/13504)\n",
      "234 Epoch: 15 | Train Loss: 0.047 | Train Acc: 98.444% (14743/14976)\n",
      "0 234 Test Loss: 1.834 | Test Acc: 64.062% (41/64)\n",
      "30 234 Test Loss: 1.401 | Test Acc: 70.060% (1390/1984)\n",
      "60 234 Test Loss: 1.333 | Test Acc: 71.388% (2787/3904)\n",
      "90 234 Test Loss: 1.312 | Test Acc: 71.789% (4181/5824)\n",
      "120 234 Test Loss: 1.289 | Test Acc: 72.043% (5579/7744)\n",
      "150 234 Test Loss: 1.294 | Test Acc: 71.989% (6957/9664)\n",
      "180 234 Test Loss: 1.283 | Test Acc: 71.987% (8339/11584)\n",
      "210 234 Test Loss: 1.280 | Test Acc: 71.971% (9719/13504)\n",
      "234 Epoch: 15 | Test Loss: 1.275 | Test Acc: 72.162% (10807/14976)\n",
      "\n",
      "Epoch: 16\n",
      "0 234 Train Loss: 0.009 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.036 | Train Acc: 98.538% (1955/1984)\n",
      "60 234 Train Loss: 0.034 | Train Acc: 98.642% (3851/3904)\n",
      "90 234 Train Loss: 0.037 | Train Acc: 98.609% (5743/5824)\n",
      "120 234 Train Loss: 0.040 | Train Acc: 98.502% (7628/7744)\n",
      "150 234 Train Loss: 0.040 | Train Acc: 98.510% (9520/9664)\n",
      "180 234 Train Loss: 0.038 | Train Acc: 98.610% (11423/11584)\n",
      "210 234 Train Loss: 0.037 | Train Acc: 98.615% (13317/13504)\n",
      "234 Epoch: 16 | Train Loss: 0.038 | Train Acc: 98.591% (14765/14976)\n",
      "0 234 Test Loss: 1.602 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.430 | Test Acc: 69.204% (1373/1984)\n",
      "60 234 Test Loss: 1.488 | Test Acc: 68.776% (2685/3904)\n",
      "90 234 Test Loss: 1.490 | Test Acc: 68.905% (4013/5824)\n",
      "120 234 Test Loss: 1.493 | Test Acc: 68.608% (5313/7744)\n",
      "150 234 Test Loss: 1.493 | Test Acc: 68.615% (6631/9664)\n",
      "180 234 Test Loss: 1.467 | Test Acc: 68.931% (7985/11584)\n",
      "210 234 Test Loss: 1.485 | Test Acc: 68.728% (9281/13504)\n",
      "234 Epoch: 16 | Test Loss: 1.485 | Test Acc: 68.743% (10295/14976)\n",
      "\n",
      "Epoch: 17\n",
      "0 234 Train Loss: 0.015 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.025 | Train Acc: 99.194% (1968/1984)\n",
      "60 234 Train Loss: 0.025 | Train Acc: 99.232% (3874/3904)\n",
      "90 234 Train Loss: 0.022 | Train Acc: 99.365% (5787/5824)\n",
      "120 234 Train Loss: 0.022 | Train Acc: 99.329% (7692/7744)\n",
      "150 234 Train Loss: 0.022 | Train Acc: 99.307% (9597/9664)\n",
      "180 234 Train Loss: 0.024 | Train Acc: 99.275% (11500/11584)\n",
      "210 234 Train Loss: 0.024 | Train Acc: 99.252% (13403/13504)\n",
      "234 Epoch: 17 | Train Loss: 0.025 | Train Acc: 99.205% (14857/14976)\n",
      "0 234 Test Loss: 1.355 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 1.331 | Test Acc: 72.026% (1429/1984)\n",
      "60 234 Test Loss: 1.346 | Test Acc: 71.773% (2802/3904)\n",
      "90 234 Test Loss: 1.358 | Test Acc: 71.411% (4159/5824)\n",
      "120 234 Test Loss: 1.372 | Test Acc: 71.320% (5523/7744)\n",
      "150 234 Test Loss: 1.360 | Test Acc: 71.534% (6913/9664)\n",
      "180 234 Test Loss: 1.355 | Test Acc: 71.417% (8273/11584)\n",
      "210 234 Test Loss: 1.363 | Test Acc: 71.445% (9648/13504)\n",
      "234 Epoch: 17 | Test Loss: 1.369 | Test Acc: 71.274% (10674/14976)\n",
      "\n",
      "Epoch: 18\n",
      "0 234 Train Loss: 0.023 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.033 | Train Acc: 98.891% (1962/1984)\n",
      "60 234 Train Loss: 0.030 | Train Acc: 99.001% (3865/3904)\n",
      "90 234 Train Loss: 0.031 | Train Acc: 98.987% (5765/5824)\n",
      "120 234 Train Loss: 0.030 | Train Acc: 99.006% (7667/7744)\n",
      "150 234 Train Loss: 0.030 | Train Acc: 99.038% (9571/9664)\n",
      "180 234 Train Loss: 0.030 | Train Acc: 99.007% (11469/11584)\n",
      "210 234 Train Loss: 0.031 | Train Acc: 98.993% (13368/13504)\n",
      "234 Epoch: 18 | Train Loss: 0.030 | Train Acc: 99.012% (14828/14976)\n",
      "0 234 Test Loss: 0.881 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 1.154 | Test Acc: 74.899% (1486/1984)\n",
      "60 234 Test Loss: 1.210 | Test Acc: 73.975% (2888/3904)\n",
      "90 234 Test Loss: 1.183 | Test Acc: 74.433% (4335/5824)\n",
      "120 234 Test Loss: 1.174 | Test Acc: 74.509% (5770/7744)\n",
      "150 234 Test Loss: 1.170 | Test Acc: 74.648% (7214/9664)\n",
      "180 234 Test Loss: 1.165 | Test Acc: 74.551% (8636/11584)\n",
      "210 234 Test Loss: 1.165 | Test Acc: 74.615% (10076/13504)\n",
      "234 Epoch: 18 | Test Loss: 1.169 | Test Acc: 74.472% (11153/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 19\n",
      "0 234 Train Loss: 0.022 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.014 | Train Acc: 99.597% (1976/1984)\n",
      "60 234 Train Loss: 0.012 | Train Acc: 99.693% (3892/3904)\n",
      "90 234 Train Loss: 0.015 | Train Acc: 99.536% (5797/5824)\n",
      "120 234 Train Loss: 0.017 | Train Acc: 99.458% (7702/7744)\n",
      "150 234 Train Loss: 0.020 | Train Acc: 99.358% (9602/9664)\n",
      "180 234 Train Loss: 0.021 | Train Acc: 99.309% (11504/11584)\n",
      "210 234 Train Loss: 0.023 | Train Acc: 99.237% (13401/13504)\n",
      "234 Epoch: 19 | Train Loss: 0.024 | Train Acc: 99.225% (14860/14976)\n",
      "0 234 Test Loss: 1.650 | Test Acc: 67.188% (43/64)\n",
      "30 234 Test Loss: 1.266 | Test Acc: 73.286% (1454/1984)\n",
      "60 234 Test Loss: 1.270 | Test Acc: 72.823% (2843/3904)\n",
      "90 234 Test Loss: 1.237 | Test Acc: 73.420% (4276/5824)\n",
      "120 234 Test Loss: 1.246 | Test Acc: 73.179% (5667/7744)\n",
      "150 234 Test Loss: 1.241 | Test Acc: 73.313% (7085/9664)\n",
      "180 234 Test Loss: 1.252 | Test Acc: 73.213% (8481/11584)\n",
      "210 234 Test Loss: 1.242 | Test Acc: 73.371% (9908/13504)\n",
      "234 Epoch: 19 | Test Loss: 1.233 | Test Acc: 73.417% (10995/14976)\n",
      "\n",
      "Epoch: 20\n",
      "0 234 Train Loss: 0.079 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.030 | Train Acc: 99.093% (1966/1984)\n",
      "60 234 Train Loss: 0.029 | Train Acc: 99.001% (3865/3904)\n",
      "90 234 Train Loss: 0.030 | Train Acc: 98.953% (5763/5824)\n",
      "120 234 Train Loss: 0.029 | Train Acc: 99.019% (7668/7744)\n",
      "150 234 Train Loss: 0.029 | Train Acc: 99.027% (9570/9664)\n",
      "180 234 Train Loss: 0.029 | Train Acc: 99.050% (11474/11584)\n",
      "210 234 Train Loss: 0.028 | Train Acc: 99.089% (13381/13504)\n",
      "234 Epoch: 20 | Train Loss: 0.028 | Train Acc: 99.079% (14838/14976)\n",
      "0 234 Test Loss: 1.650 | Test Acc: 62.500% (40/64)\n",
      "30 234 Test Loss: 1.301 | Test Acc: 73.135% (1451/1984)\n",
      "60 234 Test Loss: 1.302 | Test Acc: 73.105% (2854/3904)\n",
      "90 234 Test Loss: 1.263 | Test Acc: 73.918% (4305/5824)\n",
      "120 234 Test Loss: 1.251 | Test Acc: 73.864% (5720/7744)\n",
      "150 234 Test Loss: 1.277 | Test Acc: 73.489% (7102/9664)\n",
      "180 234 Test Loss: 1.295 | Test Acc: 73.127% (8471/11584)\n",
      "210 234 Test Loss: 1.304 | Test Acc: 73.008% (9859/13504)\n",
      "234 Epoch: 20 | Test Loss: 1.303 | Test Acc: 73.003% (10933/14976)\n",
      "\n",
      "Epoch: 21\n",
      "0 234 Train Loss: 0.037 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.026 | Train Acc: 98.992% (1964/1984)\n",
      "60 234 Train Loss: 0.024 | Train Acc: 99.257% (3875/3904)\n",
      "90 234 Train Loss: 0.025 | Train Acc: 99.227% (5779/5824)\n",
      "120 234 Train Loss: 0.026 | Train Acc: 99.174% (7680/7744)\n",
      "150 234 Train Loss: 0.028 | Train Acc: 99.100% (9577/9664)\n",
      "180 234 Train Loss: 0.029 | Train Acc: 99.050% (11474/11584)\n",
      "210 234 Train Loss: 0.031 | Train Acc: 98.919% (13358/13504)\n",
      "234 Epoch: 21 | Train Loss: 0.032 | Train Acc: 98.918% (14814/14976)\n",
      "0 234 Test Loss: 0.663 | Test Acc: 85.938% (55/64)\n",
      "30 234 Test Loss: 1.267 | Test Acc: 73.790% (1464/1984)\n",
      "60 234 Test Loss: 1.276 | Test Acc: 73.309% (2862/3904)\n",
      "90 234 Test Loss: 1.280 | Test Acc: 73.180% (4262/5824)\n",
      "120 234 Test Loss: 1.286 | Test Acc: 73.011% (5654/7744)\n",
      "150 234 Test Loss: 1.283 | Test Acc: 72.848% (7040/9664)\n",
      "180 234 Test Loss: 1.260 | Test Acc: 73.040% (8461/11584)\n",
      "210 234 Test Loss: 1.254 | Test Acc: 73.082% (9869/13504)\n",
      "234 Epoch: 21 | Test Loss: 1.265 | Test Acc: 72.890% (10916/14976)\n",
      "\n",
      "Epoch: 22\n",
      "0 234 Train Loss: 0.029 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.025 | Train Acc: 98.992% (1964/1984)\n",
      "60 234 Train Loss: 0.024 | Train Acc: 99.103% (3869/3904)\n",
      "90 234 Train Loss: 0.026 | Train Acc: 99.056% (5769/5824)\n",
      "120 234 Train Loss: 0.029 | Train Acc: 98.941% (7662/7744)\n",
      "150 234 Train Loss: 0.032 | Train Acc: 98.851% (9553/9664)\n",
      "180 234 Train Loss: 0.032 | Train Acc: 98.886% (11455/11584)\n",
      "210 234 Train Loss: 0.032 | Train Acc: 98.897% (13355/13504)\n",
      "234 Epoch: 22 | Train Loss: 0.031 | Train Acc: 98.925% (14815/14976)\n",
      "0 234 Test Loss: 1.461 | Test Acc: 71.875% (46/64)\n",
      "30 234 Test Loss: 1.361 | Test Acc: 70.413% (1397/1984)\n",
      "60 234 Test Loss: 1.360 | Test Acc: 70.902% (2768/3904)\n",
      "90 234 Test Loss: 1.323 | Test Acc: 71.223% (4148/5824)\n",
      "120 234 Test Loss: 1.314 | Test Acc: 71.397% (5529/7744)\n",
      "150 234 Test Loss: 1.301 | Test Acc: 71.720% (6931/9664)\n",
      "180 234 Test Loss: 1.292 | Test Acc: 71.979% (8338/11584)\n",
      "210 234 Test Loss: 1.277 | Test Acc: 72.305% (9764/13504)\n",
      "234 Epoch: 22 | Test Loss: 1.284 | Test Acc: 72.202% (10813/14976)\n",
      "\n",
      "Epoch: 23\n",
      "0 234 Train Loss: 0.057 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.026 | Train Acc: 99.093% (1966/1984)\n",
      "60 234 Train Loss: 0.021 | Train Acc: 99.283% (3876/3904)\n",
      "90 234 Train Loss: 0.019 | Train Acc: 99.416% (5790/5824)\n",
      "120 234 Train Loss: 0.019 | Train Acc: 99.380% (7696/7744)\n",
      "150 234 Train Loss: 0.017 | Train Acc: 99.452% (9611/9664)\n",
      "180 234 Train Loss: 0.016 | Train Acc: 99.473% (11523/11584)\n",
      "210 234 Train Loss: 0.016 | Train Acc: 99.496% (13436/13504)\n",
      "234 Epoch: 23 | Train Loss: 0.016 | Train Acc: 99.493% (14900/14976)\n",
      "0 234 Test Loss: 0.984 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 1.281 | Test Acc: 71.169% (1412/1984)\n",
      "60 234 Test Loss: 1.290 | Test Acc: 71.926% (2808/3904)\n",
      "90 234 Test Loss: 1.288 | Test Acc: 71.738% (4178/5824)\n",
      "120 234 Test Loss: 1.288 | Test Acc: 71.978% (5574/7744)\n",
      "150 234 Test Loss: 1.272 | Test Acc: 72.279% (6985/9664)\n",
      "180 234 Test Loss: 1.261 | Test Acc: 72.410% (8388/11584)\n",
      "210 234 Test Loss: 1.260 | Test Acc: 72.393% (9776/13504)\n",
      "234 Epoch: 23 | Test Loss: 1.267 | Test Acc: 72.369% (10838/14976)\n",
      "\n",
      "Epoch: 24\n",
      "0 234 Train Loss: 0.018 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.019 | Train Acc: 99.244% (1969/1984)\n",
      "60 234 Train Loss: 0.013 | Train Acc: 99.488% (3884/3904)\n",
      "90 234 Train Loss: 0.011 | Train Acc: 99.571% (5799/5824)\n",
      "120 234 Train Loss: 0.010 | Train Acc: 99.638% (7716/7744)\n",
      "150 234 Train Loss: 0.009 | Train Acc: 99.700% (9635/9664)\n",
      "180 234 Train Loss: 0.009 | Train Acc: 99.724% (11552/11584)\n",
      "210 234 Train Loss: 0.009 | Train Acc: 99.704% (13464/13504)\n",
      "234 Epoch: 24 | Train Loss: 0.009 | Train Acc: 99.706% (14932/14976)\n",
      "0 234 Test Loss: 0.879 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 1.220 | Test Acc: 74.798% (1484/1984)\n",
      "60 234 Test Loss: 1.280 | Test Acc: 73.745% (2879/3904)\n",
      "90 234 Test Loss: 1.257 | Test Acc: 73.626% (4288/5824)\n",
      "120 234 Test Loss: 1.243 | Test Acc: 73.993% (5730/7744)\n",
      "150 234 Test Loss: 1.247 | Test Acc: 73.996% (7151/9664)\n",
      "180 234 Test Loss: 1.249 | Test Acc: 74.042% (8577/11584)\n",
      "210 234 Test Loss: 1.252 | Test Acc: 73.934% (9984/13504)\n",
      "234 Epoch: 24 | Test Loss: 1.241 | Test Acc: 74.185% (11110/14976)\n",
      "\n",
      "Epoch: 25\n",
      "0 234 Train Loss: 0.014 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.010 | Train Acc: 99.748% (1979/1984)\n",
      "60 234 Train Loss: 0.008 | Train Acc: 99.795% (3896/3904)\n",
      "90 234 Train Loss: 0.009 | Train Acc: 99.811% (5813/5824)\n",
      "120 234 Train Loss: 0.008 | Train Acc: 99.819% (7730/7744)\n",
      "150 234 Train Loss: 0.008 | Train Acc: 99.824% (9647/9664)\n",
      "180 234 Train Loss: 0.007 | Train Acc: 99.827% (11564/11584)\n",
      "210 234 Train Loss: 0.007 | Train Acc: 99.822% (13480/13504)\n",
      "234 Epoch: 25 | Train Loss: 0.007 | Train Acc: 99.813% (14948/14976)\n",
      "0 234 Test Loss: 1.116 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 1.123 | Test Acc: 75.403% (1496/1984)\n",
      "60 234 Test Loss: 1.154 | Test Acc: 74.257% (2899/3904)\n",
      "90 234 Test Loss: 1.127 | Test Acc: 75.103% (4374/5824)\n",
      "120 234 Test Loss: 1.106 | Test Acc: 75.207% (5824/7744)\n",
      "150 234 Test Loss: 1.103 | Test Acc: 75.362% (7283/9664)\n",
      "180 234 Test Loss: 1.112 | Test Acc: 75.380% (8732/11584)\n",
      "210 234 Test Loss: 1.114 | Test Acc: 75.407% (10183/13504)\n",
      "234 Epoch: 25 | Test Loss: 1.112 | Test Acc: 75.528% (11311/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 26\n",
      "0 234 Train Loss: 0.023 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.006 | Train Acc: 99.849% (1981/1984)\n",
      "60 234 Train Loss: 0.004 | Train Acc: 99.898% (3900/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 99.931% (5820/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 99.923% (7738/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 99.928% (9657/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 99.940% (11577/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 99.948% (13497/13504)\n",
      "234 Epoch: 26 | Train Loss: 0.002 | Train Acc: 99.953% (14969/14976)\n",
      "0 234 Test Loss: 0.991 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 1.069 | Test Acc: 77.167% (1531/1984)\n",
      "60 234 Test Loss: 0.997 | Test Acc: 77.997% (3045/3904)\n",
      "90 234 Test Loss: 1.002 | Test Acc: 77.507% (4514/5824)\n",
      "120 234 Test Loss: 1.016 | Test Acc: 77.299% (5986/7744)\n",
      "150 234 Test Loss: 1.042 | Test Acc: 77.018% (7443/9664)\n",
      "180 234 Test Loss: 1.046 | Test Acc: 77.020% (8922/11584)\n",
      "210 234 Test Loss: 1.043 | Test Acc: 77.066% (10407/13504)\n",
      "234 Epoch: 26 | Test Loss: 1.035 | Test Acc: 77.150% (11554/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 27\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 27 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.974 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.976 | Test Acc: 76.966% (1527/1984)\n",
      "60 234 Test Loss: 0.973 | Test Acc: 77.228% (3015/3904)\n",
      "90 234 Test Loss: 1.006 | Test Acc: 76.940% (4481/5824)\n",
      "120 234 Test Loss: 1.004 | Test Acc: 77.040% (5966/7744)\n",
      "150 234 Test Loss: 1.001 | Test Acc: 77.173% (7458/9664)\n",
      "180 234 Test Loss: 1.002 | Test Acc: 77.193% (8942/11584)\n",
      "210 234 Test Loss: 1.006 | Test Acc: 77.170% (10421/13504)\n",
      "234 Epoch: 27 | Test Loss: 1.005 | Test Acc: 77.077% (11543/14976)\n",
      "\n",
      "Epoch: 28\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 28 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.745 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.971 | Test Acc: 77.772% (1543/1984)\n",
      "60 234 Test Loss: 0.996 | Test Acc: 76.742% (2996/3904)\n",
      "90 234 Test Loss: 1.004 | Test Acc: 76.906% (4479/5824)\n",
      "120 234 Test Loss: 0.979 | Test Acc: 77.376% (5992/7744)\n",
      "150 234 Test Loss: 0.990 | Test Acc: 77.038% (7445/9664)\n",
      "180 234 Test Loss: 0.985 | Test Acc: 77.124% (8934/11584)\n",
      "210 234 Test Loss: 0.983 | Test Acc: 77.177% (10422/13504)\n",
      "234 Epoch: 28 | Test Loss: 0.976 | Test Acc: 77.397% (11591/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 29\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 29 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 1.753 | Test Acc: 64.062% (41/64)\n",
      "30 234 Test Loss: 1.022 | Test Acc: 76.260% (1513/1984)\n",
      "60 234 Test Loss: 0.991 | Test Acc: 77.536% (3027/3904)\n",
      "90 234 Test Loss: 0.971 | Test Acc: 77.764% (4529/5824)\n",
      "120 234 Test Loss: 0.961 | Test Acc: 77.712% (6018/7744)\n",
      "150 234 Test Loss: 0.961 | Test Acc: 77.577% (7497/9664)\n",
      "180 234 Test Loss: 0.953 | Test Acc: 77.555% (8984/11584)\n",
      "210 234 Test Loss: 0.948 | Test Acc: 77.599% (10479/13504)\n",
      "234 Epoch: 29 | Test Loss: 0.949 | Test Acc: 77.524% (11610/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 30\n",
      "0 234 Train Loss: 0.000 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 30 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 1.129 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 0.926 | Test Acc: 77.470% (1537/1984)\n",
      "60 234 Test Loss: 0.927 | Test Acc: 77.485% (3025/3904)\n",
      "90 234 Test Loss: 0.915 | Test Acc: 77.576% (4518/5824)\n",
      "120 234 Test Loss: 0.929 | Test Acc: 77.376% (5992/7744)\n",
      "150 234 Test Loss: 0.933 | Test Acc: 77.628% (7502/9664)\n",
      "180 234 Test Loss: 0.940 | Test Acc: 77.573% (8986/11584)\n",
      "210 234 Test Loss: 0.940 | Test Acc: 77.473% (10462/13504)\n",
      "234 Epoch: 30 | Test Loss: 0.934 | Test Acc: 77.504% (11607/14976)\n",
      "\n",
      "Epoch: 31\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 31 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.723 | Test Acc: 85.938% (55/64)\n",
      "30 234 Test Loss: 0.866 | Test Acc: 77.923% (1546/1984)\n",
      "60 234 Test Loss: 0.867 | Test Acc: 78.484% (3064/3904)\n",
      "90 234 Test Loss: 0.909 | Test Acc: 77.387% (4507/5824)\n",
      "120 234 Test Loss: 0.918 | Test Acc: 77.311% (5987/7744)\n",
      "150 234 Test Loss: 0.919 | Test Acc: 77.266% (7467/9664)\n",
      "180 234 Test Loss: 0.921 | Test Acc: 77.288% (8953/11584)\n",
      "210 234 Test Loss: 0.909 | Test Acc: 77.695% (10492/13504)\n",
      "234 Epoch: 31 | Test Loss: 0.919 | Test Acc: 77.437% (11597/14976)\n",
      "\n",
      "Epoch: 32\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 32 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.997 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 0.926 | Test Acc: 77.016% (1528/1984)\n",
      "60 234 Test Loss: 0.898 | Test Acc: 77.433% (3023/3904)\n",
      "90 234 Test Loss: 0.892 | Test Acc: 77.764% (4529/5824)\n",
      "120 234 Test Loss: 0.891 | Test Acc: 77.867% (6030/7744)\n",
      "150 234 Test Loss: 0.897 | Test Acc: 77.659% (7505/9664)\n",
      "180 234 Test Loss: 0.900 | Test Acc: 77.581% (8987/11584)\n",
      "210 234 Test Loss: 0.899 | Test Acc: 77.570% (10475/13504)\n",
      "234 Epoch: 32 | Test Loss: 0.902 | Test Acc: 77.517% (11609/14976)\n",
      "\n",
      "Epoch: 33\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 33 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.873 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 0.909 | Test Acc: 77.873% (1545/1984)\n",
      "60 234 Test Loss: 0.903 | Test Acc: 77.638% (3031/3904)\n",
      "90 234 Test Loss: 0.891 | Test Acc: 77.627% (4521/5824)\n",
      "120 234 Test Loss: 0.904 | Test Acc: 77.350% (5990/7744)\n",
      "150 234 Test Loss: 0.909 | Test Acc: 77.163% (7457/9664)\n",
      "180 234 Test Loss: 0.909 | Test Acc: 77.089% (8930/11584)\n",
      "210 234 Test Loss: 0.887 | Test Acc: 77.562% (10474/13504)\n",
      "234 Epoch: 33 | Test Loss: 0.890 | Test Acc: 77.471% (11602/14976)\n",
      "\n",
      "Epoch: 34\n",
      "0 234 Train Loss: 0.000 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 34 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 1.025 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 0.880 | Test Acc: 77.470% (1537/1984)\n",
      "60 234 Test Loss: 0.870 | Test Acc: 77.433% (3023/3904)\n",
      "90 234 Test Loss: 0.834 | Test Acc: 78.331% (4562/5824)\n",
      "120 234 Test Loss: 0.857 | Test Acc: 78.009% (6041/7744)\n",
      "150 234 Test Loss: 0.853 | Test Acc: 78.032% (7541/9664)\n",
      "180 234 Test Loss: 0.854 | Test Acc: 78.013% (9037/11584)\n",
      "210 234 Test Loss: 0.860 | Test Acc: 77.777% (10503/13504)\n",
      "234 Epoch: 34 | Test Loss: 0.869 | Test Acc: 77.651% (11629/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 35\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 35 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 1.297 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 0.858 | Test Acc: 77.268% (1533/1984)\n",
      "60 234 Test Loss: 0.886 | Test Acc: 77.126% (3011/3904)\n",
      "90 234 Test Loss: 0.886 | Test Acc: 77.078% (4489/5824)\n",
      "120 234 Test Loss: 0.866 | Test Acc: 77.454% (5998/7744)\n",
      "150 234 Test Loss: 0.866 | Test Acc: 77.328% (7473/9664)\n",
      "180 234 Test Loss: 0.862 | Test Acc: 77.676% (8998/11584)\n",
      "210 234 Test Loss: 0.861 | Test Acc: 77.755% (10500/13504)\n",
      "234 Epoch: 35 | Test Loss: 0.866 | Test Acc: 77.618% (11624/14976)\n",
      "\n",
      "Epoch: 36\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 36 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.839 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 0.851 | Test Acc: 78.024% (1548/1984)\n",
      "60 234 Test Loss: 0.863 | Test Acc: 77.561% (3028/3904)\n",
      "90 234 Test Loss: 0.866 | Test Acc: 77.438% (4510/5824)\n",
      "120 234 Test Loss: 0.858 | Test Acc: 77.673% (6015/7744)\n",
      "150 234 Test Loss: 0.862 | Test Acc: 77.546% (7494/9664)\n",
      "180 234 Test Loss: 0.856 | Test Acc: 77.754% (9007/11584)\n",
      "210 234 Test Loss: 0.857 | Test Acc: 77.651% (10486/13504)\n",
      "234 Epoch: 36 | Test Loss: 0.854 | Test Acc: 77.718% (11639/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 37\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 37 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.578 | Test Acc: 85.938% (55/64)\n",
      "30 234 Test Loss: 0.839 | Test Acc: 78.528% (1558/1984)\n",
      "60 234 Test Loss: 0.866 | Test Acc: 78.074% (3048/3904)\n",
      "90 234 Test Loss: 0.874 | Test Acc: 77.627% (4521/5824)\n",
      "120 234 Test Loss: 0.865 | Test Acc: 77.699% (6017/7744)\n",
      "150 234 Test Loss: 0.862 | Test Acc: 77.970% (7535/9664)\n",
      "180 234 Test Loss: 0.857 | Test Acc: 78.065% (9043/11584)\n",
      "210 234 Test Loss: 0.852 | Test Acc: 77.932% (10524/13504)\n",
      "234 Epoch: 37 | Test Loss: 0.848 | Test Acc: 77.958% (11675/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 38\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 38 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.707 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.842 | Test Acc: 77.369% (1535/1984)\n",
      "60 234 Test Loss: 0.849 | Test Acc: 77.280% (3017/3904)\n",
      "90 234 Test Loss: 0.854 | Test Acc: 77.284% (4501/5824)\n",
      "120 234 Test Loss: 0.824 | Test Acc: 77.789% (6024/7744)\n",
      "150 234 Test Loss: 0.830 | Test Acc: 77.525% (7492/9664)\n",
      "180 234 Test Loss: 0.830 | Test Acc: 77.573% (8986/11584)\n",
      "210 234 Test Loss: 0.829 | Test Acc: 77.703% (10493/13504)\n",
      "234 Epoch: 38 | Test Loss: 0.834 | Test Acc: 77.671% (11632/14976)\n",
      "\n",
      "Epoch: 39\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 39 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.879 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 0.838 | Test Acc: 77.319% (1534/1984)\n",
      "60 234 Test Loss: 0.873 | Test Acc: 76.691% (2994/3904)\n",
      "90 234 Test Loss: 0.871 | Test Acc: 77.112% (4491/5824)\n",
      "120 234 Test Loss: 0.854 | Test Acc: 77.350% (5990/7744)\n",
      "150 234 Test Loss: 0.855 | Test Acc: 77.308% (7471/9664)\n",
      "180 234 Test Loss: 0.848 | Test Acc: 77.219% (8945/11584)\n",
      "210 234 Test Loss: 0.839 | Test Acc: 77.496% (10465/13504)\n",
      "234 Epoch: 39 | Test Loss: 0.837 | Test Acc: 77.744% (11643/14976)\n",
      "\n",
      "Epoch: 40\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 40 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 1.056 | Test Acc: 71.875% (46/64)\n",
      "30 234 Test Loss: 0.831 | Test Acc: 77.722% (1542/1984)\n",
      "60 234 Test Loss: 0.832 | Test Acc: 77.228% (3015/3904)\n",
      "90 234 Test Loss: 0.816 | Test Acc: 77.593% (4519/5824)\n",
      "120 234 Test Loss: 0.818 | Test Acc: 77.583% (6008/7744)\n",
      "150 234 Test Loss: 0.820 | Test Acc: 77.721% (7511/9664)\n",
      "180 234 Test Loss: 0.823 | Test Acc: 77.762% (9008/11584)\n",
      "210 234 Test Loss: 0.820 | Test Acc: 77.799% (10506/13504)\n",
      "234 Epoch: 40 | Test Loss: 0.819 | Test Acc: 77.945% (11673/14976)\n",
      "\n",
      "Epoch: 41\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 41 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.692 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 0.788 | Test Acc: 78.377% (1555/1984)\n",
      "60 234 Test Loss: 0.819 | Test Acc: 77.228% (3015/3904)\n",
      "90 234 Test Loss: 0.812 | Test Acc: 77.730% (4527/5824)\n",
      "120 234 Test Loss: 0.813 | Test Acc: 77.621% (6011/7744)\n",
      "150 234 Test Loss: 0.824 | Test Acc: 77.514% (7491/9664)\n",
      "180 234 Test Loss: 0.816 | Test Acc: 77.806% (9013/11584)\n",
      "210 234 Test Loss: 0.816 | Test Acc: 77.903% (10520/13504)\n",
      "234 Epoch: 41 | Test Loss: 0.817 | Test Acc: 77.898% (11666/14976)\n",
      "\n",
      "Epoch: 42\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 42 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 1.118 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 0.869 | Test Acc: 76.512% (1518/1984)\n",
      "60 234 Test Loss: 0.848 | Test Acc: 77.382% (3021/3904)\n",
      "90 234 Test Loss: 0.824 | Test Acc: 77.850% (4534/5824)\n",
      "120 234 Test Loss: 0.826 | Test Acc: 77.854% (6029/7744)\n",
      "150 234 Test Loss: 0.821 | Test Acc: 77.866% (7525/9664)\n",
      "180 234 Test Loss: 0.816 | Test Acc: 77.866% (9020/11584)\n",
      "210 234 Test Loss: 0.810 | Test Acc: 77.903% (10520/13504)\n",
      "234 Epoch: 42 | Test Loss: 0.815 | Test Acc: 77.878% (11663/14976)\n",
      "\n",
      "Epoch: 43\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 43 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.593 | Test Acc: 85.938% (55/64)\n",
      "30 234 Test Loss: 0.733 | Test Acc: 79.889% (1585/1984)\n",
      "60 234 Test Loss: 0.775 | Test Acc: 79.073% (3087/3904)\n",
      "90 234 Test Loss: 0.801 | Test Acc: 78.228% (4556/5824)\n",
      "120 234 Test Loss: 0.801 | Test Acc: 77.970% (6038/7744)\n",
      "150 234 Test Loss: 0.798 | Test Acc: 78.073% (7545/9664)\n",
      "180 234 Test Loss: 0.803 | Test Acc: 77.987% (9034/11584)\n",
      "210 234 Test Loss: 0.800 | Test Acc: 78.007% (10534/13504)\n",
      "234 Epoch: 43 | Test Loss: 0.800 | Test Acc: 78.052% (11689/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 44\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 44 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.953 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 0.839 | Test Acc: 77.923% (1546/1984)\n",
      "60 234 Test Loss: 0.772 | Test Acc: 78.893% (3080/3904)\n",
      "90 234 Test Loss: 0.771 | Test Acc: 78.795% (4589/5824)\n",
      "120 234 Test Loss: 0.767 | Test Acc: 78.784% (6101/7744)\n",
      "150 234 Test Loss: 0.788 | Test Acc: 78.322% (7569/9664)\n",
      "180 234 Test Loss: 0.793 | Test Acc: 78.125% (9050/11584)\n",
      "210 234 Test Loss: 0.805 | Test Acc: 77.821% (10509/13504)\n",
      "234 Epoch: 44 | Test Loss: 0.802 | Test Acc: 77.851% (11659/14976)\n",
      "\n",
      "Epoch: 45\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 45 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.795 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.804 | Test Acc: 77.571% (1539/1984)\n",
      "60 234 Test Loss: 0.782 | Test Acc: 78.227% (3054/3904)\n",
      "90 234 Test Loss: 0.782 | Test Acc: 78.348% (4563/5824)\n",
      "120 234 Test Loss: 0.797 | Test Acc: 77.854% (6029/7744)\n",
      "150 234 Test Loss: 0.789 | Test Acc: 77.794% (7518/9664)\n",
      "180 234 Test Loss: 0.796 | Test Acc: 77.616% (8991/11584)\n",
      "210 234 Test Loss: 0.803 | Test Acc: 77.562% (10474/13504)\n",
      "234 Epoch: 45 | Test Loss: 0.799 | Test Acc: 77.751% (11644/14976)\n",
      "\n",
      "Epoch: 46\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 46 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 1.105 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 0.765 | Test Acc: 78.276% (1553/1984)\n",
      "60 234 Test Loss: 0.800 | Test Acc: 77.869% (3040/3904)\n",
      "90 234 Test Loss: 0.807 | Test Acc: 77.455% (4511/5824)\n",
      "120 234 Test Loss: 0.816 | Test Acc: 77.492% (6001/7744)\n",
      "150 234 Test Loss: 0.805 | Test Acc: 77.825% (7521/9664)\n",
      "180 234 Test Loss: 0.808 | Test Acc: 77.883% (9022/11584)\n",
      "210 234 Test Loss: 0.799 | Test Acc: 78.007% (10534/13504)\n",
      "234 Epoch: 46 | Test Loss: 0.795 | Test Acc: 78.092% (11695/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 47\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 47 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.913 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 0.820 | Test Acc: 77.520% (1538/1984)\n",
      "60 234 Test Loss: 0.829 | Test Acc: 77.357% (3020/3904)\n",
      "90 234 Test Loss: 0.789 | Test Acc: 78.159% (4552/5824)\n",
      "120 234 Test Loss: 0.793 | Test Acc: 78.022% (6042/7744)\n",
      "150 234 Test Loss: 0.797 | Test Acc: 77.887% (7527/9664)\n",
      "180 234 Test Loss: 0.795 | Test Acc: 77.961% (9031/11584)\n",
      "210 234 Test Loss: 0.799 | Test Acc: 77.829% (10510/13504)\n",
      "234 Epoch: 47 | Test Loss: 0.787 | Test Acc: 78.092% (11695/14976)\n",
      "\n",
      "Epoch: 48\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 48 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.880 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 0.797 | Test Acc: 77.671% (1541/1984)\n",
      "60 234 Test Loss: 0.797 | Test Acc: 78.125% (3050/3904)\n",
      "90 234 Test Loss: 0.799 | Test Acc: 78.245% (4557/5824)\n",
      "120 234 Test Loss: 0.793 | Test Acc: 78.048% (6044/7744)\n",
      "150 234 Test Loss: 0.791 | Test Acc: 78.011% (7539/9664)\n",
      "180 234 Test Loss: 0.790 | Test Acc: 77.978% (9033/11584)\n",
      "210 234 Test Loss: 0.792 | Test Acc: 77.851% (10513/13504)\n",
      "234 Epoch: 48 | Test Loss: 0.785 | Test Acc: 78.065% (11691/14976)\n",
      "\n",
      "Epoch: 49\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 49 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.696 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 0.784 | Test Acc: 78.377% (1555/1984)\n",
      "60 234 Test Loss: 0.778 | Test Acc: 78.458% (3063/3904)\n",
      "90 234 Test Loss: 0.779 | Test Acc: 78.589% (4577/5824)\n",
      "120 234 Test Loss: 0.769 | Test Acc: 78.629% (6089/7744)\n",
      "150 234 Test Loss: 0.773 | Test Acc: 78.425% (7579/9664)\n",
      "180 234 Test Loss: 0.784 | Test Acc: 78.177% (9056/11584)\n",
      "210 234 Test Loss: 0.783 | Test Acc: 78.073% (10543/13504)\n",
      "234 Epoch: 49 | Test Loss: 0.781 | Test Acc: 78.118% (11699/14976)\n",
      "Saving..\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.3.3: Start Target model training\n",
    "\n",
    "max_epoch = 50  #@param {type:\"integer\"}\n",
    "train_result_summary = 'target_train_VGG-BiLSTM.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(target_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(target_testloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZxD7D1wnrIq",
    "outputId": "77ef5120-9cd1-49d0-b5c6-451c2047fb7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_training_summary(filepath = 'target_train_VGG-BiLSTM.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpiecUvqnrIr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiymTeGFnrIr"
   },
   "source": [
    "## Method 4: ResNet18 (with bi-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7S-OVkQcnrIr",
    "outputId": "dc0e5415-e981-4537-e0e2-3712979ece30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model for ResNet18-BiLSTM\n",
      "==> Training model from scratch..\n",
      "Total trained parameters:  53185610\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.4.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ResNet18\n",
    "#@markdown * Option 2: ResNet18-LSTM\n",
    "#@markdown * Option 3: ResNet18-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'ResNet18-BiLSTM'  #@param {type:\"string\"}\n",
    "save_model_folder = './ResNet18-BiLSTM_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'ResNet18-BiLSTM':\n",
    "  # Model\n",
    "  net = ResNet(BasicBlock, [2, 2, 2, 2], enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'ResNet18-LSTM':\n",
    "  # Model\n",
    "  net = ResNet(BasicBlock, [2, 2, 2, 2], enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'ResNet18':\n",
    "  # Model\n",
    "  net = ResNet(BasicBlock, [2, 2, 2, 2], enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQ2nLj3OnrIr",
    "outputId": "d2d43644-7cba-4a0d-b674-d159618d60dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Setting target_train_dataset size to  15000 Counter({4: 1548, 1: 1543, 3: 1536, 5: 1519, 7: 1514, 6: 1513, 8: 1485, 2: 1451, 0: 1446, 9: 1445})\n",
      "Setting target_test_dataset size to  15000 Counter({5: 1523, 9: 1522, 4: 1522, 0: 1509, 7: 1506, 2: 1503, 8: 1496, 6: 1487, 3: 1481, 1: 1451})\n",
      "Setting shadow_train_dataset size to  15000 Counter({9: 1558, 8: 1551, 0: 1520, 2: 1515, 7: 1511, 3: 1499, 6: 1484, 4: 1458, 5: 1454, 1: 1450})\n",
      "Setting shadow_test_dataset size to  15000 Counter({1: 1556, 2: 1531, 0: 1525, 6: 1516, 5: 1504, 3: 1484, 9: 1475, 4: 1472, 7: 1469, 8: 1468})\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.4.2: Setup Target and Shadow datasets for DLA Training\n",
    "\n",
    "target_train_size = 15000 #@param {type:\"integer\"}\n",
    "target_test_size= 15000 #@param {type:\"integer\"}\n",
    "shadow_train_size = 15000  #@param {type:\"integer\"} \n",
    "shadow_test_size= 15000 #@param {type:\"integer\"}\n",
    "\n",
    "# create dataset for renet \n",
    "print('==> Preparing dataset..')\n",
    "target_trainloader, target_testloader, shadow_train_dataset, shadow_testloader = create_cifar_dataset_torch(batch_size=batch_size, target_train_size = target_train_size, target_test_size= target_train_size, shadow_train_size = target_train_size, shadow_test_size= target_train_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyDij04znrIr",
    "outputId": "f1cfb872-7756-4079-d211-707ebefa5b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 234 Train Loss: 2.301 | Train Acc: 14.062% (9/64)\n",
      "30 234 Train Loss: 2.278 | Train Acc: 12.550% (249/1984)\n",
      "60 234 Train Loss: 2.165 | Train Acc: 16.522% (645/3904)\n",
      "90 234 Train Loss: 2.072 | Train Acc: 19.420% (1131/5824)\n",
      "120 234 Train Loss: 1.988 | Train Acc: 22.766% (1763/7744)\n",
      "150 234 Train Loss: 1.931 | Train Acc: 25.228% (2438/9664)\n",
      "180 234 Train Loss: 1.885 | Train Acc: 26.778% (3102/11584)\n",
      "210 234 Train Loss: 1.846 | Train Acc: 28.303% (3822/13504)\n",
      "234 Epoch: 0 | Train Loss: 1.816 | Train Acc: 29.728% (4452/14976)\n",
      "0 234 Test Loss: 1.735 | Test Acc: 35.938% (23/64)\n",
      "30 234 Test Loss: 1.705 | Test Acc: 36.139% (717/1984)\n",
      "60 234 Test Loss: 1.687 | Test Acc: 36.142% (1411/3904)\n",
      "90 234 Test Loss: 1.679 | Test Acc: 36.762% (2141/5824)\n",
      "120 234 Test Loss: 1.682 | Test Acc: 36.299% (2811/7744)\n",
      "150 234 Test Loss: 1.680 | Test Acc: 36.362% (3514/9664)\n",
      "180 234 Test Loss: 1.677 | Test Acc: 36.274% (4202/11584)\n",
      "210 234 Test Loss: 1.675 | Test Acc: 36.411% (4917/13504)\n",
      "234 Epoch: 0 | Test Loss: 1.674 | Test Acc: 36.478% (5463/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "0 234 Train Loss: 1.603 | Train Acc: 31.250% (20/64)\n",
      "30 234 Train Loss: 1.487 | Train Acc: 42.792% (849/1984)\n",
      "60 234 Train Loss: 1.434 | Train Acc: 45.236% (1766/3904)\n",
      "90 234 Train Loss: 1.415 | Train Acc: 46.669% (2718/5824)\n",
      "120 234 Train Loss: 1.409 | Train Acc: 47.314% (3664/7744)\n",
      "150 234 Train Loss: 1.391 | Train Acc: 48.137% (4652/9664)\n",
      "180 234 Train Loss: 1.375 | Train Acc: 48.662% (5637/11584)\n",
      "210 234 Train Loss: 1.358 | Train Acc: 49.282% (6655/13504)\n",
      "234 Epoch: 1 | Train Loss: 1.351 | Train Acc: 49.619% (7431/14976)\n",
      "0 234 Test Loss: 1.449 | Test Acc: 46.875% (30/64)\n",
      "30 234 Test Loss: 1.398 | Test Acc: 48.085% (954/1984)\n",
      "60 234 Test Loss: 1.421 | Test Acc: 47.669% (1861/3904)\n",
      "90 234 Test Loss: 1.411 | Test Acc: 48.128% (2803/5824)\n",
      "120 234 Test Loss: 1.411 | Test Acc: 48.425% (3750/7744)\n",
      "150 234 Test Loss: 1.421 | Test Acc: 48.117% (4650/9664)\n",
      "180 234 Test Loss: 1.415 | Test Acc: 48.282% (5593/11584)\n",
      "210 234 Test Loss: 1.411 | Test Acc: 48.578% (6560/13504)\n",
      "234 Epoch: 1 | Test Loss: 1.414 | Test Acc: 48.544% (7270/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "0 234 Train Loss: 1.094 | Train Acc: 59.375% (38/64)\n",
      "30 234 Train Loss: 1.141 | Train Acc: 58.720% (1165/1984)\n",
      "60 234 Train Loss: 1.134 | Train Acc: 59.068% (2306/3904)\n",
      "90 234 Train Loss: 1.119 | Train Acc: 59.427% (3461/5824)\n",
      "120 234 Train Loss: 1.118 | Train Acc: 59.233% (4587/7744)\n",
      "150 234 Train Loss: 1.108 | Train Acc: 59.644% (5764/9664)\n",
      "180 234 Train Loss: 1.099 | Train Acc: 60.057% (6957/11584)\n",
      "210 234 Train Loss: 1.090 | Train Acc: 60.493% (8169/13504)\n",
      "234 Epoch: 2 | Train Loss: 1.078 | Train Acc: 60.971% (9131/14976)\n",
      "0 234 Test Loss: 1.099 | Test Acc: 54.688% (35/64)\n",
      "30 234 Test Loss: 1.119 | Test Acc: 60.786% (1206/1984)\n",
      "60 234 Test Loss: 1.116 | Test Acc: 61.040% (2383/3904)\n",
      "90 234 Test Loss: 1.124 | Test Acc: 60.165% (3504/5824)\n",
      "120 234 Test Loss: 1.124 | Test Acc: 60.163% (4659/7744)\n",
      "150 234 Test Loss: 1.132 | Test Acc: 59.716% (5771/9664)\n",
      "180 234 Test Loss: 1.131 | Test Acc: 59.720% (6918/11584)\n",
      "210 234 Test Loss: 1.126 | Test Acc: 59.960% (8097/13504)\n",
      "234 Epoch: 2 | Test Loss: 1.124 | Test Acc: 59.963% (8980/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "0 234 Train Loss: 0.909 | Train Acc: 64.062% (41/64)\n",
      "30 234 Train Loss: 0.853 | Train Acc: 69.204% (1373/1984)\n",
      "60 234 Train Loss: 0.889 | Train Acc: 67.879% (2650/3904)\n",
      "90 234 Train Loss: 0.887 | Train Acc: 67.977% (3959/5824)\n",
      "120 234 Train Loss: 0.886 | Train Acc: 68.143% (5277/7744)\n",
      "150 234 Train Loss: 0.874 | Train Acc: 68.615% (6631/9664)\n",
      "180 234 Train Loss: 0.874 | Train Acc: 68.672% (7955/11584)\n",
      "210 234 Train Loss: 0.866 | Train Acc: 69.135% (9336/13504)\n",
      "234 Epoch: 3 | Train Loss: 0.869 | Train Acc: 69.017% (10336/14976)\n",
      "0 234 Test Loss: 1.221 | Test Acc: 54.688% (35/64)\n",
      "30 234 Test Loss: 1.079 | Test Acc: 62.399% (1238/1984)\n",
      "60 234 Test Loss: 1.059 | Test Acc: 62.859% (2454/3904)\n",
      "90 234 Test Loss: 1.051 | Test Acc: 63.204% (3681/5824)\n",
      "120 234 Test Loss: 1.048 | Test Acc: 63.210% (4895/7744)\n",
      "150 234 Test Loss: 1.061 | Test Acc: 62.603% (6050/9664)\n",
      "180 234 Test Loss: 1.058 | Test Acc: 62.543% (7245/11584)\n",
      "210 234 Test Loss: 1.058 | Test Acc: 62.537% (8445/13504)\n",
      "234 Epoch: 3 | Test Loss: 1.064 | Test Acc: 62.360% (9339/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "0 234 Train Loss: 0.692 | Train Acc: 71.875% (46/64)\n",
      "30 234 Train Loss: 0.680 | Train Acc: 75.958% (1507/1984)\n",
      "60 234 Train Loss: 0.675 | Train Acc: 75.922% (2964/3904)\n",
      "90 234 Train Loss: 0.661 | Train Acc: 76.580% (4460/5824)\n",
      "120 234 Train Loss: 0.670 | Train Acc: 75.943% (5881/7744)\n",
      "150 234 Train Loss: 0.674 | Train Acc: 75.755% (7321/9664)\n",
      "180 234 Train Loss: 0.683 | Train Acc: 75.337% (8727/11584)\n",
      "210 234 Train Loss: 0.684 | Train Acc: 75.474% (10192/13504)\n",
      "234 Epoch: 4 | Train Loss: 0.686 | Train Acc: 75.467% (11302/14976)\n",
      "0 234 Test Loss: 1.491 | Test Acc: 48.438% (31/64)\n",
      "30 234 Test Loss: 1.216 | Test Acc: 59.173% (1174/1984)\n",
      "60 234 Test Loss: 1.213 | Test Acc: 60.067% (2345/3904)\n",
      "90 234 Test Loss: 1.207 | Test Acc: 60.113% (3501/5824)\n",
      "120 234 Test Loss: 1.211 | Test Acc: 60.227% (4664/7744)\n",
      "150 234 Test Loss: 1.211 | Test Acc: 60.068% (5805/9664)\n",
      "180 234 Test Loss: 1.218 | Test Acc: 60.135% (6966/11584)\n",
      "210 234 Test Loss: 1.214 | Test Acc: 60.093% (8115/13504)\n",
      "234 Epoch: 4 | Test Loss: 1.213 | Test Acc: 60.103% (9001/14976)\n",
      "\n",
      "Epoch: 5\n",
      "0 234 Train Loss: 0.561 | Train Acc: 78.125% (50/64)\n",
      "30 234 Train Loss: 0.486 | Train Acc: 83.619% (1659/1984)\n",
      "60 234 Train Loss: 0.497 | Train Acc: 82.633% (3226/3904)\n",
      "90 234 Train Loss: 0.493 | Train Acc: 82.435% (4801/5824)\n",
      "120 234 Train Loss: 0.513 | Train Acc: 81.547% (6315/7744)\n",
      "150 234 Train Loss: 0.526 | Train Acc: 81.136% (7841/9664)\n",
      "180 234 Train Loss: 0.534 | Train Acc: 80.818% (9362/11584)\n",
      "210 234 Train Loss: 0.538 | Train Acc: 80.739% (10903/13504)\n",
      "234 Epoch: 5 | Train Loss: 0.538 | Train Acc: 80.876% (12112/14976)\n",
      "0 234 Test Loss: 1.022 | Test Acc: 60.938% (39/64)\n",
      "30 234 Test Loss: 1.242 | Test Acc: 62.450% (1239/1984)\n",
      "60 234 Test Loss: 1.282 | Test Acc: 61.988% (2420/3904)\n",
      "90 234 Test Loss: 1.266 | Test Acc: 62.122% (3618/5824)\n",
      "120 234 Test Loss: 1.252 | Test Acc: 62.087% (4808/7744)\n",
      "150 234 Test Loss: 1.260 | Test Acc: 61.755% (5968/9664)\n",
      "180 234 Test Loss: 1.261 | Test Acc: 61.619% (7138/11584)\n",
      "210 234 Test Loss: 1.259 | Test Acc: 61.537% (8310/13504)\n",
      "234 Epoch: 5 | Test Loss: 1.263 | Test Acc: 61.438% (9201/14976)\n",
      "\n",
      "Epoch: 6\n",
      "0 234 Train Loss: 0.651 | Train Acc: 79.688% (51/64)\n",
      "30 234 Train Loss: 0.349 | Train Acc: 88.407% (1754/1984)\n",
      "60 234 Train Loss: 0.346 | Train Acc: 88.089% (3439/3904)\n",
      "90 234 Train Loss: 0.362 | Train Acc: 87.483% (5095/5824)\n",
      "120 234 Train Loss: 0.371 | Train Acc: 87.268% (6758/7744)\n",
      "150 234 Train Loss: 0.379 | Train Acc: 86.755% (8384/9664)\n",
      "180 234 Train Loss: 0.383 | Train Acc: 86.628% (10035/11584)\n",
      "210 234 Train Loss: 0.389 | Train Acc: 86.271% (11650/13504)\n",
      "234 Epoch: 6 | Train Loss: 0.396 | Train Acc: 86.038% (12885/14976)\n",
      "0 234 Test Loss: 1.359 | Test Acc: 57.812% (37/64)\n",
      "30 234 Test Loss: 1.024 | Test Acc: 67.087% (1331/1984)\n",
      "60 234 Test Loss: 1.037 | Test Acc: 67.162% (2622/3904)\n",
      "90 234 Test Loss: 1.029 | Test Acc: 67.188% (3913/5824)\n",
      "120 234 Test Loss: 1.045 | Test Acc: 66.865% (5178/7744)\n",
      "150 234 Test Loss: 1.049 | Test Acc: 66.722% (6448/9664)\n",
      "180 234 Test Loss: 1.048 | Test Acc: 66.825% (7741/11584)\n",
      "210 234 Test Loss: 1.043 | Test Acc: 66.802% (9021/13504)\n",
      "234 Epoch: 6 | Test Loss: 1.041 | Test Acc: 66.934% (10024/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      "0 234 Train Loss: 0.201 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.224 | Train Acc: 92.893% (1843/1984)\n",
      "60 234 Train Loss: 0.232 | Train Acc: 92.162% (3598/3904)\n",
      "90 234 Train Loss: 0.240 | Train Acc: 91.690% (5340/5824)\n",
      "120 234 Train Loss: 0.254 | Train Acc: 91.103% (7055/7744)\n",
      "150 234 Train Loss: 0.266 | Train Acc: 90.728% (8768/9664)\n",
      "180 234 Train Loss: 0.274 | Train Acc: 90.349% (10466/11584)\n",
      "210 234 Train Loss: 0.279 | Train Acc: 90.040% (12159/13504)\n",
      "234 Epoch: 7 | Train Loss: 0.281 | Train Acc: 89.931% (13468/14976)\n",
      "0 234 Test Loss: 1.214 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.197 | Test Acc: 65.524% (1300/1984)\n",
      "60 234 Test Loss: 1.198 | Test Acc: 65.984% (2576/3904)\n",
      "90 234 Test Loss: 1.209 | Test Acc: 65.745% (3829/5824)\n",
      "120 234 Test Loss: 1.211 | Test Acc: 65.999% (5111/7744)\n",
      "150 234 Test Loss: 1.219 | Test Acc: 65.842% (6363/9664)\n",
      "180 234 Test Loss: 1.210 | Test Acc: 66.117% (7659/11584)\n",
      "210 234 Test Loss: 1.204 | Test Acc: 66.151% (8933/13504)\n",
      "234 Epoch: 7 | Test Loss: 1.193 | Test Acc: 66.226% (9918/14976)\n",
      "\n",
      "Epoch: 8\n",
      "0 234 Train Loss: 0.174 | Train Acc: 92.188% (59/64)\n",
      "30 234 Train Loss: 0.184 | Train Acc: 93.548% (1856/1984)\n",
      "60 234 Train Loss: 0.189 | Train Acc: 93.468% (3649/3904)\n",
      "90 234 Train Loss: 0.183 | Train Acc: 93.630% (5453/5824)\n",
      "120 234 Train Loss: 0.180 | Train Acc: 93.776% (7262/7744)\n",
      "150 234 Train Loss: 0.187 | Train Acc: 93.460% (9032/9664)\n",
      "180 234 Train Loss: 0.192 | Train Acc: 93.258% (10803/11584)\n",
      "210 234 Train Loss: 0.196 | Train Acc: 93.217% (12588/13504)\n",
      "234 Epoch: 8 | Train Loss: 0.197 | Train Acc: 93.202% (13958/14976)\n",
      "0 234 Test Loss: 1.095 | Test Acc: 67.188% (43/64)\n",
      "30 234 Test Loss: 1.198 | Test Acc: 68.296% (1355/1984)\n",
      "60 234 Test Loss: 1.222 | Test Acc: 67.623% (2640/3904)\n",
      "90 234 Test Loss: 1.215 | Test Acc: 67.771% (3947/5824)\n",
      "120 234 Test Loss: 1.206 | Test Acc: 67.536% (5230/7744)\n",
      "150 234 Test Loss: 1.201 | Test Acc: 67.695% (6542/9664)\n",
      "180 234 Test Loss: 1.183 | Test Acc: 68.025% (7880/11584)\n",
      "210 234 Test Loss: 1.172 | Test Acc: 68.121% (9199/13504)\n",
      "234 Epoch: 8 | Test Loss: 1.167 | Test Acc: 68.109% (10200/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      "0 234 Train Loss: 0.168 | Train Acc: 93.750% (60/64)\n",
      "30 234 Train Loss: 0.134 | Train Acc: 95.262% (1890/1984)\n",
      "60 234 Train Loss: 0.128 | Train Acc: 95.774% (3739/3904)\n",
      "90 234 Train Loss: 0.122 | Train Acc: 95.810% (5580/5824)\n",
      "120 234 Train Loss: 0.128 | Train Acc: 95.661% (7408/7744)\n",
      "150 234 Train Loss: 0.131 | Train Acc: 95.592% (9238/9664)\n",
      "180 234 Train Loss: 0.139 | Train Acc: 95.226% (11031/11584)\n",
      "210 234 Train Loss: 0.151 | Train Acc: 94.757% (12796/13504)\n",
      "234 Epoch: 9 | Train Loss: 0.157 | Train Acc: 94.525% (14156/14976)\n",
      "0 234 Test Loss: 1.345 | Test Acc: 64.062% (41/64)\n",
      "30 234 Test Loss: 1.405 | Test Acc: 65.121% (1292/1984)\n",
      "60 234 Test Loss: 1.405 | Test Acc: 65.394% (2553/3904)\n",
      "90 234 Test Loss: 1.402 | Test Acc: 65.453% (3812/5824)\n",
      "120 234 Test Loss: 1.408 | Test Acc: 64.966% (5031/7744)\n",
      "150 234 Test Loss: 1.418 | Test Acc: 64.839% (6266/9664)\n",
      "180 234 Test Loss: 1.441 | Test Acc: 64.546% (7477/11584)\n",
      "210 234 Test Loss: 1.442 | Test Acc: 64.477% (8707/13504)\n",
      "234 Epoch: 9 | Test Loss: 1.448 | Test Acc: 64.216% (9617/14976)\n",
      "\n",
      "Epoch: 10\n",
      "0 234 Train Loss: 0.181 | Train Acc: 93.750% (60/64)\n",
      "30 234 Train Loss: 0.143 | Train Acc: 94.909% (1883/1984)\n",
      "60 234 Train Loss: 0.124 | Train Acc: 95.902% (3744/3904)\n",
      "90 234 Train Loss: 0.119 | Train Acc: 96.120% (5598/5824)\n",
      "120 234 Train Loss: 0.116 | Train Acc: 96.113% (7443/7744)\n",
      "150 234 Train Loss: 0.119 | Train Acc: 95.944% (9272/9664)\n",
      "180 234 Train Loss: 0.121 | Train Acc: 95.874% (11106/11584)\n",
      "210 234 Train Loss: 0.120 | Train Acc: 95.927% (12954/13504)\n",
      "234 Epoch: 10 | Train Loss: 0.120 | Train Acc: 95.933% (14367/14976)\n",
      "0 234 Test Loss: 1.177 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.177 | Test Acc: 70.161% (1392/1984)\n",
      "60 234 Test Loss: 1.184 | Test Acc: 70.082% (2736/3904)\n",
      "90 234 Test Loss: 1.167 | Test Acc: 70.725% (4119/5824)\n",
      "120 234 Test Loss: 1.152 | Test Acc: 70.752% (5479/7744)\n",
      "150 234 Test Loss: 1.147 | Test Acc: 70.851% (6847/9664)\n",
      "180 234 Test Loss: 1.147 | Test Acc: 70.891% (8212/11584)\n",
      "210 234 Test Loss: 1.148 | Test Acc: 71.001% (9588/13504)\n",
      "234 Epoch: 10 | Test Loss: 1.144 | Test Acc: 71.060% (10642/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 11\n",
      "0 234 Train Loss: 0.199 | Train Acc: 95.312% (61/64)\n",
      "30 234 Train Loss: 0.071 | Train Acc: 97.883% (1942/1984)\n",
      "60 234 Train Loss: 0.071 | Train Acc: 97.874% (3821/3904)\n",
      "90 234 Train Loss: 0.067 | Train Acc: 98.043% (5710/5824)\n",
      "120 234 Train Loss: 0.067 | Train Acc: 97.947% (7585/7744)\n",
      "150 234 Train Loss: 0.068 | Train Acc: 97.889% (9460/9664)\n",
      "180 234 Train Loss: 0.071 | Train Acc: 97.764% (11325/11584)\n",
      "210 234 Train Loss: 0.075 | Train Acc: 97.556% (13174/13504)\n",
      "234 Epoch: 11 | Train Loss: 0.078 | Train Acc: 97.476% (14598/14976)\n",
      "0 234 Test Loss: 1.647 | Test Acc: 62.500% (40/64)\n",
      "30 234 Test Loss: 1.200 | Test Acc: 70.565% (1400/1984)\n",
      "60 234 Test Loss: 1.152 | Test Acc: 70.902% (2768/3904)\n",
      "90 234 Test Loss: 1.152 | Test Acc: 71.326% (4154/5824)\n",
      "120 234 Test Loss: 1.138 | Test Acc: 71.449% (5533/7744)\n",
      "150 234 Test Loss: 1.137 | Test Acc: 71.420% (6902/9664)\n",
      "180 234 Test Loss: 1.143 | Test Acc: 71.236% (8252/11584)\n",
      "210 234 Test Loss: 1.141 | Test Acc: 71.238% (9620/13504)\n",
      "234 Epoch: 11 | Test Loss: 1.142 | Test Acc: 71.201% (10663/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 12\n",
      "0 234 Train Loss: 0.042 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.072 | Train Acc: 97.681% (1938/1984)\n",
      "60 234 Train Loss: 0.071 | Train Acc: 97.643% (3812/3904)\n",
      "90 234 Train Loss: 0.073 | Train Acc: 97.562% (5682/5824)\n",
      "120 234 Train Loss: 0.077 | Train Acc: 97.366% (7540/7744)\n",
      "150 234 Train Loss: 0.076 | Train Acc: 97.413% (9414/9664)\n",
      "180 234 Train Loss: 0.077 | Train Acc: 97.393% (11282/11584)\n",
      "210 234 Train Loss: 0.077 | Train Acc: 97.438% (13158/13504)\n",
      "234 Epoch: 12 | Train Loss: 0.078 | Train Acc: 97.423% (14590/14976)\n",
      "0 234 Test Loss: 1.439 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 1.296 | Test Acc: 70.161% (1392/1984)\n",
      "60 234 Test Loss: 1.266 | Test Acc: 70.697% (2760/3904)\n",
      "90 234 Test Loss: 1.260 | Test Acc: 70.364% (4098/5824)\n",
      "120 234 Test Loss: 1.254 | Test Acc: 70.584% (5466/7744)\n",
      "150 234 Test Loss: 1.244 | Test Acc: 70.633% (6826/9664)\n",
      "180 234 Test Loss: 1.253 | Test Acc: 70.425% (8158/11584)\n",
      "210 234 Test Loss: 1.246 | Test Acc: 70.564% (9529/13504)\n",
      "234 Epoch: 12 | Test Loss: 1.239 | Test Acc: 70.693% (10587/14976)\n",
      "\n",
      "Epoch: 13\n",
      "0 234 Train Loss: 0.037 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.058 | Train Acc: 98.135% (1947/1984)\n",
      "60 234 Train Loss: 0.053 | Train Acc: 98.309% (3838/3904)\n",
      "90 234 Train Loss: 0.047 | Train Acc: 98.541% (5739/5824)\n",
      "120 234 Train Loss: 0.053 | Train Acc: 98.244% (7608/7744)\n",
      "150 234 Train Loss: 0.055 | Train Acc: 98.241% (9494/9664)\n",
      "180 234 Train Loss: 0.056 | Train Acc: 98.213% (11377/11584)\n",
      "210 234 Train Loss: 0.055 | Train Acc: 98.260% (13269/13504)\n",
      "234 Epoch: 13 | Train Loss: 0.055 | Train Acc: 98.251% (14714/14976)\n",
      "0 234 Test Loss: 1.288 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 1.307 | Test Acc: 67.994% (1349/1984)\n",
      "60 234 Test Loss: 1.283 | Test Acc: 68.878% (2689/3904)\n",
      "90 234 Test Loss: 1.291 | Test Acc: 69.093% (4024/5824)\n",
      "120 234 Test Loss: 1.290 | Test Acc: 69.034% (5346/7744)\n",
      "150 234 Test Loss: 1.288 | Test Acc: 69.495% (6716/9664)\n",
      "180 234 Test Loss: 1.284 | Test Acc: 69.639% (8067/11584)\n",
      "210 234 Test Loss: 1.283 | Test Acc: 69.853% (9433/13504)\n",
      "234 Epoch: 13 | Test Loss: 1.294 | Test Acc: 69.685% (10436/14976)\n",
      "\n",
      "Epoch: 14\n",
      "0 234 Train Loss: 0.025 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.031 | Train Acc: 99.093% (1966/1984)\n",
      "60 234 Train Loss: 0.028 | Train Acc: 99.180% (3872/3904)\n",
      "90 234 Train Loss: 0.025 | Train Acc: 99.262% (5781/5824)\n",
      "120 234 Train Loss: 0.026 | Train Acc: 99.238% (7685/7744)\n",
      "150 234 Train Loss: 0.026 | Train Acc: 99.265% (9593/9664)\n",
      "180 234 Train Loss: 0.028 | Train Acc: 99.171% (11488/11584)\n",
      "210 234 Train Loss: 0.030 | Train Acc: 99.134% (13387/13504)\n",
      "234 Epoch: 14 | Train Loss: 0.031 | Train Acc: 99.085% (14839/14976)\n",
      "0 234 Test Loss: 1.617 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.287 | Test Acc: 70.817% (1405/1984)\n",
      "60 234 Test Loss: 1.261 | Test Acc: 70.927% (2769/3904)\n",
      "90 234 Test Loss: 1.305 | Test Acc: 70.484% (4105/5824)\n",
      "120 234 Test Loss: 1.294 | Test Acc: 70.713% (5476/7744)\n",
      "150 234 Test Loss: 1.288 | Test Acc: 70.871% (6849/9664)\n",
      "180 234 Test Loss: 1.274 | Test Acc: 71.003% (8225/11584)\n",
      "210 234 Test Loss: 1.279 | Test Acc: 70.942% (9580/13504)\n",
      "234 Epoch: 14 | Test Loss: 1.280 | Test Acc: 70.887% (10616/14976)\n",
      "\n",
      "Epoch: 15\n",
      "0 234 Train Loss: 0.043 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.036 | Train Acc: 98.639% (1957/1984)\n",
      "60 234 Train Loss: 0.034 | Train Acc: 98.745% (3855/3904)\n",
      "90 234 Train Loss: 0.033 | Train Acc: 98.867% (5758/5824)\n",
      "120 234 Train Loss: 0.032 | Train Acc: 98.915% (7660/7744)\n",
      "150 234 Train Loss: 0.031 | Train Acc: 98.965% (9564/9664)\n",
      "180 234 Train Loss: 0.031 | Train Acc: 98.981% (11466/11584)\n",
      "210 234 Train Loss: 0.032 | Train Acc: 98.904% (13356/13504)\n",
      "234 Epoch: 15 | Train Loss: 0.033 | Train Acc: 98.892% (14810/14976)\n",
      "0 234 Test Loss: 1.200 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 1.282 | Test Acc: 70.968% (1408/1984)\n",
      "60 234 Test Loss: 1.376 | Test Acc: 69.544% (2715/3904)\n",
      "90 234 Test Loss: 1.371 | Test Acc: 69.574% (4052/5824)\n",
      "120 234 Test Loss: 1.364 | Test Acc: 69.783% (5404/7744)\n",
      "150 234 Test Loss: 1.366 | Test Acc: 69.785% (6744/9664)\n",
      "180 234 Test Loss: 1.356 | Test Acc: 69.924% (8100/11584)\n",
      "210 234 Test Loss: 1.355 | Test Acc: 69.987% (9451/13504)\n",
      "234 Epoch: 15 | Test Loss: 1.351 | Test Acc: 70.126% (10502/14976)\n",
      "\n",
      "Epoch: 16\n",
      "0 234 Train Loss: 0.010 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.032 | Train Acc: 98.942% (1963/1984)\n",
      "60 234 Train Loss: 0.026 | Train Acc: 99.206% (3873/3904)\n",
      "90 234 Train Loss: 0.024 | Train Acc: 99.279% (5782/5824)\n",
      "120 234 Train Loss: 0.026 | Train Acc: 99.174% (7680/7744)\n",
      "150 234 Train Loss: 0.027 | Train Acc: 99.089% (9576/9664)\n",
      "180 234 Train Loss: 0.030 | Train Acc: 99.016% (11470/11584)\n",
      "210 234 Train Loss: 0.030 | Train Acc: 99.045% (13375/13504)\n",
      "234 Epoch: 16 | Train Loss: 0.031 | Train Acc: 98.985% (14824/14976)\n",
      "0 234 Test Loss: 1.545 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.477 | Test Acc: 68.044% (1350/1984)\n",
      "60 234 Test Loss: 1.475 | Test Acc: 67.725% (2644/3904)\n",
      "90 234 Test Loss: 1.484 | Test Acc: 67.376% (3924/5824)\n",
      "120 234 Test Loss: 1.454 | Test Acc: 67.988% (5265/7744)\n",
      "150 234 Test Loss: 1.442 | Test Acc: 68.264% (6597/9664)\n",
      "180 234 Test Loss: 1.464 | Test Acc: 67.947% (7871/11584)\n",
      "210 234 Test Loss: 1.455 | Test Acc: 68.084% (9194/13504)\n",
      "234 Epoch: 16 | Test Loss: 1.457 | Test Acc: 68.015% (10186/14976)\n",
      "\n",
      "Epoch: 17\n",
      "0 234 Train Loss: 0.032 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.028 | Train Acc: 99.093% (1966/1984)\n",
      "60 234 Train Loss: 0.029 | Train Acc: 99.129% (3870/3904)\n",
      "90 234 Train Loss: 0.028 | Train Acc: 99.107% (5772/5824)\n",
      "120 234 Train Loss: 0.032 | Train Acc: 99.057% (7671/7744)\n",
      "150 234 Train Loss: 0.034 | Train Acc: 98.955% (9563/9664)\n",
      "180 234 Train Loss: 0.035 | Train Acc: 98.912% (11458/11584)\n",
      "210 234 Train Loss: 0.038 | Train Acc: 98.808% (13343/13504)\n",
      "234 Epoch: 17 | Train Loss: 0.038 | Train Acc: 98.791% (14795/14976)\n",
      "0 234 Test Loss: 1.333 | Test Acc: 71.875% (46/64)\n",
      "30 234 Test Loss: 1.623 | Test Acc: 66.381% (1317/1984)\n",
      "60 234 Test Loss: 1.574 | Test Acc: 66.675% (2603/3904)\n",
      "90 234 Test Loss: 1.530 | Test Acc: 67.617% (3938/5824)\n",
      "120 234 Test Loss: 1.533 | Test Acc: 67.988% (5265/7744)\n",
      "150 234 Test Loss: 1.507 | Test Acc: 68.450% (6615/9664)\n",
      "180 234 Test Loss: 1.502 | Test Acc: 68.621% (7949/11584)\n",
      "210 234 Test Loss: 1.481 | Test Acc: 68.906% (9305/13504)\n",
      "234 Epoch: 17 | Test Loss: 1.461 | Test Acc: 69.164% (10358/14976)\n",
      "\n",
      "Epoch: 18\n",
      "0 234 Train Loss: 0.012 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.039 | Train Acc: 98.790% (1960/1984)\n",
      "60 234 Train Loss: 0.045 | Train Acc: 98.591% (3849/3904)\n",
      "90 234 Train Loss: 0.049 | Train Acc: 98.438% (5733/5824)\n",
      "120 234 Train Loss: 0.055 | Train Acc: 98.140% (7600/7744)\n",
      "150 234 Train Loss: 0.059 | Train Acc: 98.086% (9479/9664)\n",
      "180 234 Train Loss: 0.058 | Train Acc: 98.092% (11363/11584)\n",
      "210 234 Train Loss: 0.055 | Train Acc: 98.178% (13258/13504)\n",
      "234 Epoch: 18 | Train Loss: 0.054 | Train Acc: 98.177% (14703/14976)\n",
      "0 234 Test Loss: 1.696 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.474 | Test Acc: 69.506% (1379/1984)\n",
      "60 234 Test Loss: 1.411 | Test Acc: 69.980% (2732/3904)\n",
      "90 234 Test Loss: 1.390 | Test Acc: 69.866% (4069/5824)\n",
      "120 234 Test Loss: 1.387 | Test Acc: 69.589% (5389/7744)\n",
      "150 234 Test Loss: 1.403 | Test Acc: 69.174% (6685/9664)\n",
      "180 234 Test Loss: 1.399 | Test Acc: 69.190% (8015/11584)\n",
      "210 234 Test Loss: 1.397 | Test Acc: 68.980% (9315/13504)\n",
      "234 Epoch: 18 | Test Loss: 1.389 | Test Acc: 69.057% (10342/14976)\n",
      "\n",
      "Epoch: 19\n",
      "0 234 Train Loss: 0.025 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.025 | Train Acc: 99.395% (1972/1984)\n",
      "60 234 Train Loss: 0.025 | Train Acc: 99.462% (3883/3904)\n",
      "90 234 Train Loss: 0.022 | Train Acc: 99.451% (5792/5824)\n",
      "120 234 Train Loss: 0.021 | Train Acc: 99.483% (7704/7744)\n",
      "150 234 Train Loss: 0.020 | Train Acc: 99.483% (9614/9664)\n",
      "180 234 Train Loss: 0.022 | Train Acc: 99.422% (11517/11584)\n",
      "210 234 Train Loss: 0.024 | Train Acc: 99.356% (13417/13504)\n",
      "234 Epoch: 19 | Train Loss: 0.025 | Train Acc: 99.299% (14871/14976)\n",
      "0 234 Test Loss: 1.206 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 1.311 | Test Acc: 70.716% (1403/1984)\n",
      "60 234 Test Loss: 1.241 | Test Acc: 71.132% (2777/3904)\n",
      "90 234 Test Loss: 1.229 | Test Acc: 71.686% (4175/5824)\n",
      "120 234 Test Loss: 1.243 | Test Acc: 71.449% (5533/7744)\n",
      "150 234 Test Loss: 1.243 | Test Acc: 71.451% (6905/9664)\n",
      "180 234 Test Loss: 1.236 | Test Acc: 71.417% (8273/11584)\n",
      "210 234 Test Loss: 1.237 | Test Acc: 71.305% (9629/13504)\n",
      "234 Epoch: 19 | Test Loss: 1.240 | Test Acc: 71.401% (10693/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 20\n",
      "0 234 Train Loss: 0.006 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.024 | Train Acc: 99.244% (1969/1984)\n",
      "60 234 Train Loss: 0.021 | Train Acc: 99.257% (3875/3904)\n",
      "90 234 Train Loss: 0.022 | Train Acc: 99.245% (5780/5824)\n",
      "120 234 Train Loss: 0.022 | Train Acc: 99.277% (7688/7744)\n",
      "150 234 Train Loss: 0.022 | Train Acc: 99.265% (9593/9664)\n",
      "180 234 Train Loss: 0.022 | Train Acc: 99.258% (11498/11584)\n",
      "210 234 Train Loss: 0.023 | Train Acc: 99.222% (13399/13504)\n",
      "234 Epoch: 20 | Train Loss: 0.023 | Train Acc: 99.219% (14859/14976)\n",
      "0 234 Test Loss: 0.994 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.263 | Test Acc: 71.925% (1427/1984)\n",
      "60 234 Test Loss: 1.203 | Test Acc: 73.079% (2853/3904)\n",
      "90 234 Test Loss: 1.221 | Test Acc: 72.957% (4249/5824)\n",
      "120 234 Test Loss: 1.238 | Test Acc: 72.637% (5625/7744)\n",
      "150 234 Test Loss: 1.238 | Test Acc: 72.403% (6997/9664)\n",
      "180 234 Test Loss: 1.249 | Test Acc: 72.125% (8355/11584)\n",
      "210 234 Test Loss: 1.245 | Test Acc: 72.282% (9761/13504)\n",
      "234 Epoch: 20 | Test Loss: 1.244 | Test Acc: 72.149% (10805/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 21\n",
      "0 234 Train Loss: 0.017 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.024 | Train Acc: 99.294% (1970/1984)\n",
      "60 234 Train Loss: 0.021 | Train Acc: 99.385% (3880/3904)\n",
      "90 234 Train Loss: 0.019 | Train Acc: 99.433% (5791/5824)\n",
      "120 234 Train Loss: 0.018 | Train Acc: 99.496% (7705/7744)\n",
      "150 234 Train Loss: 0.016 | Train Acc: 99.555% (9621/9664)\n",
      "180 234 Train Loss: 0.015 | Train Acc: 99.603% (11538/11584)\n",
      "210 234 Train Loss: 0.015 | Train Acc: 99.615% (13452/13504)\n",
      "234 Epoch: 21 | Train Loss: 0.014 | Train Acc: 99.646% (14923/14976)\n",
      "0 234 Test Loss: 0.952 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.131 | Test Acc: 73.286% (1454/1984)\n",
      "60 234 Test Loss: 1.147 | Test Acc: 73.233% (2859/3904)\n",
      "90 234 Test Loss: 1.134 | Test Acc: 73.403% (4275/5824)\n",
      "120 234 Test Loss: 1.133 | Test Acc: 73.773% (5713/7744)\n",
      "150 234 Test Loss: 1.138 | Test Acc: 73.696% (7122/9664)\n",
      "180 234 Test Loss: 1.149 | Test Acc: 73.645% (8531/11584)\n",
      "210 234 Test Loss: 1.156 | Test Acc: 73.556% (9933/13504)\n",
      "234 Epoch: 21 | Test Loss: 1.149 | Test Acc: 73.665% (11032/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 22\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.003 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 99.990% (9663/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 99.991% (11583/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 99.993% (13503/13504)\n",
      "234 Epoch: 22 | Train Loss: 0.003 | Train Acc: 99.980% (14973/14976)\n",
      "0 234 Test Loss: 1.301 | Test Acc: 71.875% (46/64)\n",
      "30 234 Test Loss: 1.133 | Test Acc: 73.740% (1463/1984)\n",
      "60 234 Test Loss: 1.107 | Test Acc: 74.667% (2915/3904)\n",
      "90 234 Test Loss: 1.120 | Test Acc: 74.468% (4337/5824)\n",
      "120 234 Test Loss: 1.110 | Test Acc: 74.483% (5768/7744)\n",
      "150 234 Test Loss: 1.110 | Test Acc: 74.400% (7190/9664)\n",
      "180 234 Test Loss: 1.103 | Test Acc: 74.594% (8641/11584)\n",
      "210 234 Test Loss: 1.095 | Test Acc: 74.704% (10088/13504)\n",
      "234 Epoch: 22 | Test Loss: 1.086 | Test Acc: 74.639% (11178/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 23\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 99.966% (5822/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 99.948% (7740/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 99.938% (9658/9664)\n",
      "180 234 Train Loss: 0.005 | Train Acc: 99.896% (11572/11584)\n",
      "210 234 Train Loss: 0.005 | Train Acc: 99.896% (13490/13504)\n",
      "234 Epoch: 23 | Train Loss: 0.005 | Train Acc: 99.900% (14961/14976)\n",
      "0 234 Test Loss: 1.273 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.013 | Test Acc: 74.446% (1477/1984)\n",
      "60 234 Test Loss: 1.093 | Test Acc: 74.283% (2900/3904)\n",
      "90 234 Test Loss: 1.070 | Test Acc: 74.691% (4350/5824)\n",
      "120 234 Test Loss: 1.075 | Test Acc: 74.587% (5776/7744)\n",
      "150 234 Test Loss: 1.062 | Test Acc: 74.948% (7243/9664)\n",
      "180 234 Test Loss: 1.072 | Test Acc: 74.793% (8664/11584)\n",
      "210 234 Test Loss: 1.082 | Test Acc: 74.622% (10077/13504)\n",
      "234 Epoch: 23 | Test Loss: 1.085 | Test Acc: 74.479% (11154/14976)\n",
      "\n",
      "Epoch: 24\n",
      "0 234 Train Loss: 0.004 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.003 | Train Acc: 99.974% (3903/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 99.983% (5823/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 99.987% (7743/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 99.990% (9663/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 99.991% (11583/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 99.993% (13503/13504)\n",
      "234 Epoch: 24 | Train Loss: 0.002 | Train Acc: 99.993% (14975/14976)\n",
      "0 234 Test Loss: 1.176 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.015 | Test Acc: 75.353% (1495/1984)\n",
      "60 234 Test Loss: 0.977 | Test Acc: 76.281% (2978/3904)\n",
      "90 234 Test Loss: 0.985 | Test Acc: 75.962% (4424/5824)\n",
      "120 234 Test Loss: 0.995 | Test Acc: 76.059% (5890/7744)\n",
      "150 234 Test Loss: 1.003 | Test Acc: 75.973% (7342/9664)\n",
      "180 234 Test Loss: 1.007 | Test Acc: 76.053% (8810/11584)\n",
      "210 234 Test Loss: 1.013 | Test Acc: 75.763% (10231/13504)\n",
      "234 Epoch: 24 | Test Loss: 1.017 | Test Acc: 75.741% (11343/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 25\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 25 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.821 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 0.968 | Test Acc: 75.655% (1501/1984)\n",
      "60 234 Test Loss: 0.960 | Test Acc: 75.948% (2965/3904)\n",
      "90 234 Test Loss: 0.975 | Test Acc: 75.876% (4419/5824)\n",
      "120 234 Test Loss: 0.982 | Test Acc: 75.956% (5882/7744)\n",
      "150 234 Test Loss: 0.981 | Test Acc: 76.045% (7349/9664)\n",
      "180 234 Test Loss: 0.985 | Test Acc: 76.036% (8808/11584)\n",
      "210 234 Test Loss: 0.985 | Test Acc: 75.918% (10252/13504)\n",
      "234 Epoch: 25 | Test Loss: 0.990 | Test Acc: 75.735% (11342/14976)\n",
      "\n",
      "Epoch: 26\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 26 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.670 | Test Acc: 85.938% (55/64)\n",
      "30 234 Test Loss: 0.998 | Test Acc: 75.554% (1499/1984)\n",
      "60 234 Test Loss: 1.001 | Test Acc: 75.717% (2956/3904)\n",
      "90 234 Test Loss: 0.986 | Test Acc: 75.618% (4404/5824)\n",
      "120 234 Test Loss: 1.006 | Test Acc: 75.323% (5833/7744)\n",
      "150 234 Test Loss: 0.971 | Test Acc: 76.035% (7348/9664)\n",
      "180 234 Test Loss: 0.964 | Test Acc: 76.226% (8830/11584)\n",
      "210 234 Test Loss: 0.966 | Test Acc: 76.007% (10264/13504)\n",
      "234 Epoch: 26 | Test Loss: 0.966 | Test Acc: 75.888% (11365/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 27\n",
      "0 234 Train Loss: 0.000 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 27 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 1.008 | Test Acc: 71.875% (46/64)\n",
      "30 234 Test Loss: 0.941 | Test Acc: 76.058% (1509/1984)\n",
      "60 234 Test Loss: 0.951 | Test Acc: 75.512% (2948/3904)\n",
      "90 234 Test Loss: 0.972 | Test Acc: 75.429% (4393/5824)\n",
      "120 234 Test Loss: 0.957 | Test Acc: 75.801% (5870/7744)\n",
      "150 234 Test Loss: 0.951 | Test Acc: 75.890% (7334/9664)\n",
      "180 234 Test Loss: 0.940 | Test Acc: 76.088% (8814/11584)\n",
      "210 234 Test Loss: 0.943 | Test Acc: 76.037% (10268/13504)\n",
      "234 Epoch: 27 | Test Loss: 0.957 | Test Acc: 75.775% (11348/14976)\n",
      "\n",
      "Epoch: 28\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 28 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 1.221 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 0.935 | Test Acc: 76.462% (1517/1984)\n",
      "60 234 Test Loss: 0.987 | Test Acc: 75.179% (2935/3904)\n",
      "90 234 Test Loss: 0.975 | Test Acc: 75.172% (4378/5824)\n",
      "120 234 Test Loss: 0.993 | Test Acc: 75.000% (5808/7744)\n",
      "150 234 Test Loss: 0.973 | Test Acc: 75.393% (7286/9664)\n",
      "180 234 Test Loss: 0.953 | Test Acc: 75.665% (8765/11584)\n",
      "210 234 Test Loss: 0.951 | Test Acc: 75.585% (10207/13504)\n",
      "234 Epoch: 28 | Test Loss: 0.942 | Test Acc: 75.788% (11350/14976)\n",
      "\n",
      "Epoch: 29\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.001 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 29 | Train Loss: 0.001 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.806 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 0.913 | Test Acc: 76.260% (1513/1984)\n",
      "60 234 Test Loss: 0.902 | Test Acc: 76.511% (2987/3904)\n",
      "90 234 Test Loss: 0.873 | Test Acc: 76.562% (4459/5824)\n",
      "120 234 Test Loss: 0.893 | Test Acc: 76.472% (5922/7744)\n",
      "150 234 Test Loss: 0.894 | Test Acc: 76.366% (7380/9664)\n",
      "180 234 Test Loss: 0.908 | Test Acc: 76.174% (8824/11584)\n",
      "210 234 Test Loss: 0.910 | Test Acc: 76.222% (10293/13504)\n",
      "234 Epoch: 29 | Test Loss: 0.918 | Test Acc: 76.022% (11385/14976)\n",
      "Saving..\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.4.3: Start Target model training\n",
    "\n",
    "max_epoch = 30  #@param {type:\"integer\"}\n",
    "train_result_summary = 'target_train_ResNet18-BiLSTM.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(target_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(target_testloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_dPtmjknrIs",
    "outputId": "6f744450-086f-4f9e-b0c1-609f02220e8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_training_summary(filepath = 'target_train_ResNet18-BiLSTM.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0P8E1qQUnrIs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLfvUGOunrIs"
   },
   "source": [
    "## Method 5: DenseNet (with bi-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXKX1IvtnrIs",
    "outputId": "dc0e5415-e981-4537-e0e2-3712979ece30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model for DenseNet121-BiLSTM\n",
      "==> Training model from scratch..\n",
      "Total trained parameters:  57351434\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.5.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: DenseNet121\n",
    "#@markdown * Option 2: DenseNet121-LSTM\n",
    "#@markdown * Option 3: DenseNet121-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DenseNet121-BiLSTM'  #@param {type:\"string\"}\n",
    "save_model_folder = './DenseNet121-BiLSTM_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DenseNet121-BiLSTM':\n",
    "  # Model\n",
    "  net = DenseNet(Bottleneck, [6,12,24,16], growth_rate=32, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DenseNet121-LSTM':\n",
    "  # Model\n",
    "  net = DenseNet(Bottleneck, [6,12,24,16], growth_rate=32, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DenseNet121':\n",
    "  # Model\n",
    "  net = DenseNet(Bottleneck, [6,12,24,16], growth_rate=32, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0m6t4penrIs",
    "outputId": "d2d43644-7cba-4a0d-b674-d159618d60dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Setting target_train_dataset size to  15000 Counter({1: 1540, 5: 1510, 3: 1510, 4: 1507, 7: 1506, 0: 1504, 6: 1491, 2: 1484, 9: 1476, 8: 1472})\n",
      "Setting target_test_dataset size to  15000 Counter({4: 1562, 8: 1555, 7: 1519, 2: 1509, 3: 1503, 9: 1492, 0: 1481, 5: 1476, 6: 1474, 1: 1429})\n",
      "Setting shadow_train_dataset size to  15000 Counter({2: 1542, 5: 1527, 9: 1512, 6: 1505, 7: 1503, 3: 1503, 0: 1498, 8: 1489, 1: 1465, 4: 1456})\n",
      "Setting shadow_test_dataset size to  15000 Counter({1: 1566, 6: 1530, 9: 1520, 0: 1517, 5: 1487, 3: 1484, 8: 1484, 4: 1475, 7: 1472, 2: 1465})\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.4.2: Setup Target and Shadow datasets for DLA Training\n",
    "\n",
    "target_train_size = 15000 #@param {type:\"integer\"}\n",
    "target_test_size= 15000 #@param {type:\"integer\"}\n",
    "shadow_train_size = 15000  #@param {type:\"integer\"} \n",
    "shadow_test_size= 15000 #@param {type:\"integer\"}\n",
    "\n",
    "# create dataset for renet \n",
    "print('==> Preparing dataset..')\n",
    "target_trainloader, target_testloader, shadow_train_dataset, shadow_testloader = create_cifar_dataset_torch(batch_size=batch_size, target_train_size = target_train_size, target_test_size= target_train_size, shadow_train_size = target_train_size, shadow_test_size= target_train_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjFkyZ2AnrIt",
    "outputId": "f1cfb872-7756-4079-d211-707ebefa5b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 234 Train Loss: 2.304 | Train Acc: 12.500% (8/64)\n",
      "30 234 Train Loss: 2.264 | Train Acc: 15.222% (302/1984)\n",
      "60 234 Train Loss: 2.172 | Train Acc: 19.621% (766/3904)\n",
      "90 234 Train Loss: 2.080 | Train Acc: 21.909% (1276/5824)\n",
      "120 234 Train Loss: 2.003 | Train Acc: 23.967% (1856/7744)\n",
      "150 234 Train Loss: 1.946 | Train Acc: 26.159% (2528/9664)\n",
      "180 234 Train Loss: 1.901 | Train Acc: 27.650% (3203/11584)\n",
      "210 234 Train Loss: 1.859 | Train Acc: 29.088% (3928/13504)\n",
      "234 Epoch: 0 | Train Loss: 1.827 | Train Acc: 30.442% (4559/14976)\n",
      "0 234 Test Loss: 1.907 | Test Acc: 31.250% (20/64)\n",
      "30 234 Test Loss: 1.893 | Test Acc: 32.863% (652/1984)\n",
      "60 234 Test Loss: 1.899 | Test Acc: 32.659% (1275/3904)\n",
      "90 234 Test Loss: 1.908 | Test Acc: 31.834% (1854/5824)\n",
      "120 234 Test Loss: 1.917 | Test Acc: 32.038% (2481/7744)\n",
      "150 234 Test Loss: 1.920 | Test Acc: 32.243% (3116/9664)\n",
      "180 234 Test Loss: 1.922 | Test Acc: 32.269% (3738/11584)\n",
      "210 234 Test Loss: 1.917 | Test Acc: 32.457% (4383/13504)\n",
      "234 Epoch: 0 | Test Loss: 1.916 | Test Acc: 32.452% (4860/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "0 234 Train Loss: 1.395 | Train Acc: 43.750% (28/64)\n",
      "30 234 Train Loss: 1.528 | Train Acc: 42.440% (842/1984)\n",
      "60 234 Train Loss: 1.495 | Train Acc: 44.057% (1720/3904)\n",
      "90 234 Train Loss: 1.461 | Train Acc: 45.364% (2642/5824)\n",
      "120 234 Train Loss: 1.441 | Train Acc: 46.320% (3587/7744)\n",
      "150 234 Train Loss: 1.425 | Train Acc: 46.782% (4521/9664)\n",
      "180 234 Train Loss: 1.399 | Train Acc: 47.842% (5542/11584)\n",
      "210 234 Train Loss: 1.384 | Train Acc: 48.489% (6548/13504)\n",
      "234 Epoch: 1 | Train Loss: 1.365 | Train Acc: 49.439% (7404/14976)\n",
      "0 234 Test Loss: 2.638 | Test Acc: 28.125% (18/64)\n",
      "30 234 Test Loss: 2.568 | Test Acc: 25.252% (501/1984)\n",
      "60 234 Test Loss: 2.556 | Test Acc: 26.281% (1026/3904)\n",
      "90 234 Test Loss: 2.559 | Test Acc: 26.408% (1538/5824)\n",
      "120 234 Test Loss: 2.527 | Test Acc: 26.924% (2085/7744)\n",
      "150 234 Test Loss: 2.538 | Test Acc: 26.728% (2583/9664)\n",
      "180 234 Test Loss: 2.527 | Test Acc: 27.037% (3132/11584)\n",
      "210 234 Test Loss: 2.531 | Test Acc: 27.022% (3649/13504)\n",
      "234 Epoch: 1 | Test Loss: 2.532 | Test Acc: 27.030% (4048/14976)\n",
      "\n",
      "Epoch: 2\n",
      "0 234 Train Loss: 1.069 | Train Acc: 60.938% (39/64)\n",
      "30 234 Train Loss: 1.133 | Train Acc: 58.921% (1169/1984)\n",
      "60 234 Train Loss: 1.140 | Train Acc: 58.658% (2290/3904)\n",
      "90 234 Train Loss: 1.123 | Train Acc: 59.323% (3455/5824)\n",
      "120 234 Train Loss: 1.111 | Train Acc: 59.607% (4616/7744)\n",
      "150 234 Train Loss: 1.110 | Train Acc: 59.447% (5745/9664)\n",
      "180 234 Train Loss: 1.102 | Train Acc: 59.617% (6906/11584)\n",
      "210 234 Train Loss: 1.084 | Train Acc: 60.315% (8145/13504)\n",
      "234 Epoch: 2 | Train Loss: 1.071 | Train Acc: 60.831% (9110/14976)\n",
      "0 234 Test Loss: 1.626 | Test Acc: 45.312% (29/64)\n",
      "30 234 Test Loss: 1.276 | Test Acc: 54.435% (1080/1984)\n",
      "60 234 Test Loss: 1.273 | Test Acc: 55.097% (2151/3904)\n",
      "90 234 Test Loss: 1.265 | Test Acc: 54.894% (3197/5824)\n",
      "120 234 Test Loss: 1.260 | Test Acc: 55.243% (4278/7744)\n",
      "150 234 Test Loss: 1.263 | Test Acc: 55.391% (5353/9664)\n",
      "180 234 Test Loss: 1.264 | Test Acc: 55.300% (6406/11584)\n",
      "210 234 Test Loss: 1.261 | Test Acc: 55.287% (7466/13504)\n",
      "234 Epoch: 2 | Test Loss: 1.259 | Test Acc: 55.395% (8296/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "0 234 Train Loss: 0.796 | Train Acc: 67.188% (43/64)\n",
      "30 234 Train Loss: 0.868 | Train Acc: 67.893% (1347/1984)\n",
      "60 234 Train Loss: 0.866 | Train Acc: 68.263% (2665/3904)\n",
      "90 234 Train Loss: 0.887 | Train Acc: 68.132% (3968/5824)\n",
      "120 234 Train Loss: 0.884 | Train Acc: 67.975% (5264/7744)\n",
      "150 234 Train Loss: 0.888 | Train Acc: 67.695% (6542/9664)\n",
      "180 234 Train Loss: 0.886 | Train Acc: 67.775% (7851/11584)\n",
      "210 234 Train Loss: 0.887 | Train Acc: 67.847% (9162/13504)\n",
      "234 Epoch: 3 | Train Loss: 0.888 | Train Acc: 67.795% (10153/14976)\n",
      "0 234 Test Loss: 1.228 | Test Acc: 59.375% (38/64)\n",
      "30 234 Test Loss: 1.003 | Test Acc: 64.819% (1286/1984)\n",
      "60 234 Test Loss: 0.979 | Test Acc: 64.985% (2537/3904)\n",
      "90 234 Test Loss: 0.977 | Test Acc: 65.711% (3827/5824)\n",
      "120 234 Test Loss: 0.980 | Test Acc: 65.586% (5079/7744)\n",
      "150 234 Test Loss: 0.994 | Test Acc: 64.911% (6273/9664)\n",
      "180 234 Test Loss: 0.989 | Test Acc: 64.978% (7527/11584)\n",
      "210 234 Test Loss: 0.993 | Test Acc: 64.677% (8734/13504)\n",
      "234 Epoch: 3 | Test Loss: 0.990 | Test Acc: 64.817% (9707/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "0 234 Train Loss: 0.884 | Train Acc: 67.188% (43/64)\n",
      "30 234 Train Loss: 0.738 | Train Acc: 73.639% (1461/1984)\n",
      "60 234 Train Loss: 0.735 | Train Acc: 73.873% (2884/3904)\n",
      "90 234 Train Loss: 0.749 | Train Acc: 73.489% (4280/5824)\n",
      "120 234 Train Loss: 0.756 | Train Acc: 73.476% (5690/7744)\n",
      "150 234 Train Loss: 0.764 | Train Acc: 73.179% (7072/9664)\n",
      "180 234 Train Loss: 0.759 | Train Acc: 73.368% (8499/11584)\n",
      "210 234 Train Loss: 0.757 | Train Acc: 73.556% (9933/13504)\n",
      "234 Epoch: 4 | Train Loss: 0.758 | Train Acc: 73.324% (10981/14976)\n",
      "0 234 Test Loss: 0.855 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.098 | Test Acc: 61.492% (1220/1984)\n",
      "60 234 Test Loss: 1.120 | Test Acc: 61.270% (2392/3904)\n",
      "90 234 Test Loss: 1.107 | Test Acc: 61.418% (3577/5824)\n",
      "120 234 Test Loss: 1.098 | Test Acc: 61.854% (4790/7744)\n",
      "150 234 Test Loss: 1.101 | Test Acc: 61.786% (5971/9664)\n",
      "180 234 Test Loss: 1.096 | Test Acc: 62.034% (7186/11584)\n",
      "210 234 Test Loss: 1.095 | Test Acc: 61.915% (8361/13504)\n",
      "234 Epoch: 4 | Test Loss: 1.098 | Test Acc: 61.772% (9251/14976)\n",
      "\n",
      "Epoch: 5\n",
      "0 234 Train Loss: 0.545 | Train Acc: 78.125% (50/64)\n",
      "30 234 Train Loss: 0.597 | Train Acc: 79.788% (1583/1984)\n",
      "60 234 Train Loss: 0.595 | Train Acc: 79.739% (3113/3904)\n",
      "90 234 Train Loss: 0.615 | Train Acc: 78.554% (4575/5824)\n",
      "120 234 Train Loss: 0.617 | Train Acc: 78.345% (6067/7744)\n",
      "150 234 Train Loss: 0.624 | Train Acc: 77.949% (7533/9664)\n",
      "180 234 Train Loss: 0.623 | Train Acc: 78.082% (9045/11584)\n",
      "210 234 Train Loss: 0.618 | Train Acc: 78.273% (10570/13504)\n",
      "234 Epoch: 5 | Train Loss: 0.621 | Train Acc: 78.198% (11711/14976)\n",
      "0 234 Test Loss: 1.186 | Test Acc: 57.812% (37/64)\n",
      "30 234 Test Loss: 1.007 | Test Acc: 66.784% (1325/1984)\n",
      "60 234 Test Loss: 1.022 | Test Acc: 66.650% (2602/3904)\n",
      "90 234 Test Loss: 1.014 | Test Acc: 66.724% (3886/5824)\n",
      "120 234 Test Loss: 1.002 | Test Acc: 66.981% (5187/7744)\n",
      "150 234 Test Loss: 1.010 | Test Acc: 66.825% (6458/9664)\n",
      "180 234 Test Loss: 1.017 | Test Acc: 66.471% (7700/11584)\n",
      "210 234 Test Loss: 1.021 | Test Acc: 66.417% (8969/13504)\n",
      "234 Epoch: 5 | Test Loss: 1.018 | Test Acc: 66.426% (9948/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      "0 234 Train Loss: 0.391 | Train Acc: 85.938% (55/64)\n",
      "30 234 Train Loss: 0.530 | Train Acc: 80.897% (1605/1984)\n",
      "60 234 Train Loss: 0.518 | Train Acc: 81.429% (3179/3904)\n",
      "90 234 Train Loss: 0.524 | Train Acc: 81.559% (4750/5824)\n",
      "120 234 Train Loss: 0.531 | Train Acc: 81.289% (6295/7744)\n",
      "150 234 Train Loss: 0.542 | Train Acc: 80.722% (7801/9664)\n",
      "180 234 Train Loss: 0.545 | Train Acc: 80.715% (9350/11584)\n",
      "210 234 Train Loss: 0.545 | Train Acc: 80.717% (10900/13504)\n",
      "234 Epoch: 6 | Train Loss: 0.541 | Train Acc: 80.876% (12112/14976)\n",
      "0 234 Test Loss: 0.682 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 0.852 | Test Acc: 71.371% (1416/1984)\n",
      "60 234 Test Loss: 0.843 | Test Acc: 71.311% (2784/3904)\n",
      "90 234 Test Loss: 0.869 | Test Acc: 70.416% (4101/5824)\n",
      "120 234 Test Loss: 0.854 | Test Acc: 70.919% (5492/7744)\n",
      "150 234 Test Loss: 0.853 | Test Acc: 71.016% (6863/9664)\n",
      "180 234 Test Loss: 0.851 | Test Acc: 71.089% (8235/11584)\n",
      "210 234 Test Loss: 0.862 | Test Acc: 70.935% (9579/13504)\n",
      "234 Epoch: 6 | Test Loss: 0.859 | Test Acc: 71.074% (10644/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      "0 234 Train Loss: 0.307 | Train Acc: 89.062% (57/64)\n",
      "30 234 Train Loss: 0.418 | Train Acc: 85.181% (1690/1984)\n",
      "60 234 Train Loss: 0.401 | Train Acc: 85.989% (3357/3904)\n",
      "90 234 Train Loss: 0.411 | Train Acc: 85.680% (4990/5824)\n",
      "120 234 Train Loss: 0.425 | Train Acc: 85.072% (6588/7744)\n",
      "150 234 Train Loss: 0.435 | Train Acc: 84.799% (8195/9664)\n",
      "180 234 Train Loss: 0.446 | Train Acc: 84.401% (9777/11584)\n",
      "210 234 Train Loss: 0.444 | Train Acc: 84.419% (11400/13504)\n",
      "234 Epoch: 7 | Train Loss: 0.447 | Train Acc: 84.255% (12618/14976)\n",
      "0 234 Test Loss: 1.044 | Test Acc: 71.875% (46/64)\n",
      "30 234 Test Loss: 0.949 | Test Acc: 69.556% (1380/1984)\n",
      "60 234 Test Loss: 0.923 | Test Acc: 70.236% (2742/3904)\n",
      "90 234 Test Loss: 0.936 | Test Acc: 69.557% (4051/5824)\n",
      "120 234 Test Loss: 0.929 | Test Acc: 69.770% (5403/7744)\n",
      "150 234 Test Loss: 0.929 | Test Acc: 69.868% (6752/9664)\n",
      "180 234 Test Loss: 0.926 | Test Acc: 70.062% (8116/11584)\n",
      "210 234 Test Loss: 0.921 | Test Acc: 70.090% (9465/13504)\n",
      "234 Epoch: 7 | Test Loss: 0.921 | Test Acc: 69.905% (10469/14976)\n",
      "\n",
      "Epoch: 8\n",
      "0 234 Train Loss: 0.441 | Train Acc: 92.188% (59/64)\n",
      "30 234 Train Loss: 0.320 | Train Acc: 89.869% (1783/1984)\n",
      "60 234 Train Loss: 0.313 | Train Acc: 89.959% (3512/3904)\n",
      "90 234 Train Loss: 0.323 | Train Acc: 89.269% (5199/5824)\n",
      "120 234 Train Loss: 0.335 | Train Acc: 88.753% (6873/7744)\n",
      "150 234 Train Loss: 0.348 | Train Acc: 88.235% (8527/9664)\n",
      "180 234 Train Loss: 0.356 | Train Acc: 87.897% (10182/11584)\n",
      "210 234 Train Loss: 0.367 | Train Acc: 87.411% (11804/13504)\n",
      "234 Epoch: 8 | Train Loss: 0.372 | Train Acc: 87.166% (13054/14976)\n",
      "0 234 Test Loss: 0.753 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 0.780 | Test Acc: 74.446% (1477/1984)\n",
      "60 234 Test Loss: 0.783 | Test Acc: 74.308% (2901/3904)\n",
      "90 234 Test Loss: 0.793 | Test Acc: 73.901% (4304/5824)\n",
      "120 234 Test Loss: 0.793 | Test Acc: 74.006% (5731/7744)\n",
      "150 234 Test Loss: 0.800 | Test Acc: 73.696% (7122/9664)\n",
      "180 234 Test Loss: 0.808 | Test Acc: 73.463% (8510/11584)\n",
      "210 234 Test Loss: 0.800 | Test Acc: 73.749% (9959/13504)\n",
      "234 Epoch: 8 | Test Loss: 0.799 | Test Acc: 73.758% (11046/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      "0 234 Train Loss: 0.253 | Train Acc: 93.750% (60/64)\n",
      "30 234 Train Loss: 0.289 | Train Acc: 89.768% (1781/1984)\n",
      "60 234 Train Loss: 0.288 | Train Acc: 89.985% (3513/3904)\n",
      "90 234 Train Loss: 0.297 | Train Acc: 89.801% (5230/5824)\n",
      "120 234 Train Loss: 0.293 | Train Acc: 90.044% (6973/7744)\n",
      "150 234 Train Loss: 0.300 | Train Acc: 89.683% (8667/9664)\n",
      "180 234 Train Loss: 0.302 | Train Acc: 89.503% (10368/11584)\n",
      "210 234 Train Loss: 0.303 | Train Acc: 89.507% (12087/13504)\n",
      "234 Epoch: 9 | Train Loss: 0.308 | Train Acc: 89.343% (13380/14976)\n",
      "0 234 Test Loss: 1.075 | Test Acc: 65.625% (42/64)\n",
      "30 234 Test Loss: 0.795 | Test Acc: 74.143% (1471/1984)\n",
      "60 234 Test Loss: 0.824 | Test Acc: 73.309% (2862/3904)\n",
      "90 234 Test Loss: 0.818 | Test Acc: 73.386% (4274/5824)\n",
      "120 234 Test Loss: 0.804 | Test Acc: 73.709% (5708/7744)\n",
      "150 234 Test Loss: 0.807 | Test Acc: 73.913% (7143/9664)\n",
      "180 234 Test Loss: 0.808 | Test Acc: 73.999% (8572/11584)\n",
      "210 234 Test Loss: 0.808 | Test Acc: 74.037% (9998/13504)\n",
      "234 Epoch: 9 | Test Loss: 0.807 | Test Acc: 73.978% (11079/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 10\n",
      "0 234 Train Loss: 0.175 | Train Acc: 93.750% (60/64)\n",
      "30 234 Train Loss: 0.217 | Train Acc: 92.893% (1843/1984)\n",
      "60 234 Train Loss: 0.234 | Train Acc: 92.341% (3605/3904)\n",
      "90 234 Train Loss: 0.241 | Train Acc: 91.861% (5350/5824)\n",
      "120 234 Train Loss: 0.253 | Train Acc: 91.335% (7073/7744)\n",
      "150 234 Train Loss: 0.256 | Train Acc: 91.070% (8801/9664)\n",
      "180 234 Train Loss: 0.260 | Train Acc: 90.953% (10536/11584)\n",
      "210 234 Train Loss: 0.265 | Train Acc: 90.758% (12256/13504)\n",
      "234 Epoch: 10 | Train Loss: 0.270 | Train Acc: 90.525% (13557/14976)\n",
      "0 234 Test Loss: 0.727 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 0.775 | Test Acc: 74.093% (1470/1984)\n",
      "60 234 Test Loss: 0.787 | Test Acc: 73.822% (2882/3904)\n",
      "90 234 Test Loss: 0.797 | Test Acc: 74.073% (4314/5824)\n",
      "120 234 Test Loss: 0.798 | Test Acc: 74.019% (5732/7744)\n",
      "150 234 Test Loss: 0.801 | Test Acc: 74.038% (7155/9664)\n",
      "180 234 Test Loss: 0.802 | Test Acc: 74.163% (8591/11584)\n",
      "210 234 Test Loss: 0.799 | Test Acc: 74.126% (10010/13504)\n",
      "234 Epoch: 10 | Test Loss: 0.799 | Test Acc: 74.239% (11118/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 11\n",
      "0 234 Train Loss: 0.203 | Train Acc: 93.750% (60/64)\n",
      "30 234 Train Loss: 0.190 | Train Acc: 93.448% (1854/1984)\n",
      "60 234 Train Loss: 0.184 | Train Acc: 93.929% (3667/3904)\n",
      "90 234 Train Loss: 0.182 | Train Acc: 93.990% (5474/5824)\n",
      "120 234 Train Loss: 0.187 | Train Acc: 93.737% (7259/7744)\n",
      "150 234 Train Loss: 0.195 | Train Acc: 93.377% (9024/9664)\n",
      "180 234 Train Loss: 0.201 | Train Acc: 93.163% (10792/11584)\n",
      "210 234 Train Loss: 0.205 | Train Acc: 92.965% (12554/13504)\n",
      "234 Epoch: 11 | Train Loss: 0.208 | Train Acc: 92.929% (13917/14976)\n",
      "0 234 Test Loss: 0.677 | Test Acc: 82.812% (53/64)\n",
      "30 234 Test Loss: 0.895 | Test Acc: 74.294% (1474/1984)\n",
      "60 234 Test Loss: 0.888 | Test Acc: 74.513% (2909/3904)\n",
      "90 234 Test Loss: 0.895 | Test Acc: 74.056% (4313/5824)\n",
      "120 234 Test Loss: 0.889 | Test Acc: 74.264% (5751/7744)\n",
      "150 234 Test Loss: 0.902 | Test Acc: 73.976% (7149/9664)\n",
      "180 234 Test Loss: 0.894 | Test Acc: 74.042% (8577/11584)\n",
      "210 234 Test Loss: 0.899 | Test Acc: 74.000% (9993/13504)\n",
      "234 Epoch: 11 | Test Loss: 0.905 | Test Acc: 73.985% (11080/14976)\n",
      "\n",
      "Epoch: 12\n",
      "0 234 Train Loss: 0.099 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.134 | Train Acc: 95.867% (1902/1984)\n",
      "60 234 Train Loss: 0.127 | Train Acc: 95.876% (3743/3904)\n",
      "90 234 Train Loss: 0.137 | Train Acc: 95.433% (5558/5824)\n",
      "120 234 Train Loss: 0.142 | Train Acc: 95.300% (7380/7744)\n",
      "150 234 Train Loss: 0.144 | Train Acc: 95.095% (9190/9664)\n",
      "180 234 Train Loss: 0.154 | Train Acc: 94.769% (10978/11584)\n",
      "210 234 Train Loss: 0.162 | Train Acc: 94.491% (12760/13504)\n",
      "234 Epoch: 12 | Train Loss: 0.172 | Train Acc: 94.117% (14095/14976)\n",
      "0 234 Test Loss: 0.873 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 0.822 | Test Acc: 75.151% (1491/1984)\n",
      "60 234 Test Loss: 0.834 | Test Acc: 75.794% (2959/3904)\n",
      "90 234 Test Loss: 0.841 | Test Acc: 75.584% (4402/5824)\n",
      "120 234 Test Loss: 0.831 | Test Acc: 75.710% (5863/7744)\n",
      "150 234 Test Loss: 0.848 | Test Acc: 75.259% (7273/9664)\n",
      "180 234 Test Loss: 0.865 | Test Acc: 74.974% (8685/11584)\n",
      "210 234 Test Loss: 0.859 | Test Acc: 75.118% (10144/13504)\n",
      "234 Epoch: 12 | Test Loss: 0.855 | Test Acc: 75.120% (11250/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 13\n",
      "0 234 Train Loss: 0.248 | Train Acc: 92.188% (59/64)\n",
      "30 234 Train Loss: 0.163 | Train Acc: 95.111% (1887/1984)\n",
      "60 234 Train Loss: 0.136 | Train Acc: 95.953% (3746/3904)\n",
      "90 234 Train Loss: 0.134 | Train Acc: 95.931% (5587/5824)\n",
      "120 234 Train Loss: 0.136 | Train Acc: 95.842% (7422/7744)\n",
      "150 234 Train Loss: 0.142 | Train Acc: 95.602% (9239/9664)\n",
      "180 234 Train Loss: 0.151 | Train Acc: 95.218% (11030/11584)\n",
      "210 234 Train Loss: 0.155 | Train Acc: 94.950% (12822/13504)\n",
      "234 Epoch: 13 | Train Loss: 0.162 | Train Acc: 94.665% (14177/14976)\n",
      "0 234 Test Loss: 1.179 | Test Acc: 65.625% (42/64)\n",
      "30 234 Test Loss: 1.028 | Test Acc: 71.774% (1424/1984)\n",
      "60 234 Test Loss: 1.027 | Test Acc: 71.491% (2791/3904)\n",
      "90 234 Test Loss: 1.033 | Test Acc: 71.652% (4173/5824)\n",
      "120 234 Test Loss: 1.038 | Test Acc: 71.681% (5551/7744)\n",
      "150 234 Test Loss: 1.037 | Test Acc: 71.885% (6947/9664)\n",
      "180 234 Test Loss: 1.034 | Test Acc: 71.901% (8329/11584)\n",
      "210 234 Test Loss: 1.046 | Test Acc: 71.616% (9671/13504)\n",
      "234 Epoch: 13 | Test Loss: 1.044 | Test Acc: 71.494% (10707/14976)\n",
      "\n",
      "Epoch: 14\n",
      "0 234 Train Loss: 0.117 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.123 | Train Acc: 95.817% (1901/1984)\n",
      "60 234 Train Loss: 0.117 | Train Acc: 96.183% (3755/3904)\n",
      "90 234 Train Loss: 0.116 | Train Acc: 96.257% (5606/5824)\n",
      "120 234 Train Loss: 0.119 | Train Acc: 96.113% (7443/7744)\n",
      "150 234 Train Loss: 0.123 | Train Acc: 95.923% (9270/9664)\n",
      "180 234 Train Loss: 0.125 | Train Acc: 95.856% (11104/11584)\n",
      "210 234 Train Loss: 0.129 | Train Acc: 95.698% (12923/13504)\n",
      "234 Epoch: 14 | Train Loss: 0.131 | Train Acc: 95.586% (14315/14976)\n",
      "0 234 Test Loss: 0.744 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 0.994 | Test Acc: 71.925% (1427/1984)\n",
      "60 234 Test Loss: 0.985 | Test Acc: 73.105% (2854/3904)\n",
      "90 234 Test Loss: 0.981 | Test Acc: 73.094% (4257/5824)\n",
      "120 234 Test Loss: 0.977 | Test Acc: 73.102% (5661/7744)\n",
      "150 234 Test Loss: 0.974 | Test Acc: 73.489% (7102/9664)\n",
      "180 234 Test Loss: 0.967 | Test Acc: 73.455% (8509/11584)\n",
      "210 234 Test Loss: 0.959 | Test Acc: 73.608% (9940/13504)\n",
      "234 Epoch: 14 | Test Loss: 0.957 | Test Acc: 73.711% (11039/14976)\n",
      "\n",
      "Epoch: 15\n",
      "0 234 Train Loss: 0.111 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.110 | Train Acc: 96.673% (1918/1984)\n",
      "60 234 Train Loss: 0.104 | Train Acc: 96.798% (3779/3904)\n",
      "90 234 Train Loss: 0.105 | Train Acc: 96.686% (5631/5824)\n",
      "120 234 Train Loss: 0.111 | Train Acc: 96.436% (7468/7744)\n",
      "150 234 Train Loss: 0.114 | Train Acc: 96.275% (9304/9664)\n",
      "180 234 Train Loss: 0.121 | Train Acc: 96.029% (11124/11584)\n",
      "210 234 Train Loss: 0.122 | Train Acc: 95.979% (12961/13504)\n",
      "234 Epoch: 15 | Train Loss: 0.122 | Train Acc: 96.034% (14382/14976)\n",
      "0 234 Test Loss: 0.656 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 0.969 | Test Acc: 73.992% (1468/1984)\n",
      "60 234 Test Loss: 0.939 | Test Acc: 74.513% (2909/3904)\n",
      "90 234 Test Loss: 0.931 | Test Acc: 75.120% (4375/5824)\n",
      "120 234 Test Loss: 0.943 | Test Acc: 75.000% (5808/7744)\n",
      "150 234 Test Loss: 0.947 | Test Acc: 75.176% (7265/9664)\n",
      "180 234 Test Loss: 0.960 | Test Acc: 75.017% (8690/11584)\n",
      "210 234 Test Loss: 0.956 | Test Acc: 75.126% (10145/13504)\n",
      "234 Epoch: 15 | Test Loss: 0.959 | Test Acc: 75.073% (11243/14976)\n",
      "\n",
      "Epoch: 16\n",
      "0 234 Train Loss: 0.074 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.070 | Train Acc: 97.581% (1936/1984)\n",
      "60 234 Train Loss: 0.073 | Train Acc: 97.618% (3811/3904)\n",
      "90 234 Train Loss: 0.073 | Train Acc: 97.716% (5691/5824)\n",
      "120 234 Train Loss: 0.080 | Train Acc: 97.456% (7547/7744)\n",
      "150 234 Train Loss: 0.081 | Train Acc: 97.330% (9406/9664)\n",
      "180 234 Train Loss: 0.084 | Train Acc: 97.263% (11267/11584)\n",
      "210 234 Train Loss: 0.084 | Train Acc: 97.245% (13132/13504)\n",
      "234 Epoch: 16 | Train Loss: 0.087 | Train Acc: 97.115% (14544/14976)\n",
      "0 234 Test Loss: 0.767 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 0.923 | Test Acc: 75.554% (1499/1984)\n",
      "60 234 Test Loss: 0.925 | Test Acc: 75.922% (2964/3904)\n",
      "90 234 Test Loss: 0.938 | Test Acc: 75.326% (4387/5824)\n",
      "120 234 Test Loss: 0.931 | Test Acc: 75.542% (5850/7744)\n",
      "150 234 Test Loss: 0.933 | Test Acc: 75.476% (7294/9664)\n",
      "180 234 Test Loss: 0.934 | Test Acc: 75.492% (8745/11584)\n",
      "210 234 Test Loss: 0.933 | Test Acc: 75.600% (10209/13504)\n",
      "234 Epoch: 16 | Test Loss: 0.926 | Test Acc: 75.628% (11326/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 17\n",
      "0 234 Train Loss: 0.124 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.104 | Train Acc: 96.321% (1911/1984)\n",
      "60 234 Train Loss: 0.102 | Train Acc: 96.440% (3765/3904)\n",
      "90 234 Train Loss: 0.096 | Train Acc: 96.617% (5627/5824)\n",
      "120 234 Train Loss: 0.094 | Train Acc: 96.746% (7492/7744)\n",
      "150 234 Train Loss: 0.093 | Train Acc: 96.844% (9359/9664)\n",
      "180 234 Train Loss: 0.093 | Train Acc: 96.815% (11215/11584)\n",
      "210 234 Train Loss: 0.093 | Train Acc: 96.868% (13081/13504)\n",
      "234 Epoch: 17 | Train Loss: 0.093 | Train Acc: 96.862% (14506/14976)\n",
      "0 234 Test Loss: 1.247 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 0.922 | Test Acc: 76.210% (1512/1984)\n",
      "60 234 Test Loss: 0.902 | Test Acc: 76.562% (2989/3904)\n",
      "90 234 Test Loss: 0.895 | Test Acc: 76.442% (4452/5824)\n",
      "120 234 Test Loss: 0.896 | Test Acc: 76.730% (5942/7744)\n",
      "150 234 Test Loss: 0.892 | Test Acc: 76.800% (7422/9664)\n",
      "180 234 Test Loss: 0.898 | Test Acc: 76.701% (8885/11584)\n",
      "210 234 Test Loss: 0.891 | Test Acc: 76.807% (10372/13504)\n",
      "234 Epoch: 17 | Test Loss: 0.887 | Test Acc: 76.910% (11518/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 18\n",
      "0 234 Train Loss: 0.056 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.053 | Train Acc: 98.538% (1955/1984)\n",
      "60 234 Train Loss: 0.053 | Train Acc: 98.207% (3834/3904)\n",
      "90 234 Train Loss: 0.054 | Train Acc: 98.094% (5713/5824)\n",
      "120 234 Train Loss: 0.054 | Train Acc: 98.257% (7609/7744)\n",
      "150 234 Train Loss: 0.059 | Train Acc: 98.086% (9479/9664)\n",
      "180 234 Train Loss: 0.062 | Train Acc: 97.954% (11347/11584)\n",
      "210 234 Train Loss: 0.066 | Train Acc: 97.801% (13207/13504)\n",
      "234 Epoch: 18 | Train Loss: 0.070 | Train Acc: 97.623% (14620/14976)\n",
      "0 234 Test Loss: 0.615 | Test Acc: 84.375% (54/64)\n",
      "30 234 Test Loss: 1.093 | Test Acc: 73.942% (1467/1984)\n",
      "60 234 Test Loss: 1.095 | Test Acc: 74.206% (2897/3904)\n",
      "90 234 Test Loss: 1.082 | Test Acc: 73.901% (4304/5824)\n",
      "120 234 Test Loss: 1.082 | Test Acc: 74.083% (5737/7744)\n",
      "150 234 Test Loss: 1.084 | Test Acc: 74.151% (7166/9664)\n",
      "180 234 Test Loss: 1.088 | Test Acc: 74.102% (8584/11584)\n",
      "210 234 Test Loss: 1.085 | Test Acc: 74.097% (10006/13504)\n",
      "234 Epoch: 18 | Test Loss: 1.089 | Test Acc: 73.972% (11078/14976)\n",
      "\n",
      "Epoch: 19\n",
      "0 234 Train Loss: 0.146 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.104 | Train Acc: 96.774% (1920/1984)\n",
      "60 234 Train Loss: 0.083 | Train Acc: 97.413% (3803/3904)\n",
      "90 234 Train Loss: 0.074 | Train Acc: 97.751% (5693/5824)\n",
      "120 234 Train Loss: 0.069 | Train Acc: 97.895% (7581/7744)\n",
      "150 234 Train Loss: 0.069 | Train Acc: 97.910% (9462/9664)\n",
      "180 234 Train Loss: 0.067 | Train Acc: 97.937% (11345/11584)\n",
      "210 234 Train Loss: 0.066 | Train Acc: 97.964% (13229/13504)\n",
      "234 Epoch: 19 | Train Loss: 0.067 | Train Acc: 97.930% (14666/14976)\n",
      "0 234 Test Loss: 0.682 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 1.001 | Test Acc: 76.109% (1510/1984)\n",
      "60 234 Test Loss: 0.942 | Test Acc: 76.614% (2991/3904)\n",
      "90 234 Test Loss: 0.939 | Test Acc: 76.923% (4480/5824)\n",
      "120 234 Test Loss: 0.933 | Test Acc: 76.640% (5935/7744)\n",
      "150 234 Test Loss: 0.930 | Test Acc: 76.521% (7395/9664)\n",
      "180 234 Test Loss: 0.927 | Test Acc: 76.597% (8873/11584)\n",
      "210 234 Test Loss: 0.924 | Test Acc: 76.518% (10333/13504)\n",
      "234 Epoch: 19 | Test Loss: 0.919 | Test Acc: 76.496% (11456/14976)\n",
      "\n",
      "Epoch: 20\n",
      "0 234 Train Loss: 0.095 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.073 | Train Acc: 97.933% (1943/1984)\n",
      "60 234 Train Loss: 0.061 | Train Acc: 98.130% (3831/3904)\n",
      "90 234 Train Loss: 0.063 | Train Acc: 98.094% (5713/5824)\n",
      "120 234 Train Loss: 0.061 | Train Acc: 98.102% (7597/7744)\n",
      "150 234 Train Loss: 0.061 | Train Acc: 98.065% (9477/9664)\n",
      "180 234 Train Loss: 0.065 | Train Acc: 97.928% (11344/11584)\n",
      "210 234 Train Loss: 0.068 | Train Acc: 97.756% (13201/13504)\n",
      "234 Epoch: 20 | Train Loss: 0.073 | Train Acc: 97.630% (14621/14976)\n",
      "0 234 Test Loss: 0.711 | Test Acc: 84.375% (54/64)\n",
      "30 234 Test Loss: 0.892 | Test Acc: 77.571% (1539/1984)\n",
      "60 234 Test Loss: 0.911 | Test Acc: 77.203% (3014/3904)\n",
      "90 234 Test Loss: 0.924 | Test Acc: 76.786% (4472/5824)\n",
      "120 234 Test Loss: 0.937 | Test Acc: 76.446% (5920/7744)\n",
      "150 234 Test Loss: 0.932 | Test Acc: 76.749% (7417/9664)\n",
      "180 234 Test Loss: 0.933 | Test Acc: 76.761% (8892/11584)\n",
      "210 234 Test Loss: 0.928 | Test Acc: 76.703% (10358/13504)\n",
      "234 Epoch: 20 | Test Loss: 0.930 | Test Acc: 76.576% (11468/14976)\n",
      "\n",
      "Epoch: 21\n",
      "0 234 Train Loss: 0.145 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.052 | Train Acc: 98.639% (1957/1984)\n",
      "60 234 Train Loss: 0.052 | Train Acc: 98.566% (3848/3904)\n",
      "90 234 Train Loss: 0.053 | Train Acc: 98.420% (5732/5824)\n",
      "120 234 Train Loss: 0.054 | Train Acc: 98.399% (7620/7744)\n",
      "150 234 Train Loss: 0.053 | Train Acc: 98.406% (9510/9664)\n",
      "180 234 Train Loss: 0.051 | Train Acc: 98.420% (11401/11584)\n",
      "210 234 Train Loss: 0.053 | Train Acc: 98.363% (13283/13504)\n",
      "234 Epoch: 21 | Train Loss: 0.055 | Train Acc: 98.324% (14725/14976)\n",
      "0 234 Test Loss: 1.094 | Test Acc: 82.812% (53/64)\n",
      "30 234 Test Loss: 0.910 | Test Acc: 76.512% (1518/1984)\n",
      "60 234 Test Loss: 0.917 | Test Acc: 76.819% (2999/3904)\n",
      "90 234 Test Loss: 0.895 | Test Acc: 77.335% (4504/5824)\n",
      "120 234 Test Loss: 0.908 | Test Acc: 77.311% (5987/7744)\n",
      "150 234 Test Loss: 0.899 | Test Acc: 77.690% (7508/9664)\n",
      "180 234 Test Loss: 0.898 | Test Acc: 77.685% (8999/11584)\n",
      "210 234 Test Loss: 0.901 | Test Acc: 77.688% (10491/13504)\n",
      "234 Epoch: 21 | Test Loss: 0.898 | Test Acc: 77.651% (11629/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 22\n",
      "0 234 Train Loss: 0.014 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.044 | Train Acc: 98.589% (1956/1984)\n",
      "60 234 Train Loss: 0.044 | Train Acc: 98.642% (3851/3904)\n",
      "90 234 Train Loss: 0.043 | Train Acc: 98.661% (5746/5824)\n",
      "120 234 Train Loss: 0.044 | Train Acc: 98.605% (7636/7744)\n",
      "150 234 Train Loss: 0.045 | Train Acc: 98.551% (9524/9664)\n",
      "180 234 Train Loss: 0.044 | Train Acc: 98.610% (11423/11584)\n",
      "210 234 Train Loss: 0.045 | Train Acc: 98.593% (13314/13504)\n",
      "234 Epoch: 22 | Train Loss: 0.047 | Train Acc: 98.498% (14751/14976)\n",
      "0 234 Test Loss: 0.877 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.896 | Test Acc: 77.218% (1532/1984)\n",
      "60 234 Test Loss: 0.874 | Test Acc: 78.381% (3060/3904)\n",
      "90 234 Test Loss: 0.879 | Test Acc: 78.194% (4554/5824)\n",
      "120 234 Test Loss: 0.878 | Test Acc: 78.396% (6071/7744)\n",
      "150 234 Test Loss: 0.886 | Test Acc: 78.197% (7557/9664)\n",
      "180 234 Test Loss: 0.881 | Test Acc: 78.203% (9059/11584)\n",
      "210 234 Test Loss: 0.886 | Test Acc: 78.007% (10534/13504)\n",
      "234 Epoch: 22 | Test Loss: 0.881 | Test Acc: 78.065% (11691/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 23\n",
      "0 234 Train Loss: 0.009 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.052 | Train Acc: 98.085% (1946/1984)\n",
      "60 234 Train Loss: 0.057 | Train Acc: 98.105% (3830/3904)\n",
      "90 234 Train Loss: 0.053 | Train Acc: 98.386% (5730/5824)\n",
      "120 234 Train Loss: 0.050 | Train Acc: 98.463% (7625/7744)\n",
      "150 234 Train Loss: 0.051 | Train Acc: 98.489% (9518/9664)\n",
      "180 234 Train Loss: 0.049 | Train Acc: 98.558% (11417/11584)\n",
      "210 234 Train Loss: 0.047 | Train Acc: 98.637% (13320/13504)\n",
      "234 Epoch: 23 | Train Loss: 0.046 | Train Acc: 98.638% (14772/14976)\n",
      "0 234 Test Loss: 1.050 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 0.758 | Test Acc: 79.284% (1573/1984)\n",
      "60 234 Test Loss: 0.766 | Test Acc: 80.097% (3127/3904)\n",
      "90 234 Test Loss: 0.768 | Test Acc: 80.065% (4663/5824)\n",
      "120 234 Test Loss: 0.756 | Test Acc: 80.411% (6227/7744)\n",
      "150 234 Test Loss: 0.763 | Test Acc: 80.464% (7776/9664)\n",
      "180 234 Test Loss: 0.772 | Test Acc: 80.378% (9311/11584)\n",
      "210 234 Test Loss: 0.771 | Test Acc: 80.147% (10823/13504)\n",
      "234 Epoch: 23 | Test Loss: 0.766 | Test Acc: 80.295% (12025/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 24\n",
      "0 234 Train Loss: 0.023 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.032 | Train Acc: 99.143% (1967/1984)\n",
      "60 234 Train Loss: 0.033 | Train Acc: 99.027% (3866/3904)\n",
      "90 234 Train Loss: 0.035 | Train Acc: 98.970% (5764/5824)\n",
      "120 234 Train Loss: 0.037 | Train Acc: 98.825% (7653/7744)\n",
      "150 234 Train Loss: 0.038 | Train Acc: 98.800% (9548/9664)\n",
      "180 234 Train Loss: 0.036 | Train Acc: 98.826% (11448/11584)\n",
      "210 234 Train Loss: 0.038 | Train Acc: 98.786% (13340/13504)\n",
      "234 Epoch: 24 | Train Loss: 0.039 | Train Acc: 98.751% (14789/14976)\n",
      "0 234 Test Loss: 1.124 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 0.911 | Test Acc: 77.571% (1539/1984)\n",
      "60 234 Test Loss: 0.868 | Test Acc: 78.330% (3058/3904)\n",
      "90 234 Test Loss: 0.855 | Test Acc: 78.795% (4589/5824)\n",
      "120 234 Test Loss: 0.855 | Test Acc: 78.964% (6115/7744)\n",
      "150 234 Test Loss: 0.852 | Test Acc: 79.118% (7646/9664)\n",
      "180 234 Test Loss: 0.841 | Test Acc: 79.394% (9197/11584)\n",
      "210 234 Test Loss: 0.827 | Test Acc: 79.562% (10744/13504)\n",
      "234 Epoch: 24 | Test Loss: 0.829 | Test Acc: 79.434% (11896/14976)\n",
      "\n",
      "Epoch: 25\n",
      "0 234 Train Loss: 0.041 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.033 | Train Acc: 99.042% (1965/1984)\n",
      "60 234 Train Loss: 0.031 | Train Acc: 99.001% (3865/3904)\n",
      "90 234 Train Loss: 0.034 | Train Acc: 99.056% (5769/5824)\n",
      "120 234 Train Loss: 0.036 | Train Acc: 98.993% (7666/7744)\n",
      "150 234 Train Loss: 0.035 | Train Acc: 98.986% (9566/9664)\n",
      "180 234 Train Loss: 0.037 | Train Acc: 98.930% (11460/11584)\n",
      "210 234 Train Loss: 0.038 | Train Acc: 98.874% (13352/13504)\n",
      "234 Epoch: 25 | Train Loss: 0.038 | Train Acc: 98.898% (14811/14976)\n",
      "0 234 Test Loss: 0.981 | Test Acc: 82.812% (53/64)\n",
      "30 234 Test Loss: 0.898 | Test Acc: 77.671% (1541/1984)\n",
      "60 234 Test Loss: 0.883 | Test Acc: 77.894% (3041/3904)\n",
      "90 234 Test Loss: 0.891 | Test Acc: 77.679% (4524/5824)\n",
      "120 234 Test Loss: 0.900 | Test Acc: 77.738% (6020/7744)\n",
      "150 234 Test Loss: 0.902 | Test Acc: 77.628% (7502/9664)\n",
      "180 234 Test Loss: 0.903 | Test Acc: 77.685% (8999/11584)\n",
      "210 234 Test Loss: 0.897 | Test Acc: 77.836% (10511/13504)\n",
      "234 Epoch: 25 | Test Loss: 0.892 | Test Acc: 77.865% (11661/14976)\n",
      "\n",
      "Epoch: 26\n",
      "0 234 Train Loss: 0.026 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.040 | Train Acc: 98.538% (1955/1984)\n",
      "60 234 Train Loss: 0.046 | Train Acc: 98.361% (3840/3904)\n",
      "90 234 Train Loss: 0.044 | Train Acc: 98.472% (5735/5824)\n",
      "120 234 Train Loss: 0.043 | Train Acc: 98.463% (7625/7744)\n",
      "150 234 Train Loss: 0.043 | Train Acc: 98.551% (9524/9664)\n",
      "180 234 Train Loss: 0.041 | Train Acc: 98.653% (11428/11584)\n",
      "210 234 Train Loss: 0.039 | Train Acc: 98.726% (13332/13504)\n",
      "234 Epoch: 26 | Train Loss: 0.039 | Train Acc: 98.725% (14785/14976)\n",
      "0 234 Test Loss: 0.651 | Test Acc: 84.375% (54/64)\n",
      "30 234 Test Loss: 0.814 | Test Acc: 79.788% (1583/1984)\n",
      "60 234 Test Loss: 0.815 | Test Acc: 79.764% (3114/3904)\n",
      "90 234 Test Loss: 0.824 | Test Acc: 79.447% (4627/5824)\n",
      "120 234 Test Loss: 0.841 | Test Acc: 79.300% (6141/7744)\n",
      "150 234 Test Loss: 0.838 | Test Acc: 79.460% (7679/9664)\n",
      "180 234 Test Loss: 0.842 | Test Acc: 79.446% (9203/11584)\n",
      "210 234 Test Loss: 0.844 | Test Acc: 79.332% (10713/13504)\n",
      "234 Epoch: 26 | Test Loss: 0.835 | Test Acc: 79.447% (11898/14976)\n",
      "\n",
      "Epoch: 27\n",
      "0 234 Train Loss: 0.022 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.033 | Train Acc: 98.992% (1964/1984)\n",
      "60 234 Train Loss: 0.030 | Train Acc: 99.155% (3871/3904)\n",
      "90 234 Train Loss: 0.029 | Train Acc: 99.090% (5771/5824)\n",
      "120 234 Train Loss: 0.029 | Train Acc: 99.083% (7673/7744)\n",
      "150 234 Train Loss: 0.033 | Train Acc: 98.945% (9562/9664)\n",
      "180 234 Train Loss: 0.037 | Train Acc: 98.774% (11442/11584)\n",
      "210 234 Train Loss: 0.039 | Train Acc: 98.689% (13327/13504)\n",
      "234 Epoch: 27 | Train Loss: 0.039 | Train Acc: 98.671% (14777/14976)\n",
      "0 234 Test Loss: 0.727 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 0.769 | Test Acc: 80.645% (1600/1984)\n",
      "60 234 Test Loss: 0.766 | Test Acc: 80.507% (3143/3904)\n",
      "90 234 Test Loss: 0.786 | Test Acc: 79.876% (4652/5824)\n",
      "120 234 Test Loss: 0.784 | Test Acc: 79.946% (6191/7744)\n",
      "150 234 Test Loss: 0.789 | Test Acc: 79.884% (7720/9664)\n",
      "180 234 Test Loss: 0.790 | Test Acc: 79.852% (9250/11584)\n",
      "210 234 Test Loss: 0.792 | Test Acc: 79.791% (10775/13504)\n",
      "234 Epoch: 27 | Test Loss: 0.791 | Test Acc: 79.714% (11938/14976)\n",
      "\n",
      "Epoch: 28\n",
      "0 234 Train Loss: 0.035 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.029 | Train Acc: 98.891% (1962/1984)\n",
      "60 234 Train Loss: 0.029 | Train Acc: 99.078% (3868/3904)\n",
      "90 234 Train Loss: 0.029 | Train Acc: 99.107% (5772/5824)\n",
      "120 234 Train Loss: 0.030 | Train Acc: 99.032% (7669/7744)\n",
      "150 234 Train Loss: 0.031 | Train Acc: 99.017% (9569/9664)\n",
      "180 234 Train Loss: 0.035 | Train Acc: 98.852% (11451/11584)\n",
      "210 234 Train Loss: 0.037 | Train Acc: 98.800% (13342/13504)\n",
      "234 Epoch: 28 | Train Loss: 0.038 | Train Acc: 98.758% (14790/14976)\n",
      "0 234 Test Loss: 1.077 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 0.934 | Test Acc: 77.268% (1533/1984)\n",
      "60 234 Test Loss: 0.899 | Test Acc: 77.177% (3013/3904)\n",
      "90 234 Test Loss: 0.903 | Test Acc: 77.370% (4506/5824)\n",
      "120 234 Test Loss: 0.926 | Test Acc: 77.182% (5977/7744)\n",
      "150 234 Test Loss: 0.932 | Test Acc: 77.163% (7457/9664)\n",
      "180 234 Test Loss: 0.921 | Test Acc: 77.417% (8968/11584)\n",
      "210 234 Test Loss: 0.913 | Test Acc: 77.614% (10481/13504)\n",
      "234 Epoch: 28 | Test Loss: 0.916 | Test Acc: 77.597% (11621/14976)\n",
      "\n",
      "Epoch: 29\n",
      "0 234 Train Loss: 0.065 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.048 | Train Acc: 98.034% (1945/1984)\n",
      "60 234 Train Loss: 0.053 | Train Acc: 98.181% (3833/3904)\n",
      "90 234 Train Loss: 0.055 | Train Acc: 98.180% (5718/5824)\n",
      "120 234 Train Loss: 0.055 | Train Acc: 98.270% (7610/7744)\n",
      "150 234 Train Loss: 0.054 | Train Acc: 98.282% (9498/9664)\n",
      "180 234 Train Loss: 0.053 | Train Acc: 98.299% (11387/11584)\n",
      "210 234 Train Loss: 0.054 | Train Acc: 98.304% (13275/13504)\n",
      "234 Epoch: 29 | Train Loss: 0.052 | Train Acc: 98.351% (14729/14976)\n",
      "0 234 Test Loss: 0.659 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.727 | Test Acc: 79.940% (1586/1984)\n",
      "60 234 Test Loss: 0.787 | Test Acc: 79.098% (3088/3904)\n",
      "90 234 Test Loss: 0.772 | Test Acc: 79.584% (4635/5824)\n",
      "120 234 Test Loss: 0.766 | Test Acc: 79.830% (6182/7744)\n",
      "150 234 Test Loss: 0.769 | Test Acc: 79.946% (7726/9664)\n",
      "180 234 Test Loss: 0.759 | Test Acc: 80.110% (9280/11584)\n",
      "210 234 Test Loss: 0.748 | Test Acc: 80.287% (10842/13504)\n",
      "234 Epoch: 29 | Test Loss: 0.752 | Test Acc: 80.155% (12004/14976)\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.4.3: Start Target model training\n",
    "\n",
    "max_epoch = 30  #@param {type:\"integer\"}\n",
    "train_result_summary = 'target_train_DenseNet121-BiLSTM.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(target_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(target_testloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTHXUXnznrIt",
    "outputId": "72da6042-7eda-42a9-9c25-a422711c6b0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_training_summary(filepath = 'target_train_DenseNet121-BiLSTM.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zcVRPbYk-d2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBB4QM-JnrIt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
