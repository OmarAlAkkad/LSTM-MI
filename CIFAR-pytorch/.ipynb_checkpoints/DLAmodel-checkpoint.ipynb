{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "664eea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 1: Define required functions for Data processing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "import os\n",
    "import argparse\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]\n",
    "\n",
    "def balance_val_split(dataset, train_size=12500):\n",
    "\n",
    "    try:\n",
    "        targets = np.array(dataset.targets)\n",
    "    except:\n",
    "        targets = []  # create an empty list to store the targets\n",
    "        for data in dataset.datasets:\n",
    "            targets += data.targets  # concatenate the targets from each dataset into the list\n",
    "        targets = np.array(targets)\n",
    "    #targets = np.array(dataset.datasets.targets)\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(targets.shape[0]),\n",
    "        train_size=train_size,\n",
    "        stratify=targets\n",
    "    )\n",
    "    train_dataset = Subset(dataset, indices=train_indices)\n",
    "    # Get the data from the subset dataset\n",
    "    subset_data = [train_dataset[idx][0] for idx in range(len(train_dataset))]\n",
    "    subset_labels = [train_dataset[idx][1] for idx in range(len(train_dataset))]\n",
    "    # Create a dataset from the list of data and targets\n",
    "    train_dataset = MyDataset(subset_data, subset_labels)\n",
    "    \n",
    "    \n",
    "    val_dataset = Subset(dataset, indices=val_indices)\n",
    "    # Get the data from the subset dataset\n",
    "    subset_data = [val_dataset[idx][0] for idx in range(len(val_dataset))]\n",
    "    subset_labels = [val_dataset[idx][1] for idx in range(len(val_dataset))]\n",
    "    # Create a dataset from the list of data and targets\n",
    "    val_dataset = MyDataset(subset_data, subset_labels)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def count_label_frequency(target_train_dataset):\n",
    "\tfrom collections import Counter\n",
    "\ttarget_labels = []  # create an empty list to store the labels\n",
    "\n",
    "\tfor i in range(len(target_train_dataset)):\n",
    "\t\t\t_, label = target_train_dataset[i]  # extract the label for the i-th example in the subset\n",
    "\t\t\ttarget_labels.append(label)  # append the label to the 'subset_labels' list\n",
    "\n",
    "\n",
    "\treturn Counter(target_labels)\n",
    " \n",
    "\n",
    "\n",
    "def custom_transform(image: Tensor) -> Tensor:\n",
    "    import random\n",
    "    # randomly flip horizontally or vertically with 25% chance\n",
    "    if random.random() < 0.25:\n",
    "        image = RandomHorizontalFlip(p=1)(image)\n",
    "    elif random.random() < 0.5:\n",
    "        image = RandomVerticalFlip(p=1)(image)\n",
    "    \n",
    "    # randomly shift the image by 2 pixels to the left or right with 25% chance\n",
    "    if random.random() < 0.25:\n",
    "        image = RandomCrop((image.shape[-2], image.shape[-1] - 2), pad_if_needed=False)(image)\n",
    "    elif random.random() < 0.5:\n",
    "        image = RandomCrop((image.shape[-2], image.shape[-1] + 2), pad_if_needed=False)(image)\n",
    "        \n",
    "    # randomly shift the image by 2 pixels to the top or bottom with 25% chance\n",
    "    if random.random() < 0.25:\n",
    "        image = RandomCrop((image.shape[-2] - 2, image.shape[-1]), pad_if_needed=False)(image)\n",
    "    elif random.random() < 0.5:\n",
    "        image = RandomCrop((image.shape[-2] + 2, image.shape[-1]), pad_if_needed=False)(image)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c225eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 2: Define required functions for Data Training\n",
    "\n",
    "# Training\n",
    "def train(trainloader, epoch, batch_size=128, logfile = \"train.summary\"):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        #inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        if inputs.shape[0] != batch_size:\n",
    "          print(inputs.shape)\n",
    "          continue\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if batch_idx % 30 == 0:\n",
    "                print(batch_idx, len(trainloader), 'Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    print(len(trainloader), 'Epoch: %d | Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                     % (epoch, train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    f = open(logfile, \"a\")\n",
    "    f.write('Epoch: %d | Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)\\n'\n",
    "                     % (epoch, train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    f.close()\n",
    "\n",
    "def test(testloader, epoch, batch_size=128, logfile = \"train.summary\", save_modelpath = './DLA'):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            #inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            if batch_idx % 30 == 0:\n",
    "                print(batch_idx, len(testloader), 'Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    print(len(testloader), 'Epoch: %d | Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
    "                         % (epoch, test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    f = open(logfile, \"a\")\n",
    "    f.write('Epoch: %d | Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)\\n'\n",
    "                         % (epoch, test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    f.close()\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir(save_modelpath):\n",
    "            os.mkdir(save_modelpath)\n",
    "        torch.save(state, save_modelpath+'/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "def draw_training_summary(filepath = 'target_train_DCA-BiLSTM.summary'):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        results_summary = f.read()\n",
    "\n",
    "    train_epoch = []\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_epoch = []\n",
    "    test_loss=[]\n",
    "    test_acc=[]\n",
    "    for line in results_summary.split(\"\\n\"):\n",
    "        try:\n",
    "            r_epoch = line.split('|')[0].strip().split(' ')[1]\n",
    "            r_loss = line.split('|')[1].strip().split(' ')[2].replace('%','')\n",
    "            r_acc = line.split('|')[2].strip().split(' ')[2].replace('%','')\n",
    "            if 'Train' in line:\n",
    "                train_epoch.append(int(r_epoch))\n",
    "                train_loss.append(float(r_loss))\n",
    "                train_acc.append(float(r_acc))\n",
    "            if 'Test' in line:\n",
    "                test_epoch.append(int(r_epoch))\n",
    "                test_loss.append(float(r_loss))\n",
    "                test_acc.append(float(r_acc))\n",
    "        except:\n",
    "            print(line)\n",
    "\n",
    "    # Create a new figure and plot the data\n",
    "    plt.figure(figsize=(12,4))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_acc, label='Train')\n",
    "    plt.plot(test_acc, label='Test')\n",
    "    plt.axhline(y=np.max(test_acc), color='r', linestyle='--')\n",
    "    # Add text for the horizontal line\n",
    "    plt.text(test_epoch[-10], np.max(test_acc)*1.05, np.max(test_acc), color='r', fontsize=10)\n",
    "    # Customize the plot\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_loss, label='Train')\n",
    "    plt.plot(test_loss, label='Test')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2804c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 3: Prepare Cifar10 dataset for target and shadow model\n",
    "import pickle\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def create_cifar_dataset_torch(load_data = False, batch_size=128, target_train_size = 15000, target_test_size= 15000, shadow_train_size = 15000, shadow_test_size= 15000):\n",
    "\n",
    "  # Data\n",
    "  print('==> Preparing data..')\n",
    "  if not load_data:\n",
    "\n",
    "      transform = transforms.Compose([\n",
    "          transforms.ToTensor()\n",
    "      ])\n",
    "\n",
    "      cifar_trainset = torchvision.datasets.CIFAR10(\n",
    "          root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "      cifar_testset = torchvision.datasets.CIFAR10(\n",
    "          root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "      cifar_dataset = torch.utils.data.ConcatDataset([cifar_trainset, cifar_testset])\n",
    "\n",
    "\n",
    "      #target_train_size = int(0.25 * len(cifar_dataset)) # 15000\n",
    "      remain_size = len(cifar_dataset) - target_train_size\n",
    "      target_train_dataset, remain_dataset = torch.utils.data.random_split(cifar_dataset, [target_train_size, remain_size])\n",
    "\n",
    "      #target_test_size = int(0.25 * len(cifar_dataset)) # 15000\n",
    "      remain_size = len(remain_dataset) - target_test_size\n",
    "      target_test_dataset, remain_dataset = torch.utils.data.random_split(remain_dataset, [target_test_size, remain_size])\n",
    "\n",
    "      #target_test_dataset, remain_dataset = balance_val_split(remain_dataset, train_size=target_test_size)\n",
    "\n",
    "\n",
    "      #shadow_train_size = int(0.25 * len(cifar_dataset)) # 15000\n",
    "      remain_size = len(remain_dataset) - shadow_train_size\n",
    "      shadow_train_dataset, shadow_test_dataset = torch.utils.data.random_split(remain_dataset, [shadow_train_size, remain_size])\n",
    "      #shadow_train_dataset, shadow_test_dataset = balance_val_split(remain_dataset, train_size=shadow_train_size)\n",
    "\n",
    "      print(\"Setting target_train_dataset size to \",len(target_train_dataset), count_label_frequency(target_train_dataset))\n",
    "      print(\"Setting target_test_dataset size to \",len(target_test_dataset), count_label_frequency(target_test_dataset))\n",
    "      print(\"Setting shadow_train_dataset size to \",len(shadow_train_dataset), count_label_frequency(shadow_train_dataset))\n",
    "      print(\"Setting shadow_test_dataset size to \",len(shadow_test_dataset), count_label_frequency(shadow_test_dataset))\n",
    "      #print(\"Setting testset size to \",len(testset))\n",
    "\n",
    "\n",
    "\n",
    "      '''\n",
    "      transform_train = transforms.Compose([\n",
    "          transforms.RandomCrop(32, padding=4),\n",
    "          transforms.RandomHorizontalFlip(),\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "      ])\n",
    "      '''\n",
    "\n",
    "\n",
    "\n",
    "      transform_train = transforms.Compose([\n",
    "          transforms.RandomCrop(32, padding=4),\n",
    "          custom_transform,\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "      ])\n",
    "\n",
    "      transform_test = transforms.Compose([\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "      ])\n",
    "\n",
    "      # apply the data augmentation transformations to the subset\n",
    "      target_train_dataset.dataset.transform = transform_train\n",
    "      # Load the transformed subset using a DataLoader\n",
    "      target_trainloader = DataLoader(target_train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "\n",
    "      target_test_dataset.dataset.transform = transform_test\n",
    "      # Load the transformed subset using a DataLoader\n",
    "      target_testloader = DataLoader(target_test_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "\n",
    "      # apply the data augmentation transformations to the subset\n",
    "      shadow_train_dataset.dataset.transform = transform_train\n",
    "      # Load the transformed subset using a DataLoader\n",
    "      shadow_trainloader = DataLoader(shadow_train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "\n",
    "      shadow_test_dataset.dataset.transform = transform_test\n",
    "      # Load the transformed subset using a DataLoader\n",
    "      shadow_testloader = DataLoader(shadow_test_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "      pickle.dump(target_trainloader, open('target_trainloader_DLA.p', 'wb'))\n",
    "      pickle.dump(target_testloader, open('target_testloader_DLA.p', 'wb'))\n",
    "      pickle.dump(shadow_trainloader, open('shadow_trainloader_DLA.p', 'wb'))\n",
    "      pickle.dump(shadow_testloader, open('shadow_test_dataset_DLA.p', 'wb'))\n",
    "    \n",
    "  data_file = open('target_trainloader_DLA.p', 'rb')\n",
    "  target_trainloader = pickle.load(data_file)\n",
    "  data_file.close()\n",
    "  data_file = open('target_testloader_DLA.p', 'rb')\n",
    "  target_testloader = pickle.load(data_file)\n",
    "  data_file.close()\n",
    "  data_file = open('shadow_trainloader_DLA.p', 'rb')\n",
    "  shadow_trainloader = pickle.load(data_file)\n",
    "  data_file.close()\n",
    "  data_file = open('shadow_test_dataset_DLA.p', 'rb')\n",
    "  shadow_testloader = pickle.load(data_file)\n",
    "  data_file.close()\n",
    "\n",
    "\n",
    "  return target_trainloader, target_testloader, shadow_trainloader, shadow_testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d6e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 4.2: Define required functions for DLA & DLA+RNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class Root(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1):\n",
    "        super(Root, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride=1, padding=(kernel_size - 1) // 2, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = torch.cat(xs, 1)\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Tree(nn.Module):\n",
    "    def __init__(self, block, in_channels, out_channels, level=1, stride=1):\n",
    "        super(Tree, self).__init__()\n",
    "        self.level = level\n",
    "        if level == 1:\n",
    "            self.root = Root(2*out_channels, out_channels)\n",
    "            self.left_node = block(in_channels, out_channels, stride=stride)\n",
    "            self.right_node = block(out_channels, out_channels, stride=1)\n",
    "        else:\n",
    "            self.root = Root((level+2)*out_channels, out_channels)\n",
    "            for i in reversed(range(1, level)):\n",
    "                subtree = Tree(block, in_channels, out_channels,\n",
    "                               level=i, stride=stride)\n",
    "                self.__setattr__('level_%d' % i, subtree)\n",
    "            self.prev_root = block(in_channels, out_channels, stride=stride)\n",
    "            self.left_node = block(out_channels, out_channels, stride=1)\n",
    "            self.right_node = block(out_channels, out_channels, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = [self.prev_root(x)] if self.level > 1 else []\n",
    "        for i in reversed(range(1, self.level)):\n",
    "            level_i = self.__getattr__('level_%d' % i)\n",
    "            x = level_i(x)\n",
    "            xs.append(x)\n",
    "        x = self.left_node(x)\n",
    "        xs.append(x)\n",
    "        x = self.right_node(x)\n",
    "        xs.append(x)\n",
    "        out = self.root(xs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DLA(nn.Module):\n",
    "    def __init__(self, block=BasicBlock, num_classes=10, enable_RNN='LSTM'):\n",
    "        super(DLA, self).__init__()\n",
    "        \n",
    "        self.enable_RNN = enable_RNN\n",
    "        if enable_RNN not in ['None', 'LSTM', 'Bi-LSTM']:\n",
    "          raise Exception(\"enable_RNN only supports one of ['None', 'LSTM', 'Bi-LSTM']\")\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer3 = Tree(block,  32,  64, level=1, stride=1)\n",
    "        self.layer4 = Tree(block,  64, 128, level=2, stride=2)\n",
    "        self.layer5 = Tree(block, 128, 256, level=2, stride=2)\n",
    "        self.layer6 = Tree(block, 256, 512, level=1, stride=2)\n",
    "        self.linear = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        if enable_RNN == 'None':\n",
    "          self.linear = nn.Linear(512, num_classes)\n",
    "        elif enable_RNN == 'LSTM':\n",
    "          self.rnn = nn.LSTM(512, 1024, dropout = 0.6)\n",
    "          self.linear = nn.Linear(1024, num_classes)\n",
    "        elif enable_RNN == 'Bi-LSTM':\n",
    "          self.rnn = nn.LSTM(512, 2048, 1, dropout = 0.3, batch_first=True, bidirectional=True)\n",
    "          self.linear = nn.Linear(4096, num_classes)\n",
    "        else:\n",
    "          raise Exception(\"enable_RNN only supports one of ['None', 'LSTM', 'Bi-LSTM']\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "\n",
    "        if self.enable_RNN == 'None':\n",
    "          out = out.view(out.size(0), -1)\n",
    "          out = self.linear(out)\n",
    "        else:\n",
    "          # add LSTM\n",
    "          #print(out.shape)\n",
    "          out = out.view(out.size(0), 1,  -1)\n",
    "          out,_ = self.rnn(out)\n",
    "          out = out.view(out.size(0), -1)\n",
    "          out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd64b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model for DLA-BiLSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omars\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training model from scratch..\n",
      "Total trained parameters:  58303034\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DLA-BiLSTM'  #@param {type:\"string\"}\n",
    "save_model_folder = './Target-DLA-BiLSTM_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DLA-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a02bb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Setting target_train_dataset size to  15000 Counter({5: 1552, 4: 1509, 6: 1508, 9: 1506, 2: 1501, 7: 1500, 8: 1495, 1: 1493, 0: 1469, 3: 1467})\n",
      "Setting target_test_dataset size to  15000 Counter({9: 1557, 8: 1533, 1: 1527, 5: 1512, 3: 1504, 0: 1494, 7: 1493, 6: 1479, 4: 1462, 2: 1439})\n",
      "Setting shadow_train_dataset size to  15000 Counter({2: 1564, 0: 1528, 4: 1524, 8: 1517, 6: 1511, 7: 1491, 5: 1485, 9: 1479, 1: 1464, 3: 1437})\n",
      "Setting shadow_test_dataset size to  15000 Counter({3: 1592, 1: 1516, 7: 1516, 0: 1509, 4: 1505, 6: 1502, 2: 1496, 9: 1458, 8: 1455, 5: 1451})\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.1.2: Setup Target and Shadow datasets for ReNet Training\n",
    "\n",
    "target_train_size = 15000 #@param {type:\"integer\"}\n",
    "target_test_size= 15000 #@param {type:\"integer\"}\n",
    "shadow_train_size = 15000  #@param {type:\"integer\"} \n",
    "shadow_test_size= 15000 #@param {type:\"integer\"}\n",
    "\n",
    "# create dataset for renet \n",
    "print('==> Preparing dataset..')\n",
    "target_trainloader, target_testloader, shadow_trainloader, shadow_testloader = create_cifar_dataset_torch(load_data=True,batch_size=batch_size, target_train_size = target_train_size, target_test_size= target_test_size, shadow_train_size = shadow_train_size, shadow_test_size= shadow_test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f363b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 234 Train Loss: 2.299 | Train Acc: 18.750% (12/64)\n",
      "30 234 Train Loss: 2.299 | Train Acc: 13.357% (265/1984)\n",
      "60 234 Train Loss: 2.278 | Train Acc: 14.472% (565/3904)\n",
      "90 234 Train Loss: 2.221 | Train Acc: 16.844% (981/5824)\n",
      "120 234 Train Loss: 2.154 | Train Acc: 18.363% (1422/7744)\n",
      "150 234 Train Loss: 2.093 | Train Acc: 19.743% (1908/9664)\n",
      "180 234 Train Loss: 2.044 | Train Acc: 21.107% (2445/11584)\n",
      "210 234 Train Loss: 2.010 | Train Acc: 21.986% (2969/13504)\n",
      "234 Epoch: 0 | Train Loss: 1.985 | Train Acc: 22.857% (3423/14976)\n",
      "0 234 Test Loss: 1.783 | Test Acc: 39.062% (25/64)\n",
      "30 234 Test Loss: 1.875 | Test Acc: 27.167% (539/1984)\n",
      "60 234 Test Loss: 1.870 | Test Acc: 27.049% (1056/3904)\n",
      "90 234 Test Loss: 1.870 | Test Acc: 26.219% (1527/5824)\n",
      "120 234 Test Loss: 1.864 | Test Acc: 26.498% (2052/7744)\n",
      "150 234 Test Loss: 1.866 | Test Acc: 26.521% (2563/9664)\n",
      "180 234 Test Loss: 1.868 | Test Acc: 26.364% (3054/11584)\n",
      "210 234 Test Loss: 1.867 | Test Acc: 26.163% (3533/13504)\n",
      "234 Epoch: 0 | Test Loss: 1.865 | Test Acc: 26.135% (3914/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "0 234 Train Loss: 1.779 | Train Acc: 25.000% (16/64)\n",
      "30 234 Train Loss: 1.720 | Train Acc: 31.905% (633/1984)\n",
      "60 234 Train Loss: 1.708 | Train Acc: 32.812% (1281/3904)\n",
      "90 234 Train Loss: 1.701 | Train Acc: 33.087% (1927/5824)\n",
      "120 234 Train Loss: 1.682 | Train Acc: 33.897% (2625/7744)\n",
      "150 234 Train Loss: 1.673 | Train Acc: 34.199% (3305/9664)\n",
      "180 234 Train Loss: 1.660 | Train Acc: 35.014% (4056/11584)\n",
      "210 234 Train Loss: 1.645 | Train Acc: 35.738% (4826/13504)\n",
      "234 Epoch: 1 | Train Loss: 1.638 | Train Acc: 36.198% (5421/14976)\n",
      "0 234 Test Loss: 1.606 | Test Acc: 45.312% (29/64)\n",
      "30 234 Test Loss: 1.548 | Test Acc: 40.675% (807/1984)\n",
      "60 234 Test Loss: 1.559 | Test Acc: 40.318% (1574/3904)\n",
      "90 234 Test Loss: 1.559 | Test Acc: 39.921% (2325/5824)\n",
      "120 234 Test Loss: 1.558 | Test Acc: 40.664% (3149/7744)\n",
      "150 234 Test Loss: 1.558 | Test Acc: 40.822% (3945/9664)\n",
      "180 234 Test Loss: 1.558 | Test Acc: 40.729% (4718/11584)\n",
      "210 234 Test Loss: 1.559 | Test Acc: 40.514% (5471/13504)\n",
      "234 Epoch: 1 | Test Loss: 1.557 | Test Acc: 40.618% (6083/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "0 234 Train Loss: 1.393 | Train Acc: 48.438% (31/64)\n",
      "30 234 Train Loss: 1.497 | Train Acc: 44.153% (876/1984)\n",
      "60 234 Train Loss: 1.488 | Train Acc: 44.749% (1747/3904)\n",
      "90 234 Train Loss: 1.453 | Train Acc: 46.343% (2699/5824)\n",
      "120 234 Train Loss: 1.431 | Train Acc: 46.927% (3634/7744)\n",
      "150 234 Train Loss: 1.419 | Train Acc: 47.185% (4560/9664)\n",
      "180 234 Train Loss: 1.403 | Train Acc: 48.006% (5561/11584)\n",
      "210 234 Train Loss: 1.389 | Train Acc: 48.541% (6555/13504)\n",
      "234 Epoch: 2 | Train Loss: 1.378 | Train Acc: 48.925% (7327/14976)\n",
      "0 234 Test Loss: 1.661 | Test Acc: 35.938% (23/64)\n",
      "30 234 Test Loss: 1.708 | Test Acc: 40.927% (812/1984)\n",
      "60 234 Test Loss: 1.738 | Test Acc: 41.445% (1618/3904)\n",
      "90 234 Test Loss: 1.761 | Test Acc: 40.745% (2373/5824)\n",
      "120 234 Test Loss: 1.765 | Test Acc: 40.444% (3132/7744)\n",
      "150 234 Test Loss: 1.754 | Test Acc: 40.780% (3941/9664)\n",
      "180 234 Test Loss: 1.744 | Test Acc: 40.737% (4719/11584)\n",
      "210 234 Test Loss: 1.742 | Test Acc: 40.721% (5499/13504)\n",
      "234 Epoch: 2 | Test Loss: 1.744 | Test Acc: 40.685% (6093/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "0 234 Train Loss: 1.074 | Train Acc: 60.938% (39/64)\n",
      "30 234 Train Loss: 1.188 | Train Acc: 56.653% (1124/1984)\n",
      "60 234 Train Loss: 1.186 | Train Acc: 56.660% (2212/3904)\n",
      "90 234 Train Loss: 1.179 | Train Acc: 56.920% (3315/5824)\n",
      "120 234 Train Loss: 1.168 | Train Acc: 57.670% (4466/7744)\n",
      "150 234 Train Loss: 1.159 | Train Acc: 57.906% (5596/9664)\n",
      "180 234 Train Loss: 1.152 | Train Acc: 58.166% (6738/11584)\n",
      "210 234 Train Loss: 1.147 | Train Acc: 58.375% (7883/13504)\n",
      "234 Epoch: 3 | Train Loss: 1.146 | Train Acc: 58.387% (8744/14976)\n",
      "0 234 Test Loss: 1.467 | Test Acc: 46.875% (30/64)\n",
      "30 234 Test Loss: 1.331 | Test Acc: 51.058% (1013/1984)\n",
      "60 234 Test Loss: 1.303 | Test Acc: 52.254% (2040/3904)\n",
      "90 234 Test Loss: 1.310 | Test Acc: 52.026% (3030/5824)\n",
      "120 234 Test Loss: 1.302 | Test Acc: 52.505% (4066/7744)\n",
      "150 234 Test Loss: 1.306 | Test Acc: 52.483% (5072/9664)\n",
      "180 234 Test Loss: 1.311 | Test Acc: 52.029% (6027/11584)\n",
      "210 234 Test Loss: 1.309 | Test Acc: 52.110% (7037/13504)\n",
      "234 Epoch: 3 | Test Loss: 1.311 | Test Acc: 52.143% (7809/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "0 234 Train Loss: 1.127 | Train Acc: 54.688% (35/64)\n",
      "30 234 Train Loss: 0.927 | Train Acc: 67.339% (1336/1984)\n",
      "60 234 Train Loss: 0.941 | Train Acc: 65.753% (2567/3904)\n",
      "90 234 Train Loss: 0.964 | Train Acc: 65.144% (3794/5824)\n",
      "120 234 Train Loss: 0.955 | Train Acc: 65.651% (5084/7744)\n",
      "150 234 Train Loss: 0.960 | Train Acc: 65.573% (6337/9664)\n",
      "180 234 Train Loss: 0.959 | Train Acc: 65.487% (7586/11584)\n",
      "210 234 Train Loss: 0.961 | Train Acc: 65.247% (8811/13504)\n",
      "234 Epoch: 4 | Train Loss: 0.962 | Train Acc: 65.311% (9781/14976)\n",
      "0 234 Test Loss: 1.253 | Test Acc: 54.688% (35/64)\n",
      "30 234 Test Loss: 1.428 | Test Acc: 49.899% (990/1984)\n",
      "60 234 Test Loss: 1.416 | Test Acc: 50.768% (1982/3904)\n",
      "90 234 Test Loss: 1.418 | Test Acc: 50.721% (2954/5824)\n",
      "120 234 Test Loss: 1.410 | Test Acc: 50.917% (3943/7744)\n",
      "150 234 Test Loss: 1.403 | Test Acc: 51.149% (4943/9664)\n",
      "180 234 Test Loss: 1.398 | Test Acc: 51.407% (5955/11584)\n",
      "210 234 Test Loss: 1.402 | Test Acc: 51.133% (6905/13504)\n",
      "234 Epoch: 4 | Test Loss: 1.399 | Test Acc: 51.322% (7686/14976)\n",
      "\n",
      "Epoch: 5\n",
      "0 234 Train Loss: 0.947 | Train Acc: 64.062% (41/64)\n",
      "30 234 Train Loss: 0.798 | Train Acc: 71.976% (1428/1984)\n",
      "60 234 Train Loss: 0.781 | Train Acc: 72.208% (2819/3904)\n",
      "90 234 Train Loss: 0.781 | Train Acc: 72.253% (4208/5824)\n",
      "120 234 Train Loss: 0.787 | Train Acc: 71.836% (5563/7744)\n",
      "150 234 Train Loss: 0.795 | Train Acc: 71.792% (6938/9664)\n",
      "180 234 Train Loss: 0.795 | Train Acc: 71.866% (8325/11584)\n",
      "210 234 Train Loss: 0.799 | Train Acc: 71.557% (9663/13504)\n",
      "234 Epoch: 5 | Train Loss: 0.800 | Train Acc: 71.595% (10722/14976)\n",
      "0 234 Test Loss: 0.962 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.008 | Test Acc: 64.365% (1277/1984)\n",
      "60 234 Test Loss: 1.014 | Test Acc: 64.216% (2507/3904)\n",
      "90 234 Test Loss: 1.011 | Test Acc: 64.011% (3728/5824)\n",
      "120 234 Test Loss: 1.020 | Test Acc: 63.856% (4945/7744)\n",
      "150 234 Test Loss: 1.017 | Test Acc: 63.938% (6179/9664)\n",
      "180 234 Test Loss: 1.018 | Test Acc: 63.769% (7387/11584)\n",
      "210 234 Test Loss: 1.014 | Test Acc: 63.937% (8634/13504)\n",
      "234 Epoch: 5 | Test Loss: 1.013 | Test Acc: 63.956% (9578/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      "0 234 Train Loss: 0.484 | Train Acc: 90.625% (58/64)\n",
      "30 234 Train Loss: 0.639 | Train Acc: 77.571% (1539/1984)\n",
      "60 234 Train Loss: 0.621 | Train Acc: 77.792% (3037/3904)\n",
      "90 234 Train Loss: 0.622 | Train Acc: 77.747% (4528/5824)\n",
      "120 234 Train Loss: 0.623 | Train Acc: 77.854% (6029/7744)\n",
      "150 234 Train Loss: 0.629 | Train Acc: 77.546% (7494/9664)\n",
      "180 234 Train Loss: 0.637 | Train Acc: 77.219% (8945/11584)\n",
      "210 234 Train Loss: 0.640 | Train Acc: 77.236% (10430/13504)\n",
      "234 Epoch: 6 | Train Loss: 0.645 | Train Acc: 77.110% (11548/14976)\n",
      "0 234 Test Loss: 1.756 | Test Acc: 50.000% (32/64)\n",
      "30 234 Test Loss: 1.436 | Test Acc: 56.653% (1124/1984)\n",
      "60 234 Test Loss: 1.512 | Test Acc: 54.944% (2145/3904)\n",
      "90 234 Test Loss: 1.505 | Test Acc: 54.876% (3196/5824)\n",
      "120 234 Test Loss: 1.499 | Test Acc: 54.713% (4237/7744)\n",
      "150 234 Test Loss: 1.487 | Test Acc: 55.567% (5370/9664)\n",
      "180 234 Test Loss: 1.491 | Test Acc: 55.663% (6448/11584)\n",
      "210 234 Test Loss: 1.484 | Test Acc: 55.717% (7524/13504)\n",
      "234 Epoch: 6 | Test Loss: 1.488 | Test Acc: 55.682% (8339/14976)\n",
      "\n",
      "Epoch: 7\n",
      "0 234 Train Loss: 0.498 | Train Acc: 82.812% (53/64)\n",
      "30 234 Train Loss: 0.486 | Train Acc: 83.720% (1661/1984)\n",
      "60 234 Train Loss: 0.465 | Train Acc: 84.273% (3290/3904)\n",
      "90 234 Train Loss: 0.482 | Train Acc: 83.345% (4854/5824)\n",
      "120 234 Train Loss: 0.495 | Train Acc: 83.187% (6442/7744)\n",
      "150 234 Train Loss: 0.506 | Train Acc: 82.750% (7997/9664)\n",
      "180 234 Train Loss: 0.509 | Train Acc: 82.588% (9567/11584)\n",
      "210 234 Train Loss: 0.522 | Train Acc: 81.991% (11072/13504)\n",
      "234 Epoch: 7 | Train Loss: 0.523 | Train Acc: 81.891% (12264/14976)\n",
      "0 234 Test Loss: 1.193 | Test Acc: 62.500% (40/64)\n",
      "30 234 Test Loss: 1.272 | Test Acc: 60.585% (1202/1984)\n",
      "60 234 Test Loss: 1.248 | Test Acc: 60.912% (2378/3904)\n",
      "90 234 Test Loss: 1.241 | Test Acc: 61.178% (3563/5824)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 234 Test Loss: 1.237 | Test Acc: 61.635% (4773/7744)\n",
      "150 234 Test Loss: 1.229 | Test Acc: 62.014% (5993/9664)\n",
      "180 234 Test Loss: 1.233 | Test Acc: 62.086% (7192/11584)\n",
      "210 234 Test Loss: 1.235 | Test Acc: 62.152% (8393/13504)\n",
      "234 Epoch: 7 | Test Loss: 1.242 | Test Acc: 62.039% (9291/14976)\n",
      "\n",
      "Epoch: 8\n",
      "0 234 Train Loss: 0.350 | Train Acc: 89.062% (57/64)\n",
      "30 234 Train Loss: 0.391 | Train Acc: 86.895% (1724/1984)\n",
      "60 234 Train Loss: 0.367 | Train Acc: 88.089% (3439/3904)\n",
      "90 234 Train Loss: 0.369 | Train Acc: 87.792% (5113/5824)\n",
      "120 234 Train Loss: 0.369 | Train Acc: 87.565% (6781/7744)\n",
      "150 234 Train Loss: 0.381 | Train Acc: 87.148% (8422/9664)\n",
      "180 234 Train Loss: 0.391 | Train Acc: 86.689% (10042/11584)\n",
      "210 234 Train Loss: 0.405 | Train Acc: 86.130% (11631/13504)\n",
      "234 Epoch: 8 | Train Loss: 0.415 | Train Acc: 85.617% (12822/14976)\n",
      "0 234 Test Loss: 0.858 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 0.922 | Test Acc: 70.111% (1391/1984)\n",
      "60 234 Test Loss: 0.892 | Test Acc: 70.902% (2768/3904)\n",
      "90 234 Test Loss: 0.868 | Test Acc: 71.308% (4153/5824)\n",
      "120 234 Test Loss: 0.873 | Test Acc: 71.139% (5509/7744)\n",
      "150 234 Test Loss: 0.893 | Test Acc: 70.613% (6824/9664)\n",
      "180 234 Test Loss: 0.888 | Test Acc: 70.779% (8199/11584)\n",
      "210 234 Test Loss: 0.884 | Test Acc: 70.698% (9547/13504)\n",
      "234 Epoch: 8 | Test Loss: 0.887 | Test Acc: 70.606% (10574/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      "0 234 Train Loss: 0.243 | Train Acc: 92.188% (59/64)\n",
      "30 234 Train Loss: 0.240 | Train Acc: 92.188% (1829/1984)\n",
      "60 234 Train Loss: 0.241 | Train Acc: 91.880% (3587/3904)\n",
      "90 234 Train Loss: 0.245 | Train Acc: 91.758% (5344/5824)\n",
      "120 234 Train Loss: 0.265 | Train Acc: 91.038% (7050/7744)\n",
      "150 234 Train Loss: 0.284 | Train Acc: 90.346% (8731/9664)\n",
      "180 234 Train Loss: 0.296 | Train Acc: 89.952% (10420/11584)\n",
      "210 234 Train Loss: 0.312 | Train Acc: 89.455% (12080/13504)\n",
      "234 Epoch: 9 | Train Loss: 0.318 | Train Acc: 89.243% (13365/14976)\n",
      "0 234 Test Loss: 1.269 | Test Acc: 62.500% (40/64)\n",
      "30 234 Test Loss: 1.078 | Test Acc: 67.742% (1344/1984)\n",
      "60 234 Test Loss: 1.092 | Test Acc: 68.263% (2665/3904)\n",
      "90 234 Test Loss: 1.082 | Test Acc: 68.080% (3965/5824)\n",
      "120 234 Test Loss: 1.075 | Test Acc: 68.169% (5279/7744)\n",
      "150 234 Test Loss: 1.082 | Test Acc: 67.943% (6566/9664)\n",
      "180 234 Test Loss: 1.101 | Test Acc: 67.576% (7828/11584)\n",
      "210 234 Test Loss: 1.111 | Test Acc: 67.461% (9110/13504)\n",
      "234 Epoch: 9 | Test Loss: 1.110 | Test Acc: 67.521% (10112/14976)\n",
      "\n",
      "Epoch: 10\n",
      "0 234 Train Loss: 0.232 | Train Acc: 92.188% (59/64)\n",
      "30 234 Train Loss: 0.239 | Train Acc: 91.734% (1820/1984)\n",
      "60 234 Train Loss: 0.226 | Train Acc: 92.392% (3607/3904)\n",
      "90 234 Train Loss: 0.223 | Train Acc: 92.617% (5394/5824)\n",
      "120 234 Train Loss: 0.223 | Train Acc: 92.549% (7167/7744)\n",
      "150 234 Train Loss: 0.227 | Train Acc: 92.384% (8928/9664)\n",
      "180 234 Train Loss: 0.231 | Train Acc: 92.205% (10681/11584)\n",
      "210 234 Train Loss: 0.243 | Train Acc: 91.795% (12396/13504)\n",
      "234 Epoch: 10 | Train Loss: 0.250 | Train Acc: 91.520% (13706/14976)\n",
      "0 234 Test Loss: 1.001 | Test Acc: 71.875% (46/64)\n",
      "30 234 Test Loss: 1.203 | Test Acc: 66.734% (1324/1984)\n",
      "60 234 Test Loss: 1.167 | Test Acc: 67.341% (2629/3904)\n",
      "90 234 Test Loss: 1.185 | Test Acc: 66.775% (3889/5824)\n",
      "120 234 Test Loss: 1.193 | Test Acc: 66.581% (5156/7744)\n",
      "150 234 Test Loss: 1.196 | Test Acc: 66.380% (6415/9664)\n",
      "180 234 Test Loss: 1.183 | Test Acc: 66.713% (7728/11584)\n",
      "210 234 Test Loss: 1.199 | Test Acc: 66.491% (8979/13504)\n",
      "234 Epoch: 10 | Test Loss: 1.199 | Test Acc: 66.673% (9985/14976)\n",
      "\n",
      "Epoch: 11\n",
      "0 234 Train Loss: 0.097 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.164 | Train Acc: 94.506% (1875/1984)\n",
      "60 234 Train Loss: 0.152 | Train Acc: 95.005% (3709/3904)\n",
      "90 234 Train Loss: 0.151 | Train Acc: 95.072% (5537/5824)\n",
      "120 234 Train Loss: 0.159 | Train Acc: 94.861% (7346/7744)\n",
      "150 234 Train Loss: 0.165 | Train Acc: 94.650% (9147/9664)\n",
      "180 234 Train Loss: 0.170 | Train Acc: 94.467% (10943/11584)\n",
      "210 234 Train Loss: 0.179 | Train Acc: 94.083% (12705/13504)\n",
      "234 Epoch: 11 | Train Loss: 0.182 | Train Acc: 93.977% (14074/14976)\n",
      "0 234 Test Loss: 1.375 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.080 | Test Acc: 70.262% (1394/1984)\n",
      "60 234 Test Loss: 1.091 | Test Acc: 70.005% (2733/3904)\n",
      "90 234 Test Loss: 1.111 | Test Acc: 69.162% (4028/5824)\n",
      "120 234 Test Loss: 1.105 | Test Acc: 69.318% (5368/7744)\n",
      "150 234 Test Loss: 1.094 | Test Acc: 69.764% (6742/9664)\n",
      "180 234 Test Loss: 1.078 | Test Acc: 69.846% (8091/11584)\n",
      "210 234 Test Loss: 1.074 | Test Acc: 69.853% (9433/13504)\n",
      "234 Epoch: 11 | Test Loss: 1.073 | Test Acc: 69.852% (10461/14976)\n",
      "\n",
      "Epoch: 12\n",
      "0 234 Train Loss: 0.097 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.117 | Train Acc: 95.968% (1904/1984)\n",
      "60 234 Train Loss: 0.120 | Train Acc: 95.799% (3740/3904)\n",
      "90 234 Train Loss: 0.124 | Train Acc: 95.742% (5576/5824)\n",
      "120 234 Train Loss: 0.133 | Train Acc: 95.287% (7379/7744)\n",
      "150 234 Train Loss: 0.143 | Train Acc: 94.857% (9167/9664)\n",
      "180 234 Train Loss: 0.150 | Train Acc: 94.570% (10955/11584)\n",
      "210 234 Train Loss: 0.155 | Train Acc: 94.409% (12749/13504)\n",
      "234 Epoch: 12 | Train Loss: 0.158 | Train Acc: 94.378% (14134/14976)\n",
      "0 234 Test Loss: 1.034 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.010 | Test Acc: 71.371% (1416/1984)\n",
      "60 234 Test Loss: 1.012 | Test Acc: 71.414% (2788/3904)\n",
      "90 234 Test Loss: 1.039 | Test Acc: 71.257% (4150/5824)\n",
      "120 234 Test Loss: 1.038 | Test Acc: 71.268% (5519/7744)\n",
      "150 234 Test Loss: 1.029 | Test Acc: 71.078% (6869/9664)\n",
      "180 234 Test Loss: 1.023 | Test Acc: 71.331% (8263/11584)\n",
      "210 234 Test Loss: 1.022 | Test Acc: 71.527% (9659/13504)\n",
      "234 Epoch: 12 | Test Loss: 1.010 | Test Acc: 71.728% (10742/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 13\n",
      "0 234 Train Loss: 0.069 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.089 | Train Acc: 97.329% (1931/1984)\n",
      "60 234 Train Loss: 0.096 | Train Acc: 97.029% (3788/3904)\n",
      "90 234 Train Loss: 0.095 | Train Acc: 96.961% (5647/5824)\n",
      "120 234 Train Loss: 0.094 | Train Acc: 97.056% (7516/7744)\n",
      "150 234 Train Loss: 0.097 | Train Acc: 96.906% (9365/9664)\n",
      "180 234 Train Loss: 0.104 | Train Acc: 96.633% (11194/11584)\n",
      "210 234 Train Loss: 0.112 | Train Acc: 96.297% (13004/13504)\n",
      "234 Epoch: 13 | Train Loss: 0.120 | Train Acc: 95.947% (14369/14976)\n",
      "0 234 Test Loss: 0.846 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 1.170 | Test Acc: 71.321% (1415/1984)\n",
      "60 234 Test Loss: 1.187 | Test Acc: 70.441% (2750/3904)\n",
      "90 234 Test Loss: 1.201 | Test Acc: 69.952% (4074/5824)\n",
      "120 234 Test Loss: 1.212 | Test Acc: 69.693% (5397/7744)\n",
      "150 234 Test Loss: 1.213 | Test Acc: 69.588% (6725/9664)\n",
      "180 234 Test Loss: 1.222 | Test Acc: 69.441% (8044/11584)\n",
      "210 234 Test Loss: 1.219 | Test Acc: 69.587% (9397/13504)\n",
      "234 Epoch: 13 | Test Loss: 1.220 | Test Acc: 69.705% (10439/14976)\n",
      "\n",
      "Epoch: 14\n",
      "0 234 Train Loss: 0.089 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.095 | Train Acc: 97.228% (1929/1984)\n",
      "60 234 Train Loss: 0.107 | Train Acc: 96.568% (3770/3904)\n",
      "90 234 Train Loss: 0.104 | Train Acc: 96.703% (5632/5824)\n",
      "120 234 Train Loss: 0.102 | Train Acc: 96.707% (7489/7744)\n",
      "150 234 Train Loss: 0.101 | Train Acc: 96.730% (9348/9664)\n",
      "180 234 Train Loss: 0.101 | Train Acc: 96.711% (11203/11584)\n",
      "210 234 Train Loss: 0.101 | Train Acc: 96.638% (13050/13504)\n",
      "234 Epoch: 14 | Train Loss: 0.104 | Train Acc: 96.488% (14450/14976)\n",
      "0 234 Test Loss: 1.104 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 1.084 | Test Acc: 71.119% (1411/1984)\n",
      "60 234 Test Loss: 1.095 | Test Acc: 71.004% (2772/3904)\n",
      "90 234 Test Loss: 1.088 | Test Acc: 70.913% (4130/5824)\n",
      "120 234 Test Loss: 1.084 | Test Acc: 71.371% (5527/7744)\n",
      "150 234 Test Loss: 1.068 | Test Acc: 71.813% (6940/9664)\n",
      "180 234 Test Loss: 1.075 | Test Acc: 71.694% (8305/11584)\n",
      "210 234 Test Loss: 1.078 | Test Acc: 71.882% (9707/13504)\n",
      "234 Epoch: 14 | Test Loss: 1.080 | Test Acc: 71.968% (10778/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 15\n",
      "0 234 Train Loss: 0.153 | Train Acc: 95.312% (61/64)\n",
      "30 234 Train Loss: 0.075 | Train Acc: 97.631% (1937/1984)\n",
      "60 234 Train Loss: 0.069 | Train Acc: 97.848% (3820/3904)\n",
      "90 234 Train Loss: 0.071 | Train Acc: 97.716% (5691/5824)\n",
      "120 234 Train Loss: 0.073 | Train Acc: 97.650% (7562/7744)\n",
      "150 234 Train Loss: 0.076 | Train Acc: 97.610% (9433/9664)\n",
      "180 234 Train Loss: 0.076 | Train Acc: 97.600% (11306/11584)\n",
      "210 234 Train Loss: 0.076 | Train Acc: 97.556% (13174/13504)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234 Epoch: 15 | Train Loss: 0.078 | Train Acc: 97.476% (14598/14976)\n",
      "0 234 Test Loss: 1.064 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 1.216 | Test Acc: 70.111% (1391/1984)\n",
      "60 234 Test Loss: 1.220 | Test Acc: 70.594% (2756/3904)\n",
      "90 234 Test Loss: 1.224 | Test Acc: 70.639% (4114/5824)\n",
      "120 234 Test Loss: 1.261 | Test Acc: 70.028% (5423/7744)\n",
      "150 234 Test Loss: 1.279 | Test Acc: 69.733% (6739/9664)\n",
      "180 234 Test Loss: 1.271 | Test Acc: 69.924% (8100/11584)\n",
      "210 234 Test Loss: 1.273 | Test Acc: 69.742% (9418/13504)\n",
      "234 Epoch: 15 | Test Loss: 1.279 | Test Acc: 69.718% (10441/14976)\n",
      "\n",
      "Epoch: 16\n",
      "0 234 Train Loss: 0.077 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.058 | Train Acc: 98.236% (1949/1984)\n",
      "60 234 Train Loss: 0.050 | Train Acc: 98.412% (3842/3904)\n",
      "90 234 Train Loss: 0.049 | Train Acc: 98.455% (5734/5824)\n",
      "120 234 Train Loss: 0.049 | Train Acc: 98.412% (7621/7744)\n",
      "150 234 Train Loss: 0.051 | Train Acc: 98.313% (9501/9664)\n",
      "180 234 Train Loss: 0.056 | Train Acc: 98.170% (11372/11584)\n",
      "210 234 Train Loss: 0.061 | Train Acc: 97.956% (13228/13504)\n",
      "234 Epoch: 16 | Train Loss: 0.063 | Train Acc: 97.930% (14666/14976)\n",
      "0 234 Test Loss: 1.269 | Test Acc: 64.062% (41/64)\n",
      "30 234 Test Loss: 1.104 | Test Acc: 71.976% (1428/1984)\n",
      "60 234 Test Loss: 1.108 | Test Acc: 71.747% (2801/3904)\n",
      "90 234 Test Loss: 1.114 | Test Acc: 71.600% (4170/5824)\n",
      "120 234 Test Loss: 1.106 | Test Acc: 71.888% (5567/7744)\n",
      "150 234 Test Loss: 1.099 | Test Acc: 72.041% (6962/9664)\n",
      "180 234 Test Loss: 1.086 | Test Acc: 72.436% (8391/11584)\n",
      "210 234 Test Loss: 1.090 | Test Acc: 72.408% (9778/13504)\n",
      "234 Epoch: 16 | Test Loss: 1.094 | Test Acc: 72.369% (10838/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 17\n",
      "0 234 Train Loss: 0.074 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.090 | Train Acc: 96.825% (1921/1984)\n",
      "60 234 Train Loss: 0.082 | Train Acc: 97.131% (3792/3904)\n",
      "90 234 Train Loss: 0.075 | Train Acc: 97.459% (5676/5824)\n",
      "120 234 Train Loss: 0.074 | Train Acc: 97.469% (7548/7744)\n",
      "150 234 Train Loss: 0.072 | Train Acc: 97.548% (9427/9664)\n",
      "180 234 Train Loss: 0.073 | Train Acc: 97.531% (11298/11584)\n",
      "210 234 Train Loss: 0.072 | Train Acc: 97.564% (13175/13504)\n",
      "234 Epoch: 17 | Train Loss: 0.072 | Train Acc: 97.529% (14606/14976)\n",
      "0 234 Test Loss: 1.032 | Test Acc: 71.875% (46/64)\n",
      "30 234 Test Loss: 1.196 | Test Acc: 70.565% (1400/1984)\n",
      "60 234 Test Loss: 1.180 | Test Acc: 70.850% (2766/3904)\n",
      "90 234 Test Loss: 1.160 | Test Acc: 71.137% (4143/5824)\n",
      "120 234 Test Loss: 1.132 | Test Acc: 71.746% (5556/7744)\n",
      "150 234 Test Loss: 1.145 | Test Acc: 71.565% (6916/9664)\n",
      "180 234 Test Loss: 1.134 | Test Acc: 71.711% (8307/11584)\n",
      "210 234 Test Loss: 1.130 | Test Acc: 71.749% (9689/13504)\n",
      "234 Epoch: 17 | Test Loss: 1.133 | Test Acc: 71.681% (10735/14976)\n",
      "\n",
      "Epoch: 18\n",
      "0 234 Train Loss: 0.113 | Train Acc: 95.312% (61/64)\n",
      "30 234 Train Loss: 0.063 | Train Acc: 98.034% (1945/1984)\n",
      "60 234 Train Loss: 0.056 | Train Acc: 98.079% (3829/3904)\n",
      "90 234 Train Loss: 0.053 | Train Acc: 98.249% (5722/5824)\n",
      "120 234 Train Loss: 0.050 | Train Acc: 98.347% (7616/7744)\n",
      "150 234 Train Loss: 0.050 | Train Acc: 98.334% (9503/9664)\n",
      "180 234 Train Loss: 0.050 | Train Acc: 98.334% (11391/11584)\n",
      "210 234 Train Loss: 0.051 | Train Acc: 98.304% (13275/13504)\n",
      "234 Epoch: 18 | Train Loss: 0.055 | Train Acc: 98.177% (14703/14976)\n",
      "0 234 Test Loss: 1.331 | Test Acc: 60.938% (39/64)\n",
      "30 234 Test Loss: 1.289 | Test Acc: 69.002% (1369/1984)\n",
      "60 234 Test Loss: 1.260 | Test Acc: 70.236% (2742/3904)\n",
      "90 234 Test Loss: 1.266 | Test Acc: 70.261% (4092/5824)\n",
      "120 234 Test Loss: 1.251 | Test Acc: 70.261% (5441/7744)\n",
      "150 234 Test Loss: 1.257 | Test Acc: 70.323% (6796/9664)\n",
      "180 234 Test Loss: 1.267 | Test Acc: 70.088% (8119/11584)\n",
      "210 234 Test Loss: 1.269 | Test Acc: 70.061% (9461/13504)\n",
      "234 Epoch: 18 | Test Loss: 1.265 | Test Acc: 70.099% (10498/14976)\n",
      "\n",
      "Epoch: 19\n",
      "0 234 Train Loss: 0.042 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.082 | Train Acc: 97.379% (1932/1984)\n",
      "60 234 Train Loss: 0.068 | Train Acc: 97.797% (3818/3904)\n",
      "90 234 Train Loss: 0.066 | Train Acc: 97.871% (5700/5824)\n",
      "120 234 Train Loss: 0.065 | Train Acc: 97.934% (7584/7744)\n",
      "150 234 Train Loss: 0.065 | Train Acc: 97.899% (9461/9664)\n",
      "180 234 Train Loss: 0.067 | Train Acc: 97.816% (11331/11584)\n",
      "210 234 Train Loss: 0.070 | Train Acc: 97.704% (13194/13504)\n",
      "234 Epoch: 19 | Train Loss: 0.070 | Train Acc: 97.696% (14631/14976)\n",
      "0 234 Test Loss: 0.868 | Test Acc: 71.875% (46/64)\n",
      "30 234 Test Loss: 1.069 | Test Acc: 72.933% (1447/1984)\n",
      "60 234 Test Loss: 1.040 | Test Acc: 73.489% (2869/3904)\n",
      "90 234 Test Loss: 1.053 | Test Acc: 73.214% (4264/5824)\n",
      "120 234 Test Loss: 1.067 | Test Acc: 73.179% (5667/7744)\n",
      "150 234 Test Loss: 1.072 | Test Acc: 73.220% (7076/9664)\n",
      "180 234 Test Loss: 1.070 | Test Acc: 73.153% (8474/11584)\n",
      "210 234 Test Loss: 1.079 | Test Acc: 73.075% (9868/13504)\n",
      "234 Epoch: 19 | Test Loss: 1.074 | Test Acc: 73.084% (10945/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 20\n",
      "0 234 Train Loss: 0.017 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.045 | Train Acc: 98.488% (1954/1984)\n",
      "60 234 Train Loss: 0.045 | Train Acc: 98.540% (3847/3904)\n",
      "90 234 Train Loss: 0.044 | Train Acc: 98.592% (5742/5824)\n",
      "120 234 Train Loss: 0.043 | Train Acc: 98.605% (7636/7744)\n",
      "150 234 Train Loss: 0.043 | Train Acc: 98.603% (9529/9664)\n",
      "180 234 Train Loss: 0.044 | Train Acc: 98.567% (11418/11584)\n",
      "210 234 Train Loss: 0.045 | Train Acc: 98.475% (13298/13504)\n",
      "234 Epoch: 20 | Train Loss: 0.045 | Train Acc: 98.518% (14754/14976)\n",
      "0 234 Test Loss: 1.471 | Test Acc: 65.625% (42/64)\n",
      "30 234 Test Loss: 1.033 | Test Acc: 74.345% (1475/1984)\n",
      "60 234 Test Loss: 0.987 | Test Acc: 75.000% (2928/3904)\n",
      "90 234 Test Loss: 1.019 | Test Acc: 74.828% (4358/5824)\n",
      "120 234 Test Loss: 1.019 | Test Acc: 74.858% (5797/7744)\n",
      "150 234 Test Loss: 1.021 | Test Acc: 74.741% (7223/9664)\n",
      "180 234 Test Loss: 1.029 | Test Acc: 74.603% (8642/11584)\n",
      "210 234 Test Loss: 1.039 | Test Acc: 74.452% (10054/13504)\n",
      "234 Epoch: 20 | Test Loss: 1.044 | Test Acc: 74.306% (11128/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 21\n",
      "0 234 Train Loss: 0.007 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.025 | Train Acc: 99.395% (1972/1984)\n",
      "60 234 Train Loss: 0.024 | Train Acc: 99.308% (3877/3904)\n",
      "90 234 Train Loss: 0.023 | Train Acc: 99.399% (5789/5824)\n",
      "120 234 Train Loss: 0.022 | Train Acc: 99.367% (7695/7744)\n",
      "150 234 Train Loss: 0.022 | Train Acc: 99.389% (9605/9664)\n",
      "180 234 Train Loss: 0.022 | Train Acc: 99.404% (11515/11584)\n",
      "210 234 Train Loss: 0.021 | Train Acc: 99.445% (13429/13504)\n",
      "234 Epoch: 21 | Train Loss: 0.021 | Train Acc: 99.459% (14895/14976)\n",
      "0 234 Test Loss: 0.782 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 0.946 | Test Acc: 76.210% (1512/1984)\n",
      "60 234 Test Loss: 0.974 | Test Acc: 76.076% (2970/3904)\n",
      "90 234 Test Loss: 0.984 | Test Acc: 75.962% (4424/5824)\n",
      "120 234 Test Loss: 0.991 | Test Acc: 75.994% (5885/7744)\n",
      "150 234 Test Loss: 1.007 | Test Acc: 75.476% (7294/9664)\n",
      "180 234 Test Loss: 1.006 | Test Acc: 75.397% (8734/11584)\n",
      "210 234 Test Loss: 1.011 | Test Acc: 75.141% (10147/13504)\n",
      "234 Epoch: 21 | Test Loss: 0.999 | Test Acc: 75.347% (11284/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 22\n",
      "0 234 Train Loss: 0.022 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.014 | Train Acc: 99.849% (1981/1984)\n",
      "60 234 Train Loss: 0.013 | Train Acc: 99.821% (3897/3904)\n",
      "90 234 Train Loss: 0.012 | Train Acc: 99.845% (5815/5824)\n",
      "120 234 Train Loss: 0.013 | Train Acc: 99.806% (7729/7744)\n",
      "150 234 Train Loss: 0.013 | Train Acc: 99.793% (9644/9664)\n",
      "180 234 Train Loss: 0.012 | Train Acc: 99.801% (11561/11584)\n",
      "210 234 Train Loss: 0.014 | Train Acc: 99.763% (13472/13504)\n",
      "234 Epoch: 22 | Train Loss: 0.014 | Train Acc: 99.746% (14938/14976)\n",
      "0 234 Test Loss: 1.245 | Test Acc: 67.188% (43/64)\n",
      "30 234 Test Loss: 1.006 | Test Acc: 74.950% (1487/1984)\n",
      "60 234 Test Loss: 1.018 | Test Acc: 75.026% (2929/3904)\n",
      "90 234 Test Loss: 0.991 | Test Acc: 75.721% (4410/5824)\n",
      "120 234 Test Loss: 1.009 | Test Acc: 75.917% (5879/7744)\n",
      "150 234 Test Loss: 1.012 | Test Acc: 75.859% (7331/9664)\n",
      "180 234 Test Loss: 1.031 | Test Acc: 75.587% (8756/11584)\n",
      "210 234 Test Loss: 1.031 | Test Acc: 75.504% (10196/13504)\n",
      "234 Epoch: 22 | Test Loss: 1.033 | Test Acc: 75.461% (11301/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 23\n",
      "0 234 Train Loss: 0.028 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.014 | Train Acc: 99.698% (1978/1984)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 234 Train Loss: 0.014 | Train Acc: 99.641% (3890/3904)\n",
      "90 234 Train Loss: 0.012 | Train Acc: 99.742% (5809/5824)\n",
      "120 234 Train Loss: 0.013 | Train Acc: 99.690% (7720/7744)\n",
      "150 234 Train Loss: 0.014 | Train Acc: 99.627% (9628/9664)\n",
      "180 234 Train Loss: 0.015 | Train Acc: 99.603% (11538/11584)\n",
      "210 234 Train Loss: 0.015 | Train Acc: 99.570% (13446/13504)\n",
      "234 Epoch: 23 | Train Loss: 0.015 | Train Acc: 99.579% (14913/14976)\n",
      "0 234 Test Loss: 1.165 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.018 | Test Acc: 75.252% (1493/1984)\n",
      "60 234 Test Loss: 0.999 | Test Acc: 75.717% (2956/3904)\n",
      "90 234 Test Loss: 1.013 | Test Acc: 75.172% (4378/5824)\n",
      "120 234 Test Loss: 1.029 | Test Acc: 74.819% (5794/7744)\n",
      "150 234 Test Loss: 1.037 | Test Acc: 74.700% (7219/9664)\n",
      "180 234 Test Loss: 1.045 | Test Acc: 74.586% (8640/11584)\n",
      "210 234 Test Loss: 1.047 | Test Acc: 74.467% (10056/13504)\n",
      "234 Epoch: 23 | Test Loss: 1.053 | Test Acc: 74.493% (11156/14976)\n",
      "\n",
      "Epoch: 24\n",
      "0 234 Train Loss: 0.017 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.015 | Train Acc: 99.597% (1976/1984)\n",
      "60 234 Train Loss: 0.016 | Train Acc: 99.590% (3888/3904)\n",
      "90 234 Train Loss: 0.017 | Train Acc: 99.588% (5800/5824)\n",
      "120 234 Train Loss: 0.016 | Train Acc: 99.613% (7714/7744)\n",
      "150 234 Train Loss: 0.017 | Train Acc: 99.555% (9621/9664)\n",
      "180 234 Train Loss: 0.018 | Train Acc: 99.560% (11533/11584)\n",
      "210 234 Train Loss: 0.018 | Train Acc: 99.548% (13443/13504)\n",
      "234 Epoch: 24 | Train Loss: 0.018 | Train Acc: 99.546% (14908/14976)\n",
      "0 234 Test Loss: 1.281 | Test Acc: 71.875% (46/64)\n",
      "30 234 Test Loss: 1.020 | Test Acc: 75.151% (1491/1984)\n",
      "60 234 Test Loss: 1.030 | Test Acc: 75.359% (2942/3904)\n",
      "90 234 Test Loss: 1.039 | Test Acc: 75.120% (4375/5824)\n",
      "120 234 Test Loss: 1.052 | Test Acc: 74.923% (5802/7744)\n",
      "150 234 Test Loss: 1.053 | Test Acc: 74.596% (7209/9664)\n",
      "180 234 Test Loss: 1.055 | Test Acc: 74.594% (8641/11584)\n",
      "210 234 Test Loss: 1.062 | Test Acc: 74.326% (10037/13504)\n",
      "234 Epoch: 24 | Test Loss: 1.070 | Test Acc: 74.279% (11124/14976)\n",
      "\n",
      "Epoch: 25\n",
      "0 234 Train Loss: 0.014 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.012 | Train Acc: 99.597% (1976/1984)\n",
      "60 234 Train Loss: 0.012 | Train Acc: 99.667% (3891/3904)\n",
      "90 234 Train Loss: 0.013 | Train Acc: 99.657% (5804/5824)\n",
      "120 234 Train Loss: 0.016 | Train Acc: 99.496% (7705/7744)\n",
      "150 234 Train Loss: 0.016 | Train Acc: 99.493% (9615/9664)\n",
      "180 234 Train Loss: 0.018 | Train Acc: 99.465% (11522/11584)\n",
      "210 234 Train Loss: 0.019 | Train Acc: 99.437% (13428/13504)\n",
      "234 Epoch: 25 | Train Loss: 0.019 | Train Acc: 99.446% (14893/14976)\n",
      "0 234 Test Loss: 1.350 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 1.106 | Test Acc: 73.841% (1465/1984)\n",
      "60 234 Test Loss: 1.082 | Test Acc: 74.334% (2902/3904)\n",
      "90 234 Test Loss: 1.067 | Test Acc: 74.124% (4317/5824)\n",
      "120 234 Test Loss: 1.056 | Test Acc: 74.264% (5751/7744)\n",
      "150 234 Test Loss: 1.062 | Test Acc: 74.141% (7165/9664)\n",
      "180 234 Test Loss: 1.061 | Test Acc: 74.076% (8581/11584)\n",
      "210 234 Test Loss: 1.064 | Test Acc: 74.178% (10017/13504)\n",
      "234 Epoch: 25 | Test Loss: 1.063 | Test Acc: 74.312% (11129/14976)\n",
      "\n",
      "Epoch: 26\n",
      "0 234 Train Loss: 0.006 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.011 | Train Acc: 99.748% (1979/1984)\n",
      "60 234 Train Loss: 0.013 | Train Acc: 99.795% (3896/3904)\n",
      "90 234 Train Loss: 0.012 | Train Acc: 99.794% (5812/5824)\n",
      "120 234 Train Loss: 0.011 | Train Acc: 99.819% (7730/7744)\n",
      "150 234 Train Loss: 0.010 | Train Acc: 99.814% (9646/9664)\n",
      "180 234 Train Loss: 0.010 | Train Acc: 99.827% (11564/11584)\n",
      "210 234 Train Loss: 0.009 | Train Acc: 99.837% (13482/13504)\n",
      "234 Epoch: 26 | Train Loss: 0.010 | Train Acc: 99.826% (14950/14976)\n",
      "0 234 Test Loss: 1.265 | Test Acc: 67.188% (43/64)\n",
      "30 234 Test Loss: 1.025 | Test Acc: 73.690% (1462/1984)\n",
      "60 234 Test Loss: 1.052 | Test Acc: 73.822% (2882/3904)\n",
      "90 234 Test Loss: 1.017 | Test Acc: 74.262% (4325/5824)\n",
      "120 234 Test Loss: 1.004 | Test Acc: 74.897% (5800/7744)\n",
      "150 234 Test Loss: 1.007 | Test Acc: 75.041% (7252/9664)\n",
      "180 234 Test Loss: 1.006 | Test Acc: 75.207% (8712/11584)\n",
      "210 234 Test Loss: 1.005 | Test Acc: 75.333% (10173/13504)\n",
      "234 Epoch: 26 | Test Loss: 1.008 | Test Acc: 75.367% (11287/14976)\n",
      "\n",
      "Epoch: 27\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.011 | Train Acc: 99.748% (1979/1984)\n",
      "60 234 Train Loss: 0.011 | Train Acc: 99.744% (3894/3904)\n",
      "90 234 Train Loss: 0.012 | Train Acc: 99.708% (5807/5824)\n",
      "120 234 Train Loss: 0.012 | Train Acc: 99.716% (7722/7744)\n",
      "150 234 Train Loss: 0.014 | Train Acc: 99.607% (9626/9664)\n",
      "180 234 Train Loss: 0.014 | Train Acc: 99.646% (11543/11584)\n",
      "210 234 Train Loss: 0.014 | Train Acc: 99.645% (13456/13504)\n",
      "234 Epoch: 27 | Train Loss: 0.015 | Train Acc: 99.613% (14918/14976)\n",
      "0 234 Test Loss: 1.034 | Test Acc: 67.188% (43/64)\n",
      "30 234 Test Loss: 1.156 | Test Acc: 72.681% (1442/1984)\n",
      "60 234 Test Loss: 1.153 | Test Acc: 73.386% (2865/3904)\n",
      "90 234 Test Loss: 1.113 | Test Acc: 74.004% (4310/5824)\n",
      "120 234 Test Loss: 1.135 | Test Acc: 73.528% (5694/7744)\n",
      "150 234 Test Loss: 1.122 | Test Acc: 74.069% (7158/9664)\n",
      "180 234 Test Loss: 1.121 | Test Acc: 73.981% (8570/11584)\n",
      "210 234 Test Loss: 1.110 | Test Acc: 74.156% (10014/13504)\n",
      "234 Epoch: 27 | Test Loss: 1.113 | Test Acc: 74.199% (11112/14976)\n",
      "\n",
      "Epoch: 28\n",
      "0 234 Train Loss: 0.011 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.026 | Train Acc: 99.093% (1966/1984)\n",
      "60 234 Train Loss: 0.028 | Train Acc: 99.129% (3870/3904)\n",
      "90 234 Train Loss: 0.026 | Train Acc: 99.227% (5779/5824)\n",
      "120 234 Train Loss: 0.026 | Train Acc: 99.212% (7683/7744)\n",
      "150 234 Train Loss: 0.026 | Train Acc: 99.234% (9590/9664)\n",
      "180 234 Train Loss: 0.027 | Train Acc: 99.154% (11486/11584)\n",
      "210 234 Train Loss: 0.030 | Train Acc: 99.074% (13379/13504)\n",
      "234 Epoch: 28 | Train Loss: 0.032 | Train Acc: 98.972% (14822/14976)\n",
      "0 234 Test Loss: 0.898 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 1.116 | Test Acc: 73.942% (1467/1984)\n",
      "60 234 Test Loss: 1.142 | Test Acc: 73.463% (2868/3904)\n",
      "90 234 Test Loss: 1.153 | Test Acc: 73.094% (4257/5824)\n",
      "120 234 Test Loss: 1.137 | Test Acc: 73.218% (5670/7744)\n",
      "150 234 Test Loss: 1.128 | Test Acc: 73.541% (7107/9664)\n",
      "180 234 Test Loss: 1.131 | Test Acc: 73.403% (8503/11584)\n",
      "210 234 Test Loss: 1.135 | Test Acc: 73.475% (9922/13504)\n",
      "234 Epoch: 28 | Test Loss: 1.130 | Test Acc: 73.578% (11019/14976)\n",
      "\n",
      "Epoch: 29\n",
      "0 234 Train Loss: 0.015 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.042 | Train Acc: 98.589% (1956/1984)\n",
      "60 234 Train Loss: 0.051 | Train Acc: 98.335% (3839/3904)\n",
      "90 234 Train Loss: 0.051 | Train Acc: 98.317% (5726/5824)\n",
      "120 234 Train Loss: 0.054 | Train Acc: 98.231% (7607/7744)\n",
      "150 234 Train Loss: 0.060 | Train Acc: 98.055% (9476/9664)\n",
      "180 234 Train Loss: 0.063 | Train Acc: 97.945% (11346/11584)\n",
      "210 234 Train Loss: 0.064 | Train Acc: 97.860% (13215/13504)\n",
      "234 Epoch: 29 | Train Loss: 0.066 | Train Acc: 97.817% (14649/14976)\n",
      "0 234 Test Loss: 1.221 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 1.034 | Test Acc: 73.790% (1464/1984)\n",
      "60 234 Test Loss: 1.114 | Test Acc: 72.951% (2848/3904)\n",
      "90 234 Test Loss: 1.126 | Test Acc: 72.957% (4249/5824)\n",
      "120 234 Test Loss: 1.120 | Test Acc: 73.115% (5662/7744)\n",
      "150 234 Test Loss: 1.120 | Test Acc: 73.055% (7060/9664)\n",
      "180 234 Test Loss: 1.113 | Test Acc: 73.058% (8463/11584)\n",
      "210 234 Test Loss: 1.115 | Test Acc: 72.912% (9846/13504)\n",
      "234 Epoch: 29 | Test Loss: 1.116 | Test Acc: 72.857% (10911/14976)\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.2.3: Start Target model training\n",
    "\n",
    "max_epoch = 30  #@param {type:\"integer\"}\n",
    "train_result_summary = 'target_train_DCA-BiLSTM.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(target_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(target_testloader, epoch, batch_size=batch_size, logfile = train_result_summary, save_modelpath = save_model_folder)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea5c825e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'draw_training_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdraw_training_summary\u001b[49m(filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_train_DCA-BiLSTM.summary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'draw_training_summary' is not defined"
     ]
    }
   ],
   "source": [
    "draw_training_summary(filepath = 'target_train_DCA-BiLSTM.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23706efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DLA-BiLSTM'  #@param {type:\"string\"}\n",
    "save_model_folder = './Shadow-DLA-BiLSTM_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DLA-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76652da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.2.3: Start Shadow model training\n",
    "\n",
    "max_epoch = 30  #@param {type:\"integer\"}\n",
    "train_result_summary = 'shadow_train_DCA-BiLSTM.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(shadow_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(shadow_testloader, epoch, batch_size=batch_size, logfile = train_result_summary, save_modelpath = save_model_folder)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34726edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_training_summary(filepath = 'shadow_train_DCA-BiLSTM.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DLA-LSTM'  #@param {type:\"string\"}\n",
    "save_model_folder = './Target-DLA-LSTM_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DLA-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80fac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.2.3: Start Target model training\n",
    "\n",
    "max_epoch = 30  #@param {type:\"integer\"}\n",
    "train_result_summary = 'target_train_DCA-LSTM.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(target_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(target_testloader, epoch, batch_size=batch_size, logfile = train_result_summary, save_modelpath = save_model_folder)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fecd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_training_summary(filepath = 'target_train_DCA-LSTM.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7576e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DLA-LSTM'  #@param {type:\"string\"}\n",
    "save_model_folder = './Shadow-DLA-LSTM_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DLA-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.2.3: Start Shadow model training\n",
    "\n",
    "max_epoch = 30  #@param {type:\"integer\"}\n",
    "train_result_summary = 'shadow_train_DCA-LSTM.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(shadow_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(shadow_testloader, epoch, batch_size=batch_size, logfile = train_result_summary, save_modelpath = save_model_folder)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_training_summary(filepath = 'shadow_train_DCA-LSTM.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8daac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DLA'  #@param {type:\"string\"}\n",
    "save_model_folder = './Target-DLA_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DLA-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3504245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.2.3: Start Target model training\n",
    "\n",
    "max_epoch = 30  #@param {type:\"integer\"}\n",
    "train_result_summary = 'target_train_DCA.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(target_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(target_testloader, epoch, batch_size=batch_size, logfile = train_result_summary, save_modelpath = save_model_folder)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c765fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_training_summary(filepath = 'target_train_DCA.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b220fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DLA'  #@param {type:\"string\"}\n",
    "save_model_folder = './Shadow-DLA_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DLA-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1954c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.2.3: Start Shadow model training\n",
    "\n",
    "max_epoch = 30  #@param {type:\"integer\"}\n",
    "train_result_summary = 'shadow_train_DCA.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(shadow_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(shadow_testloader, epoch, batch_size=batch_size, logfile = train_result_summary, save_modelpath = save_model_folder)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0050b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_training_summary(filepath = 'shadow_train_DCA.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9fb94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
