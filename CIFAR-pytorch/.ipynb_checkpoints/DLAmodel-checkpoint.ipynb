{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "664eea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 1: Define required functions for Data processing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import Tensor\n",
    "import os\n",
    "import argparse\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]\n",
    "\n",
    "def balance_val_split(dataset, train_size=12500):\n",
    "\n",
    "    try:\n",
    "        targets = np.array(dataset.targets)\n",
    "    except:\n",
    "        targets = []  # create an empty list to store the targets\n",
    "        for data in dataset.datasets:\n",
    "            targets += data.targets  # concatenate the targets from each dataset into the list\n",
    "        targets = np.array(targets)\n",
    "    #targets = np.array(dataset.datasets.targets)\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(targets.shape[0]),\n",
    "        train_size=train_size,\n",
    "        stratify=targets\n",
    "    )\n",
    "    train_dataset = Subset(dataset, indices=train_indices)\n",
    "    # Get the data from the subset dataset\n",
    "    subset_data = [train_dataset[idx][0] for idx in range(len(train_dataset))]\n",
    "    subset_labels = [train_dataset[idx][1] for idx in range(len(train_dataset))]\n",
    "    # Create a dataset from the list of data and targets\n",
    "    train_dataset = MyDataset(subset_data, subset_labels)\n",
    "    \n",
    "    \n",
    "    val_dataset = Subset(dataset, indices=val_indices)\n",
    "    # Get the data from the subset dataset\n",
    "    subset_data = [val_dataset[idx][0] for idx in range(len(val_dataset))]\n",
    "    subset_labels = [val_dataset[idx][1] for idx in range(len(val_dataset))]\n",
    "    # Create a dataset from the list of data and targets\n",
    "    val_dataset = MyDataset(subset_data, subset_labels)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def count_label_frequency(target_train_dataset):\n",
    "\tfrom collections import Counter\n",
    "\ttarget_labels = []  # create an empty list to store the labels\n",
    "\n",
    "\tfor i in range(len(target_train_dataset)):\n",
    "\t\t\t_, label = target_train_dataset[i]  # extract the label for the i-th example in the subset\n",
    "\t\t\ttarget_labels.append(label)  # append the label to the 'subset_labels' list\n",
    "\n",
    "\n",
    "\treturn Counter(target_labels)\n",
    " \n",
    "\n",
    "\n",
    "def custom_transform(image: Tensor) -> Tensor:\n",
    "    import random\n",
    "    # randomly flip horizontally or vertically with 25% chance\n",
    "    if random.random() < 0.25:\n",
    "        image = RandomHorizontalFlip(p=1)(image)\n",
    "    elif random.random() < 0.5:\n",
    "        image = RandomVerticalFlip(p=1)(image)\n",
    "    \n",
    "    # randomly shift the image by 2 pixels to the left or right with 25% chance\n",
    "    if random.random() < 0.25:\n",
    "        image = RandomCrop((image.shape[-2], image.shape[-1] - 2), pad_if_needed=False)(image)\n",
    "    elif random.random() < 0.5:\n",
    "        image = RandomCrop((image.shape[-2], image.shape[-1] + 2), pad_if_needed=False)(image)\n",
    "        \n",
    "    # randomly shift the image by 2 pixels to the top or bottom with 25% chance\n",
    "    if random.random() < 0.25:\n",
    "        image = RandomCrop((image.shape[-2] - 2, image.shape[-1]), pad_if_needed=False)(image)\n",
    "    elif random.random() < 0.5:\n",
    "        image = RandomCrop((image.shape[-2] + 2, image.shape[-1]), pad_if_needed=False)(image)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23c225eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 2: Define required functions for Data Training\n",
    "\n",
    "# Training\n",
    "def train(trainloader, epoch, batch_size=128, logfile = \"train.summary\"):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        #inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        if inputs.shape[0] != batch_size:\n",
    "          print(inputs.shape)\n",
    "          continue\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if batch_idx % 30 == 0:\n",
    "                print(batch_idx, len(trainloader), 'Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    print(len(trainloader), 'Epoch: %d | Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                     % (epoch, train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    f = open(logfile, \"a\")\n",
    "    f.write('Epoch: %d | Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)\\n'\n",
    "                     % (epoch, train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    f.close()\n",
    "\n",
    "def test(testloader, epoch, batch_size=128, logfile = \"train.summary\", save_modelpath = './DLA'):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            #inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            if batch_idx % 30 == 0:\n",
    "                print(batch_idx, len(testloader), 'Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    print(len(testloader), 'Epoch: %d | Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
    "                         % (epoch, test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    f = open(logfile, \"a\")\n",
    "    f.write('Epoch: %d | Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)\\n'\n",
    "                         % (epoch, test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    f.close()\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir(save_modelpath):\n",
    "            os.mkdir(save_modelpath)\n",
    "        torch.save(state, save_modelpath+'/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "def draw_training_summary(filepath = 'target_train_DCA-BiLSTM.summary'):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        results_summary = f.read()\n",
    "\n",
    "    train_epoch = []\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_epoch = []\n",
    "    test_loss=[]\n",
    "    test_acc=[]\n",
    "    for line in results_summary.split(\"\\n\"):\n",
    "        try:\n",
    "            r_epoch = line.split('|')[0].strip().split(' ')[1]\n",
    "            r_loss = line.split('|')[1].strip().split(' ')[2].replace('%','')\n",
    "            r_acc = line.split('|')[2].strip().split(' ')[2].replace('%','')\n",
    "            if 'Train' in line:\n",
    "                train_epoch.append(int(r_epoch))\n",
    "                train_loss.append(float(r_loss))\n",
    "                train_acc.append(float(r_acc))\n",
    "            if 'Test' in line:\n",
    "                test_epoch.append(int(r_epoch))\n",
    "                test_loss.append(float(r_loss))\n",
    "                test_acc.append(float(r_acc))\n",
    "        except:\n",
    "            print(line)\n",
    "\n",
    "    # Create a new figure and plot the data\n",
    "    plt.figure(figsize=(12,4))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_acc, label='Train')\n",
    "    plt.plot(test_acc, label='Test')\n",
    "    plt.axhline(y=np.max(test_acc), color='r', linestyle='--')\n",
    "    # Add text for the horizontal line\n",
    "    plt.text(test_epoch[-10], np.max(test_acc)*1.05, np.max(test_acc), color='r', fontsize=10)\n",
    "    # Customize the plot\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_loss, label='Train')\n",
    "    plt.plot(test_loss, label='Test')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2804c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 3: Prepare Cifar10 dataset for target and shadow model\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "def create_cifar_dataset_torch(batch_size=128, target_train_size = 15000, target_test_size= 15000, shadow_train_size = 15000, shadow_test_size= 15000):\n",
    "\n",
    "  # Data\n",
    "  print('==> Preparing data..')\n",
    "\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "      transforms.ToTensor()\n",
    "  ])\n",
    "  \n",
    "  cifar_trainset = torchvision.datasets.CIFAR10(\n",
    "      root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "  cifar_testset = torchvision.datasets.CIFAR10(\n",
    "      root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "  cifar_dataset = torch.utils.data.ConcatDataset([cifar_trainset, cifar_testset])\n",
    "\n",
    "\n",
    "  #target_train_size = int(0.25 * len(cifar_dataset)) # 15000\n",
    "  remain_size = len(cifar_dataset) - target_train_size\n",
    "  target_train_dataset, remain_dataset = torch.utils.data.random_split(cifar_dataset, [target_train_size, remain_size])\n",
    "\n",
    "  #target_test_size = int(0.25 * len(cifar_dataset)) # 15000\n",
    "  remain_size = len(remain_dataset) - target_test_size\n",
    "  target_test_dataset, remain_dataset = torch.utils.data.random_split(remain_dataset, [target_test_size, remain_size])\n",
    "\n",
    "  #target_test_dataset, remain_dataset = balance_val_split(remain_dataset, train_size=target_test_size)\n",
    "\n",
    "\n",
    "  #shadow_train_size = int(0.25 * len(cifar_dataset)) # 15000\n",
    "  remain_size = len(remain_dataset) - shadow_train_size\n",
    "  shadow_train_dataset, shadow_test_dataset = torch.utils.data.random_split(remain_dataset, [shadow_train_size, remain_size])\n",
    "  #shadow_train_dataset, shadow_test_dataset = balance_val_split(remain_dataset, train_size=shadow_train_size)\n",
    "\n",
    "  print(\"Setting target_train_dataset size to \",len(target_train_dataset), count_label_frequency(target_train_dataset))\n",
    "  print(\"Setting target_test_dataset size to \",len(target_test_dataset), count_label_frequency(target_test_dataset))\n",
    "  print(\"Setting shadow_train_dataset size to \",len(shadow_train_dataset), count_label_frequency(shadow_train_dataset))\n",
    "  print(\"Setting shadow_test_dataset size to \",len(shadow_test_dataset), count_label_frequency(shadow_test_dataset))\n",
    "  #print(\"Setting testset size to \",len(testset))\n",
    "\n",
    "\n",
    "\n",
    "  '''\n",
    "  transform_train = transforms.Compose([\n",
    "      transforms.RandomCrop(32, padding=4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "  ])\n",
    "  '''\n",
    "\n",
    "\n",
    "\n",
    "  transform_train = transforms.Compose([\n",
    "      transforms.RandomCrop(32, padding=4),\n",
    "      custom_transform,\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "  ])\n",
    "\n",
    "  transform_test = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "  ])\n",
    "\n",
    "  # apply the data augmentation transformations to the subset\n",
    "  target_train_dataset.dataset.transform = transform_train\n",
    "  # Load the transformed subset using a DataLoader\n",
    "  target_trainloader = DataLoader(target_train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "\n",
    "  target_test_dataset.dataset.transform = transform_test\n",
    "  # Load the transformed subset using a DataLoader\n",
    "  target_testloader = DataLoader(target_test_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "\n",
    "  # apply the data augmentation transformations to the subset\n",
    "  shadow_train_dataset.dataset.transform = transform_train\n",
    "  # Load the transformed subset using a DataLoader\n",
    "  shadow_trainloader = DataLoader(shadow_train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "\n",
    "  shadow_test_dataset.dataset.transform = transform_test\n",
    "  # Load the transformed subset using a DataLoader\n",
    "  shadow_testloader = DataLoader(shadow_test_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "  return target_trainloader, target_testloader, shadow_trainloader, shadow_testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26d6e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 4.2: Define required functions for DLA & DLA+RNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class Root(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1):\n",
    "        super(Root, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride=1, padding=(kernel_size - 1) // 2, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = torch.cat(xs, 1)\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Tree(nn.Module):\n",
    "    def __init__(self, block, in_channels, out_channels, level=1, stride=1):\n",
    "        super(Tree, self).__init__()\n",
    "        self.level = level\n",
    "        if level == 1:\n",
    "            self.root = Root(2*out_channels, out_channels)\n",
    "            self.left_node = block(in_channels, out_channels, stride=stride)\n",
    "            self.right_node = block(out_channels, out_channels, stride=1)\n",
    "        else:\n",
    "            self.root = Root((level+2)*out_channels, out_channels)\n",
    "            for i in reversed(range(1, level)):\n",
    "                subtree = Tree(block, in_channels, out_channels,\n",
    "                               level=i, stride=stride)\n",
    "                self.__setattr__('level_%d' % i, subtree)\n",
    "            self.prev_root = block(in_channels, out_channels, stride=stride)\n",
    "            self.left_node = block(out_channels, out_channels, stride=1)\n",
    "            self.right_node = block(out_channels, out_channels, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = [self.prev_root(x)] if self.level > 1 else []\n",
    "        for i in reversed(range(1, self.level)):\n",
    "            level_i = self.__getattr__('level_%d' % i)\n",
    "            x = level_i(x)\n",
    "            xs.append(x)\n",
    "        x = self.left_node(x)\n",
    "        xs.append(x)\n",
    "        x = self.right_node(x)\n",
    "        xs.append(x)\n",
    "        out = self.root(xs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DLA(nn.Module):\n",
    "    def __init__(self, block=BasicBlock, num_classes=10, enable_RNN='LSTM'):\n",
    "        super(DLA, self).__init__()\n",
    "        \n",
    "        self.enable_RNN = enable_RNN\n",
    "        if enable_RNN not in ['None', 'LSTM', 'Bi-LSTM']:\n",
    "          raise Exception(\"enable_RNN only supports one of ['None', 'LSTM', 'Bi-LSTM']\")\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer3 = Tree(block,  32,  64, level=1, stride=1)\n",
    "        self.layer4 = Tree(block,  64, 128, level=2, stride=2)\n",
    "        self.layer5 = Tree(block, 128, 256, level=2, stride=2)\n",
    "        self.layer6 = Tree(block, 256, 512, level=1, stride=2)\n",
    "        self.linear = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        if enable_RNN == 'None':\n",
    "          self.linear = nn.Linear(512, num_classes)\n",
    "        elif enable_RNN == 'LSTM':\n",
    "          self.rnn = nn.LSTM(512, 1024, dropout = 0.6)\n",
    "          self.linear = nn.Linear(1024, num_classes)\n",
    "        elif enable_RNN == 'Bi-LSTM':\n",
    "          self.rnn = nn.LSTM(512, 2048, 1, dropout = 0.3, batch_first=True, bidirectional=True)\n",
    "          self.linear = nn.Linear(4096, num_classes)\n",
    "        else:\n",
    "          raise Exception(\"enable_RNN only supports one of ['None', 'LSTM', 'Bi-LSTM']\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "\n",
    "        if self.enable_RNN == 'None':\n",
    "          out = out.view(out.size(0), -1)\n",
    "          out = self.linear(out)\n",
    "        else:\n",
    "          # add LSTM\n",
    "          #print(out.shape)\n",
    "          out = out.view(out.size(0), 1,  -1)\n",
    "          out,_ = self.rnn(out)\n",
    "          out = out.view(out.size(0), -1)\n",
    "          out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abd64b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model for DLA-BiLSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omars\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training model from scratch..\n",
      "Total trained parameters:  58303034\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DLA-BiLSTM'  #@param {type:\"string\"}\n",
    "save_model_folder = './Target-DLA-BiLSTM_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DLA-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a02bb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset..\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Setting target_train_dataset size to  15000 Counter({6: 1556, 2: 1540, 3: 1518, 4: 1509, 7: 1506, 9: 1479, 5: 1478, 0: 1472, 8: 1471, 1: 1471})\n",
      "Setting target_test_dataset size to  15000 Counter({2: 1540, 9: 1526, 5: 1523, 7: 1511, 1: 1506, 3: 1501, 8: 1497, 6: 1475, 0: 1472, 4: 1449})\n",
      "Setting shadow_train_dataset size to  15000 Counter({9: 1591, 4: 1539, 0: 1535, 8: 1525, 5: 1492, 1: 1486, 7: 1475, 2: 1469, 3: 1451, 6: 1437})\n",
      "Setting shadow_test_dataset size to  15000 Counter({1: 1537, 6: 1532, 3: 1530, 0: 1521, 7: 1508, 5: 1507, 8: 1507, 4: 1503, 2: 1451, 9: 1404})\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.1.2: Setup Target and Shadow datasets for ReNet Training\n",
    "\n",
    "target_train_size = 15000 #@param {type:\"integer\"}\n",
    "target_test_size= 15000 #@param {type:\"integer\"}\n",
    "shadow_train_size = 15000  #@param {type:\"integer\"} \n",
    "shadow_test_size= 15000 #@param {type:\"integer\"}\n",
    "\n",
    "# create dataset for renet \n",
    "print('==> Preparing dataset..')\n",
    "target_trainloader, target_testloader, shadow_trainloader, shadow_testloader = create_cifar_dataset_torch(batch_size=batch_size, target_train_size = target_train_size, target_test_size= target_test_size, shadow_train_size = shadow_train_size, shadow_test_size= shadow_test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f363b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 234 Train Loss: 2.305 | Train Acc: 9.375% (6/64)\n",
      "30 234 Train Loss: 2.299 | Train Acc: 10.030% (199/1984)\n",
      "60 234 Train Loss: 2.282 | Train Acc: 12.398% (484/3904)\n",
      "90 234 Train Loss: 2.247 | Train Acc: 14.166% (825/5824)\n",
      "120 234 Train Loss: 2.192 | Train Acc: 15.121% (1171/7744)\n",
      "150 234 Train Loss: 2.136 | Train Acc: 17.022% (1645/9664)\n",
      "180 234 Train Loss: 2.087 | Train Acc: 18.802% (2178/11584)\n",
      "210 234 Train Loss: 2.041 | Train Acc: 20.327% (2745/13504)\n",
      "234 Epoch: 0 | Train Loss: 2.015 | Train Acc: 21.314% (3192/14976)\n",
      "0 234 Test Loss: 1.839 | Test Acc: 31.250% (20/64)\n",
      "30 234 Test Loss: 1.795 | Test Acc: 29.788% (591/1984)\n",
      "60 234 Test Loss: 1.793 | Test Acc: 29.892% (1167/3904)\n",
      "90 234 Test Loss: 1.798 | Test Acc: 29.928% (1743/5824)\n",
      "120 234 Test Loss: 1.799 | Test Acc: 30.191% (2338/7744)\n",
      "150 234 Test Loss: 1.796 | Test Acc: 30.288% (2927/9664)\n",
      "180 234 Test Loss: 1.796 | Test Acc: 30.361% (3517/11584)\n",
      "210 234 Test Loss: 1.800 | Test Acc: 30.287% (4090/13504)\n",
      "234 Epoch: 0 | Test Loss: 1.802 | Test Acc: 30.442% (4559/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "0 234 Train Loss: 1.624 | Train Acc: 26.562% (17/64)\n",
      "30 234 Train Loss: 1.717 | Train Acc: 33.014% (655/1984)\n",
      "60 234 Train Loss: 1.703 | Train Acc: 33.607% (1312/3904)\n",
      "90 234 Train Loss: 1.689 | Train Acc: 34.976% (2037/5824)\n",
      "120 234 Train Loss: 1.661 | Train Acc: 35.795% (2772/7744)\n",
      "150 234 Train Loss: 1.647 | Train Acc: 36.579% (3535/9664)\n",
      "180 234 Train Loss: 1.622 | Train Acc: 37.517% (4346/11584)\n",
      "210 234 Train Loss: 1.608 | Train Acc: 38.337% (5177/13504)\n",
      "234 Epoch: 1 | Train Loss: 1.600 | Train Acc: 38.909% (5827/14976)\n",
      "0 234 Test Loss: 1.492 | Test Acc: 46.875% (30/64)\n",
      "30 234 Test Loss: 1.511 | Test Acc: 45.565% (904/1984)\n",
      "60 234 Test Loss: 1.514 | Test Acc: 44.749% (1747/3904)\n",
      "90 234 Test Loss: 1.504 | Test Acc: 44.797% (2609/5824)\n",
      "120 234 Test Loss: 1.507 | Test Acc: 44.628% (3456/7744)\n",
      "150 234 Test Loss: 1.512 | Test Acc: 44.433% (4294/9664)\n",
      "180 234 Test Loss: 1.510 | Test Acc: 44.475% (5152/11584)\n",
      "210 234 Test Loss: 1.507 | Test Acc: 44.402% (5996/13504)\n",
      "234 Epoch: 1 | Test Loss: 1.509 | Test Acc: 44.371% (6645/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "0 234 Train Loss: 1.429 | Train Acc: 53.125% (34/64)\n",
      "30 234 Train Loss: 1.383 | Train Acc: 49.698% (986/1984)\n",
      "60 234 Train Loss: 1.379 | Train Acc: 48.899% (1909/3904)\n",
      "90 234 Train Loss: 1.362 | Train Acc: 50.069% (2916/5824)\n",
      "120 234 Train Loss: 1.350 | Train Acc: 50.194% (3887/7744)\n",
      "150 234 Train Loss: 1.341 | Train Acc: 50.600% (4890/9664)\n",
      "180 234 Train Loss: 1.325 | Train Acc: 51.295% (5942/11584)\n",
      "210 234 Train Loss: 1.312 | Train Acc: 51.859% (7003/13504)\n",
      "234 Epoch: 2 | Train Loss: 1.299 | Train Acc: 52.350% (7840/14976)\n",
      "0 234 Test Loss: 1.235 | Test Acc: 53.125% (34/64)\n",
      "30 234 Test Loss: 1.348 | Test Acc: 50.958% (1011/1984)\n",
      "60 234 Test Loss: 1.353 | Test Acc: 51.230% (2000/3904)\n",
      "90 234 Test Loss: 1.362 | Test Acc: 51.065% (2974/5824)\n",
      "120 234 Test Loss: 1.374 | Test Acc: 50.671% (3924/7744)\n",
      "150 234 Test Loss: 1.369 | Test Acc: 51.024% (4931/9664)\n",
      "180 234 Test Loss: 1.369 | Test Acc: 50.855% (5891/11584)\n",
      "210 234 Test Loss: 1.364 | Test Acc: 51.007% (6888/13504)\n",
      "234 Epoch: 2 | Test Loss: 1.360 | Test Acc: 51.122% (7656/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "0 234 Train Loss: 1.016 | Train Acc: 67.188% (43/64)\n",
      "30 234 Train Loss: 1.069 | Train Acc: 61.341% (1217/1984)\n",
      "60 234 Train Loss: 1.073 | Train Acc: 60.912% (2378/3904)\n",
      "90 234 Train Loss: 1.085 | Train Acc: 60.560% (3527/5824)\n",
      "120 234 Train Loss: 1.068 | Train Acc: 61.428% (4757/7744)\n",
      "150 234 Train Loss: 1.061 | Train Acc: 61.662% (5959/9664)\n",
      "180 234 Train Loss: 1.052 | Train Acc: 62.137% (7198/11584)\n",
      "210 234 Train Loss: 1.047 | Train Acc: 62.204% (8400/13504)\n",
      "234 Epoch: 3 | Train Loss: 1.036 | Train Acc: 62.654% (9383/14976)\n",
      "0 234 Test Loss: 1.095 | Test Acc: 59.375% (38/64)\n",
      "30 234 Test Loss: 1.234 | Test Acc: 56.502% (1121/1984)\n",
      "60 234 Test Loss: 1.269 | Test Acc: 55.789% (2178/3904)\n",
      "90 234 Test Loss: 1.276 | Test Acc: 55.529% (3234/5824)\n",
      "120 234 Test Loss: 1.266 | Test Acc: 55.888% (4328/7744)\n",
      "150 234 Test Loss: 1.263 | Test Acc: 55.846% (5397/9664)\n",
      "180 234 Test Loss: 1.266 | Test Acc: 55.905% (6476/11584)\n",
      "210 234 Test Loss: 1.265 | Test Acc: 55.961% (7557/13504)\n",
      "234 Epoch: 3 | Test Loss: 1.275 | Test Acc: 55.662% (8336/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "0 234 Train Loss: 0.865 | Train Acc: 65.625% (42/64)\n",
      "30 234 Train Loss: 0.845 | Train Acc: 69.960% (1388/1984)\n",
      "60 234 Train Loss: 0.869 | Train Acc: 68.827% (2687/3904)\n",
      "90 234 Train Loss: 0.879 | Train Acc: 68.029% (3962/5824)\n",
      "120 234 Train Loss: 0.869 | Train Acc: 68.660% (5317/7744)\n",
      "150 234 Train Loss: 0.879 | Train Acc: 68.243% (6595/9664)\n",
      "180 234 Train Loss: 0.877 | Train Acc: 68.439% (7928/11584)\n",
      "210 234 Train Loss: 0.877 | Train Acc: 68.469% (9246/13504)\n",
      "234 Epoch: 4 | Train Loss: 0.872 | Train Acc: 68.596% (10273/14976)\n",
      "0 234 Test Loss: 0.623 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 1.014 | Test Acc: 63.357% (1257/1984)\n",
      "60 234 Test Loss: 1.028 | Test Acc: 63.653% (2485/3904)\n",
      "90 234 Test Loss: 1.033 | Test Acc: 63.462% (3696/5824)\n",
      "120 234 Test Loss: 1.034 | Test Acc: 63.533% (4920/7744)\n",
      "150 234 Test Loss: 1.026 | Test Acc: 63.845% (6170/9664)\n",
      "180 234 Test Loss: 1.027 | Test Acc: 63.968% (7410/11584)\n",
      "210 234 Test Loss: 1.027 | Test Acc: 63.974% (8639/13504)\n",
      "234 Epoch: 4 | Test Loss: 1.027 | Test Acc: 64.022% (9588/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      "0 234 Train Loss: 0.587 | Train Acc: 81.250% (52/64)\n",
      "30 234 Train Loss: 0.705 | Train Acc: 74.950% (1487/1984)\n",
      "60 234 Train Loss: 0.695 | Train Acc: 75.692% (2955/3904)\n",
      "90 234 Train Loss: 0.684 | Train Acc: 75.876% (4419/5824)\n",
      "120 234 Train Loss: 0.692 | Train Acc: 75.671% (5860/7744)\n",
      "150 234 Train Loss: 0.703 | Train Acc: 75.248% (7272/9664)\n",
      "180 234 Train Loss: 0.705 | Train Acc: 75.345% (8728/11584)\n",
      "210 234 Train Loss: 0.702 | Train Acc: 75.496% (10195/13504)\n",
      "234 Epoch: 5 | Train Loss: 0.703 | Train Acc: 75.454% (11300/14976)\n",
      "0 234 Test Loss: 1.051 | Test Acc: 67.188% (43/64)\n",
      "30 234 Test Loss: 1.082 | Test Acc: 64.365% (1277/1984)\n",
      "60 234 Test Loss: 1.126 | Test Acc: 62.526% (2441/3904)\n",
      "90 234 Test Loss: 1.125 | Test Acc: 62.517% (3641/5824)\n",
      "120 234 Test Loss: 1.126 | Test Acc: 62.474% (4838/7744)\n",
      "150 234 Test Loss: 1.135 | Test Acc: 62.231% (6014/9664)\n",
      "180 234 Test Loss: 1.139 | Test Acc: 62.120% (7196/11584)\n",
      "210 234 Test Loss: 1.131 | Test Acc: 62.263% (8408/13504)\n",
      "234 Epoch: 5 | Test Loss: 1.134 | Test Acc: 62.346% (9337/14976)\n",
      "\n",
      "Epoch: 6\n",
      "0 234 Train Loss: 0.501 | Train Acc: 84.375% (54/64)\n",
      "30 234 Train Loss: 0.538 | Train Acc: 81.502% (1617/1984)\n",
      "60 234 Train Loss: 0.539 | Train Acc: 81.250% (3172/3904)\n",
      "90 234 Train Loss: 0.530 | Train Acc: 81.576% (4751/5824)\n",
      "120 234 Train Loss: 0.548 | Train Acc: 80.759% (6254/7744)\n",
      "150 234 Train Loss: 0.561 | Train Acc: 80.174% (7748/9664)\n",
      "180 234 Train Loss: 0.561 | Train Acc: 80.171% (9287/11584)\n",
      "210 234 Train Loss: 0.569 | Train Acc: 79.836% (10781/13504)\n",
      "234 Epoch: 6 | Train Loss: 0.574 | Train Acc: 79.694% (11935/14976)\n",
      "0 234 Test Loss: 1.442 | Test Acc: 50.000% (32/64)\n",
      "30 234 Test Loss: 1.299 | Test Acc: 57.308% (1137/1984)\n",
      "60 234 Test Loss: 1.280 | Test Acc: 58.171% (2271/3904)\n",
      "90 234 Test Loss: 1.286 | Test Acc: 58.053% (3381/5824)\n",
      "120 234 Test Loss: 1.293 | Test Acc: 58.032% (4494/7744)\n",
      "150 234 Test Loss: 1.291 | Test Acc: 57.916% (5597/9664)\n",
      "180 234 Test Loss: 1.291 | Test Acc: 57.907% (6708/11584)\n",
      "210 234 Test Loss: 1.287 | Test Acc: 57.990% (7831/13504)\n",
      "234 Epoch: 6 | Test Loss: 1.286 | Test Acc: 58.093% (8700/14976)\n",
      "\n",
      "Epoch: 7\n",
      "0 234 Train Loss: 0.494 | Train Acc: 81.250% (52/64)\n",
      "30 234 Train Loss: 0.425 | Train Acc: 85.585% (1698/1984)\n",
      "60 234 Train Loss: 0.407 | Train Acc: 85.989% (3357/3904)\n",
      "90 234 Train Loss: 0.418 | Train Acc: 85.800% (4997/5824)\n",
      "120 234 Train Loss: 0.438 | Train Acc: 84.969% (6580/7744)\n",
      "150 234 Train Loss: 0.449 | Train Acc: 84.437% (8160/9664)\n",
      "180 234 Train Loss: 0.450 | Train Acc: 84.496% (9788/11584)\n",
      "210 234 Train Loss: 0.450 | Train Acc: 84.412% (11399/13504)\n",
      "234 Epoch: 7 | Train Loss: 0.460 | Train Acc: 84.075% (12591/14976)\n",
      "0 234 Test Loss: 0.904 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 0.896 | Test Acc: 70.766% (1404/1984)\n",
      "60 234 Test Loss: 0.917 | Test Acc: 70.261% (2743/3904)\n",
      "90 234 Test Loss: 0.942 | Test Acc: 69.694% (4059/5824)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 234 Test Loss: 0.944 | Test Acc: 69.757% (5402/7744)\n",
      "150 234 Test Loss: 0.935 | Test Acc: 69.899% (6755/9664)\n",
      "180 234 Test Loss: 0.935 | Test Acc: 69.890% (8096/11584)\n",
      "210 234 Test Loss: 0.947 | Test Acc: 69.772% (9422/13504)\n",
      "234 Epoch: 7 | Test Loss: 0.954 | Test Acc: 69.598% (10423/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      "0 234 Train Loss: 0.295 | Train Acc: 90.625% (58/64)\n",
      "30 234 Train Loss: 0.306 | Train Acc: 89.415% (1774/1984)\n",
      "60 234 Train Loss: 0.294 | Train Acc: 89.908% (3510/3904)\n",
      "90 234 Train Loss: 0.303 | Train Acc: 89.509% (5213/5824)\n",
      "120 234 Train Loss: 0.318 | Train Acc: 88.998% (6892/7744)\n",
      "150 234 Train Loss: 0.326 | Train Acc: 88.597% (8562/9664)\n",
      "180 234 Train Loss: 0.340 | Train Acc: 88.087% (10204/11584)\n",
      "210 234 Train Loss: 0.344 | Train Acc: 87.989% (11882/13504)\n",
      "234 Epoch: 8 | Train Loss: 0.348 | Train Acc: 87.901% (13164/14976)\n",
      "0 234 Test Loss: 0.757 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 1.108 | Test Acc: 66.986% (1329/1984)\n",
      "60 234 Test Loss: 1.063 | Test Acc: 67.264% (2626/3904)\n",
      "90 234 Test Loss: 1.052 | Test Acc: 67.857% (3952/5824)\n",
      "120 234 Test Loss: 1.046 | Test Acc: 68.272% (5287/7744)\n",
      "150 234 Test Loss: 1.046 | Test Acc: 68.212% (6592/9664)\n",
      "180 234 Test Loss: 1.034 | Test Acc: 68.569% (7943/11584)\n",
      "210 234 Test Loss: 1.036 | Test Acc: 68.506% (9251/13504)\n",
      "234 Epoch: 8 | Test Loss: 1.037 | Test Acc: 68.443% (10250/14976)\n",
      "\n",
      "Epoch: 9\n",
      "0 234 Train Loss: 0.326 | Train Acc: 93.750% (60/64)\n",
      "30 234 Train Loss: 0.238 | Train Acc: 92.288% (1831/1984)\n",
      "60 234 Train Loss: 0.232 | Train Acc: 92.546% (3613/3904)\n",
      "90 234 Train Loss: 0.235 | Train Acc: 92.376% (5380/5824)\n",
      "120 234 Train Loss: 0.238 | Train Acc: 92.020% (7126/7744)\n",
      "150 234 Train Loss: 0.246 | Train Acc: 91.774% (8869/9664)\n",
      "180 234 Train Loss: 0.251 | Train Acc: 91.531% (10603/11584)\n",
      "210 234 Train Loss: 0.260 | Train Acc: 91.114% (12304/13504)\n",
      "234 Epoch: 9 | Train Loss: 0.267 | Train Acc: 90.785% (13596/14976)\n",
      "0 234 Test Loss: 0.974 | Test Acc: 65.625% (42/64)\n",
      "30 234 Test Loss: 1.072 | Test Acc: 67.188% (1333/1984)\n",
      "60 234 Test Loss: 1.069 | Test Acc: 68.340% (2668/3904)\n",
      "90 234 Test Loss: 1.075 | Test Acc: 68.355% (3981/5824)\n",
      "120 234 Test Loss: 1.074 | Test Acc: 68.195% (5281/7744)\n",
      "150 234 Test Loss: 1.057 | Test Acc: 68.450% (6615/9664)\n",
      "180 234 Test Loss: 1.058 | Test Acc: 68.396% (7923/11584)\n",
      "210 234 Test Loss: 1.062 | Test Acc: 68.335% (9228/13504)\n",
      "234 Epoch: 9 | Test Loss: 1.061 | Test Acc: 68.309% (10230/14976)\n",
      "\n",
      "Epoch: 10\n",
      "0 234 Train Loss: 0.264 | Train Acc: 89.062% (57/64)\n",
      "30 234 Train Loss: 0.222 | Train Acc: 92.540% (1836/1984)\n",
      "60 234 Train Loss: 0.209 | Train Acc: 92.802% (3623/3904)\n",
      "90 234 Train Loss: 0.209 | Train Acc: 92.874% (5409/5824)\n",
      "120 234 Train Loss: 0.202 | Train Acc: 93.182% (7216/7744)\n",
      "150 234 Train Loss: 0.201 | Train Acc: 93.191% (9006/9664)\n",
      "180 234 Train Loss: 0.208 | Train Acc: 92.939% (10766/11584)\n",
      "210 234 Train Loss: 0.210 | Train Acc: 92.817% (12534/13504)\n",
      "234 Epoch: 10 | Train Loss: 0.212 | Train Acc: 92.688% (13881/14976)\n",
      "0 234 Test Loss: 1.120 | Test Acc: 65.625% (42/64)\n",
      "30 234 Test Loss: 1.127 | Test Acc: 68.498% (1359/1984)\n",
      "60 234 Test Loss: 1.141 | Test Acc: 68.468% (2673/3904)\n",
      "90 234 Test Loss: 1.154 | Test Acc: 68.407% (3984/5824)\n",
      "120 234 Test Loss: 1.155 | Test Acc: 68.388% (5296/7744)\n",
      "150 234 Test Loss: 1.146 | Test Acc: 68.367% (6607/9664)\n",
      "180 234 Test Loss: 1.136 | Test Acc: 68.776% (7967/11584)\n",
      "210 234 Test Loss: 1.122 | Test Acc: 69.105% (9332/13504)\n",
      "234 Epoch: 10 | Test Loss: 1.122 | Test Acc: 69.211% (10365/14976)\n",
      "\n",
      "Epoch: 11\n",
      "0 234 Train Loss: 0.174 | Train Acc: 95.312% (61/64)\n",
      "30 234 Train Loss: 0.196 | Train Acc: 93.196% (1849/1984)\n",
      "60 234 Train Loss: 0.191 | Train Acc: 93.596% (3654/3904)\n",
      "90 234 Train Loss: 0.178 | Train Acc: 93.990% (5474/5824)\n",
      "120 234 Train Loss: 0.171 | Train Acc: 94.292% (7302/7744)\n",
      "150 234 Train Loss: 0.167 | Train Acc: 94.329% (9116/9664)\n",
      "180 234 Train Loss: 0.163 | Train Acc: 94.458% (10942/11584)\n",
      "210 234 Train Loss: 0.164 | Train Acc: 94.342% (12740/13504)\n",
      "234 Epoch: 11 | Train Loss: 0.168 | Train Acc: 94.217% (14110/14976)\n",
      "0 234 Test Loss: 0.972 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.062 | Test Acc: 71.472% (1418/1984)\n",
      "60 234 Test Loss: 1.118 | Test Acc: 70.082% (2736/3904)\n",
      "90 234 Test Loss: 1.128 | Test Acc: 69.815% (4066/5824)\n",
      "120 234 Test Loss: 1.131 | Test Acc: 70.015% (5422/7744)\n",
      "150 234 Test Loss: 1.140 | Test Acc: 69.785% (6744/9664)\n",
      "180 234 Test Loss: 1.155 | Test Acc: 69.553% (8057/11584)\n",
      "210 234 Test Loss: 1.150 | Test Acc: 69.646% (9405/13504)\n",
      "234 Epoch: 11 | Test Loss: 1.146 | Test Acc: 69.658% (10432/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 12\n",
      "0 234 Train Loss: 0.069 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.124 | Train Acc: 95.968% (1904/1984)\n",
      "60 234 Train Loss: 0.122 | Train Acc: 95.978% (3747/3904)\n",
      "90 234 Train Loss: 0.124 | Train Acc: 95.913% (5586/5824)\n",
      "120 234 Train Loss: 0.123 | Train Acc: 95.984% (7433/7744)\n",
      "150 234 Train Loss: 0.128 | Train Acc: 95.737% (9252/9664)\n",
      "180 234 Train Loss: 0.128 | Train Acc: 95.779% (11095/11584)\n",
      "210 234 Train Loss: 0.128 | Train Acc: 95.786% (12935/13504)\n",
      "234 Epoch: 12 | Train Loss: 0.131 | Train Acc: 95.653% (14325/14976)\n",
      "0 234 Test Loss: 0.626 | Test Acc: 82.812% (53/64)\n",
      "30 234 Test Loss: 1.168 | Test Acc: 70.363% (1396/1984)\n",
      "60 234 Test Loss: 1.116 | Test Acc: 71.619% (2796/3904)\n",
      "90 234 Test Loss: 1.114 | Test Acc: 71.549% (4167/5824)\n",
      "120 234 Test Loss: 1.115 | Test Acc: 71.539% (5540/7744)\n",
      "150 234 Test Loss: 1.106 | Test Acc: 71.813% (6940/9664)\n",
      "180 234 Test Loss: 1.105 | Test Acc: 71.944% (8334/11584)\n",
      "210 234 Test Loss: 1.100 | Test Acc: 71.934% (9714/13504)\n",
      "234 Epoch: 12 | Test Loss: 1.099 | Test Acc: 72.009% (10784/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 13\n",
      "0 234 Train Loss: 0.129 | Train Acc: 95.312% (61/64)\n",
      "30 234 Train Loss: 0.106 | Train Acc: 95.968% (1904/1984)\n",
      "60 234 Train Loss: 0.096 | Train Acc: 96.644% (3773/3904)\n",
      "90 234 Train Loss: 0.098 | Train Acc: 96.755% (5635/5824)\n",
      "120 234 Train Loss: 0.096 | Train Acc: 96.707% (7489/7744)\n",
      "150 234 Train Loss: 0.102 | Train Acc: 96.461% (9322/9664)\n",
      "180 234 Train Loss: 0.112 | Train Acc: 96.055% (11127/11584)\n",
      "210 234 Train Loss: 0.119 | Train Acc: 95.898% (12950/13504)\n",
      "234 Epoch: 13 | Train Loss: 0.123 | Train Acc: 95.726% (14336/14976)\n",
      "0 234 Test Loss: 1.661 | Test Acc: 67.188% (43/64)\n",
      "30 234 Test Loss: 1.362 | Test Acc: 67.843% (1346/1984)\n",
      "60 234 Test Loss: 1.350 | Test Acc: 67.649% (2641/3904)\n",
      "90 234 Test Loss: 1.329 | Test Acc: 67.668% (3941/5824)\n",
      "120 234 Test Loss: 1.320 | Test Acc: 67.949% (5262/7744)\n",
      "150 234 Test Loss: 1.329 | Test Acc: 67.777% (6550/9664)\n",
      "180 234 Test Loss: 1.315 | Test Acc: 68.120% (7891/11584)\n",
      "210 234 Test Loss: 1.313 | Test Acc: 68.032% (9187/13504)\n",
      "234 Epoch: 13 | Test Loss: 1.309 | Test Acc: 68.069% (10194/14976)\n",
      "\n",
      "Epoch: 14\n",
      "0 234 Train Loss: 0.093 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.119 | Train Acc: 95.716% (1899/1984)\n",
      "60 234 Train Loss: 0.106 | Train Acc: 96.260% (3758/3904)\n",
      "90 234 Train Loss: 0.102 | Train Acc: 96.480% (5619/5824)\n",
      "120 234 Train Loss: 0.098 | Train Acc: 96.707% (7489/7744)\n",
      "150 234 Train Loss: 0.093 | Train Acc: 96.916% (9366/9664)\n",
      "180 234 Train Loss: 0.095 | Train Acc: 96.884% (11223/11584)\n",
      "210 234 Train Loss: 0.098 | Train Acc: 96.764% (13067/13504)\n",
      "234 Epoch: 14 | Train Loss: 0.101 | Train Acc: 96.621% (14470/14976)\n",
      "0 234 Test Loss: 0.753 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 0.972 | Test Acc: 74.042% (1469/1984)\n",
      "60 234 Test Loss: 0.999 | Test Acc: 74.052% (2891/3904)\n",
      "90 234 Test Loss: 1.031 | Test Acc: 73.403% (4275/5824)\n",
      "120 234 Test Loss: 1.017 | Test Acc: 73.592% (5699/7744)\n",
      "150 234 Test Loss: 1.026 | Test Acc: 73.438% (7097/9664)\n",
      "180 234 Test Loss: 1.031 | Test Acc: 73.386% (8501/11584)\n",
      "210 234 Test Loss: 1.041 | Test Acc: 73.289% (9897/13504)\n",
      "234 Epoch: 14 | Test Loss: 1.040 | Test Acc: 73.311% (10979/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 15\n",
      "0 234 Train Loss: 0.086 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.070 | Train Acc: 97.581% (1936/1984)\n",
      "60 234 Train Loss: 0.071 | Train Acc: 97.643% (3812/3904)\n",
      "90 234 Train Loss: 0.069 | Train Acc: 97.785% (5695/5824)\n",
      "120 234 Train Loss: 0.064 | Train Acc: 97.973% (7587/7744)\n",
      "150 234 Train Loss: 0.065 | Train Acc: 97.899% (9461/9664)\n",
      "180 234 Train Loss: 0.065 | Train Acc: 97.825% (11332/11584)\n",
      "210 234 Train Loss: 0.069 | Train Acc: 97.764% (13202/13504)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234 Epoch: 15 | Train Loss: 0.071 | Train Acc: 97.670% (14627/14976)\n",
      "0 234 Test Loss: 1.171 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.148 | Test Acc: 71.724% (1423/1984)\n",
      "60 234 Test Loss: 1.115 | Test Acc: 72.182% (2818/3904)\n",
      "90 234 Test Loss: 1.128 | Test Acc: 71.738% (4178/5824)\n",
      "120 234 Test Loss: 1.148 | Test Acc: 71.578% (5543/7744)\n",
      "150 234 Test Loss: 1.136 | Test Acc: 71.678% (6927/9664)\n",
      "180 234 Test Loss: 1.137 | Test Acc: 71.530% (8286/11584)\n",
      "210 234 Test Loss: 1.125 | Test Acc: 71.816% (9698/13504)\n",
      "234 Epoch: 15 | Test Loss: 1.122 | Test Acc: 71.908% (10769/14976)\n",
      "\n",
      "Epoch: 16\n",
      "0 234 Train Loss: 0.137 | Train Acc: 95.312% (61/64)\n",
      "30 234 Train Loss: 0.052 | Train Acc: 98.488% (1954/1984)\n",
      "60 234 Train Loss: 0.051 | Train Acc: 98.463% (3844/3904)\n",
      "90 234 Train Loss: 0.054 | Train Acc: 98.317% (5726/5824)\n",
      "120 234 Train Loss: 0.053 | Train Acc: 98.283% (7611/7744)\n",
      "150 234 Train Loss: 0.051 | Train Acc: 98.334% (9503/9664)\n",
      "180 234 Train Loss: 0.052 | Train Acc: 98.317% (11389/11584)\n",
      "210 234 Train Loss: 0.055 | Train Acc: 98.178% (13258/13504)\n",
      "234 Epoch: 16 | Train Loss: 0.057 | Train Acc: 98.150% (14699/14976)\n",
      "0 234 Test Loss: 1.387 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.092 | Test Acc: 72.127% (1431/1984)\n",
      "60 234 Test Loss: 1.143 | Test Acc: 71.773% (2802/3904)\n",
      "90 234 Test Loss: 1.152 | Test Acc: 71.583% (4169/5824)\n",
      "120 234 Test Loss: 1.154 | Test Acc: 71.746% (5556/7744)\n",
      "150 234 Test Loss: 1.148 | Test Acc: 71.885% (6947/9664)\n",
      "180 234 Test Loss: 1.148 | Test Acc: 71.935% (8333/11584)\n",
      "210 234 Test Loss: 1.134 | Test Acc: 72.305% (9764/13504)\n",
      "234 Epoch: 16 | Test Loss: 1.130 | Test Acc: 72.456% (10851/14976)\n",
      "\n",
      "Epoch: 17\n",
      "0 234 Train Loss: 0.031 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.074 | Train Acc: 97.530% (1935/1984)\n",
      "60 234 Train Loss: 0.072 | Train Acc: 97.618% (3811/3904)\n",
      "90 234 Train Loss: 0.074 | Train Acc: 97.648% (5687/5824)\n",
      "120 234 Train Loss: 0.072 | Train Acc: 97.650% (7562/7744)\n",
      "150 234 Train Loss: 0.072 | Train Acc: 97.734% (9445/9664)\n",
      "180 234 Train Loss: 0.070 | Train Acc: 97.790% (11328/11584)\n",
      "210 234 Train Loss: 0.069 | Train Acc: 97.830% (13211/13504)\n",
      "234 Epoch: 17 | Train Loss: 0.069 | Train Acc: 97.783% (14644/14976)\n",
      "0 234 Test Loss: 1.365 | Test Acc: 65.625% (42/64)\n",
      "30 234 Test Loss: 1.151 | Test Acc: 71.421% (1417/1984)\n",
      "60 234 Test Loss: 1.145 | Test Acc: 71.926% (2808/3904)\n",
      "90 234 Test Loss: 1.076 | Test Acc: 73.197% (4263/5824)\n",
      "120 234 Test Loss: 1.070 | Test Acc: 73.425% (5686/7744)\n",
      "150 234 Test Loss: 1.071 | Test Acc: 73.665% (7119/9664)\n",
      "180 234 Test Loss: 1.072 | Test Acc: 73.602% (8526/11584)\n",
      "210 234 Test Loss: 1.075 | Test Acc: 73.726% (9956/13504)\n",
      "234 Epoch: 17 | Test Loss: 1.075 | Test Acc: 73.698% (11037/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 18\n",
      "0 234 Train Loss: 0.072 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.085 | Train Acc: 97.429% (1933/1984)\n",
      "60 234 Train Loss: 0.075 | Train Acc: 97.643% (3812/3904)\n",
      "90 234 Train Loss: 0.067 | Train Acc: 97.837% (5698/5824)\n",
      "120 234 Train Loss: 0.063 | Train Acc: 97.934% (7584/7744)\n",
      "150 234 Train Loss: 0.064 | Train Acc: 97.951% (9466/9664)\n",
      "180 234 Train Loss: 0.066 | Train Acc: 97.894% (11340/11584)\n",
      "210 234 Train Loss: 0.065 | Train Acc: 97.912% (13222/13504)\n",
      "234 Epoch: 18 | Train Loss: 0.064 | Train Acc: 97.930% (14666/14976)\n",
      "0 234 Test Loss: 0.795 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 1.111 | Test Acc: 72.732% (1443/1984)\n",
      "60 234 Test Loss: 1.095 | Test Acc: 73.233% (2859/3904)\n",
      "90 234 Test Loss: 1.094 | Test Acc: 73.386% (4274/5824)\n",
      "120 234 Test Loss: 1.109 | Test Acc: 72.753% (5634/7744)\n",
      "150 234 Test Loss: 1.093 | Test Acc: 73.024% (7057/9664)\n",
      "180 234 Test Loss: 1.093 | Test Acc: 73.032% (8460/11584)\n",
      "210 234 Test Loss: 1.078 | Test Acc: 73.186% (9883/13504)\n",
      "234 Epoch: 18 | Test Loss: 1.077 | Test Acc: 73.210% (10964/14976)\n",
      "\n",
      "Epoch: 19\n",
      "0 234 Train Loss: 0.010 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.034 | Train Acc: 98.891% (1962/1984)\n",
      "60 234 Train Loss: 0.031 | Train Acc: 99.052% (3867/3904)\n",
      "90 234 Train Loss: 0.031 | Train Acc: 99.038% (5768/5824)\n",
      "120 234 Train Loss: 0.031 | Train Acc: 99.109% (7675/7744)\n",
      "150 234 Train Loss: 0.031 | Train Acc: 99.100% (9577/9664)\n",
      "180 234 Train Loss: 0.031 | Train Acc: 99.111% (11481/11584)\n",
      "210 234 Train Loss: 0.032 | Train Acc: 99.089% (13381/13504)\n",
      "234 Epoch: 19 | Train Loss: 0.034 | Train Acc: 98.985% (14824/14976)\n",
      "0 234 Test Loss: 1.406 | Test Acc: 65.625% (42/64)\n",
      "30 234 Test Loss: 1.167 | Test Acc: 73.438% (1457/1984)\n",
      "60 234 Test Loss: 1.154 | Test Acc: 73.540% (2871/3904)\n",
      "90 234 Test Loss: 1.150 | Test Acc: 73.163% (4261/5824)\n",
      "120 234 Test Loss: 1.156 | Test Acc: 73.140% (5664/7744)\n",
      "150 234 Test Loss: 1.150 | Test Acc: 73.106% (7065/9664)\n",
      "180 234 Test Loss: 1.146 | Test Acc: 73.040% (8461/11584)\n",
      "210 234 Test Loss: 1.141 | Test Acc: 72.949% (9851/13504)\n",
      "234 Epoch: 19 | Test Loss: 1.146 | Test Acc: 72.796% (10902/14976)\n",
      "\n",
      "Epoch: 20\n",
      "0 234 Train Loss: 0.038 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.034 | Train Acc: 98.841% (1961/1984)\n",
      "60 234 Train Loss: 0.033 | Train Acc: 98.950% (3863/3904)\n",
      "90 234 Train Loss: 0.035 | Train Acc: 98.867% (5758/5824)\n",
      "120 234 Train Loss: 0.037 | Train Acc: 98.773% (7649/7744)\n",
      "150 234 Train Loss: 0.039 | Train Acc: 98.727% (9541/9664)\n",
      "180 234 Train Loss: 0.041 | Train Acc: 98.653% (11428/11584)\n",
      "210 234 Train Loss: 0.043 | Train Acc: 98.623% (13318/13504)\n",
      "234 Epoch: 20 | Train Loss: 0.044 | Train Acc: 98.551% (14759/14976)\n",
      "0 234 Test Loss: 1.347 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 1.016 | Test Acc: 74.950% (1487/1984)\n",
      "60 234 Test Loss: 1.072 | Test Acc: 74.283% (2900/3904)\n",
      "90 234 Test Loss: 1.061 | Test Acc: 74.245% (4324/5824)\n",
      "120 234 Test Loss: 1.064 | Test Acc: 74.277% (5752/7744)\n",
      "150 234 Test Loss: 1.067 | Test Acc: 73.913% (7143/9664)\n",
      "180 234 Test Loss: 1.072 | Test Acc: 73.688% (8536/11584)\n",
      "210 234 Test Loss: 1.065 | Test Acc: 73.956% (9987/13504)\n",
      "234 Epoch: 20 | Test Loss: 1.061 | Test Acc: 73.885% (11065/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 21\n",
      "0 234 Train Loss: 0.012 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.035 | Train Acc: 98.992% (1964/1984)\n",
      "60 234 Train Loss: 0.031 | Train Acc: 99.129% (3870/3904)\n",
      "90 234 Train Loss: 0.030 | Train Acc: 99.159% (5775/5824)\n",
      "120 234 Train Loss: 0.032 | Train Acc: 99.122% (7676/7744)\n",
      "150 234 Train Loss: 0.031 | Train Acc: 99.151% (9582/9664)\n",
      "180 234 Train Loss: 0.030 | Train Acc: 99.180% (11489/11584)\n",
      "210 234 Train Loss: 0.030 | Train Acc: 99.119% (13385/13504)\n",
      "234 Epoch: 21 | Train Loss: 0.033 | Train Acc: 99.045% (14833/14976)\n",
      "0 234 Test Loss: 1.692 | Test Acc: 67.188% (43/64)\n",
      "30 234 Test Loss: 1.115 | Test Acc: 73.337% (1455/1984)\n",
      "60 234 Test Loss: 1.148 | Test Acc: 72.259% (2821/3904)\n",
      "90 234 Test Loss: 1.145 | Test Acc: 72.545% (4225/5824)\n",
      "120 234 Test Loss: 1.110 | Test Acc: 73.063% (5658/7744)\n",
      "150 234 Test Loss: 1.117 | Test Acc: 73.055% (7060/9664)\n",
      "180 234 Test Loss: 1.117 | Test Acc: 72.971% (8453/11584)\n",
      "210 234 Test Loss: 1.131 | Test Acc: 72.734% (9822/13504)\n",
      "234 Epoch: 21 | Test Loss: 1.132 | Test Acc: 72.630% (10877/14976)\n",
      "\n",
      "Epoch: 22\n",
      "0 234 Train Loss: 0.016 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.040 | Train Acc: 98.740% (1959/1984)\n",
      "60 234 Train Loss: 0.036 | Train Acc: 99.001% (3865/3904)\n",
      "90 234 Train Loss: 0.032 | Train Acc: 99.159% (5775/5824)\n",
      "120 234 Train Loss: 0.032 | Train Acc: 99.057% (7671/7744)\n",
      "150 234 Train Loss: 0.031 | Train Acc: 99.131% (9580/9664)\n",
      "180 234 Train Loss: 0.029 | Train Acc: 99.171% (11488/11584)\n",
      "210 234 Train Loss: 0.028 | Train Acc: 99.200% (13396/13504)\n",
      "234 Epoch: 22 | Train Loss: 0.027 | Train Acc: 99.245% (14863/14976)\n",
      "0 234 Test Loss: 1.155 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 1.028 | Test Acc: 75.706% (1502/1984)\n",
      "60 234 Test Loss: 1.026 | Test Acc: 75.692% (2955/3904)\n",
      "90 234 Test Loss: 0.994 | Test Acc: 76.305% (4444/5824)\n",
      "120 234 Test Loss: 0.987 | Test Acc: 76.666% (5937/7744)\n",
      "150 234 Test Loss: 0.996 | Test Acc: 76.376% (7381/9664)\n",
      "180 234 Test Loss: 1.014 | Test Acc: 75.958% (8799/11584)\n",
      "210 234 Test Loss: 1.009 | Test Acc: 75.926% (10253/13504)\n",
      "234 Epoch: 22 | Test Loss: 1.011 | Test Acc: 75.928% (11371/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 23\n",
      "0 234 Train Loss: 0.008 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.022 | Train Acc: 99.294% (1970/1984)\n",
      "60 234 Train Loss: 0.020 | Train Acc: 99.411% (3881/3904)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 234 Train Loss: 0.020 | Train Acc: 99.382% (5788/5824)\n",
      "120 234 Train Loss: 0.019 | Train Acc: 99.432% (7700/7744)\n",
      "150 234 Train Loss: 0.021 | Train Acc: 99.317% (9598/9664)\n",
      "180 234 Train Loss: 0.022 | Train Acc: 99.292% (11502/11584)\n",
      "210 234 Train Loss: 0.025 | Train Acc: 99.215% (13398/13504)\n",
      "234 Epoch: 23 | Train Loss: 0.025 | Train Acc: 99.192% (14855/14976)\n",
      "0 234 Test Loss: 1.065 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 1.053 | Test Acc: 75.252% (1493/1984)\n",
      "60 234 Test Loss: 1.073 | Test Acc: 75.615% (2952/3904)\n",
      "90 234 Test Loss: 1.083 | Test Acc: 74.948% (4365/5824)\n",
      "120 234 Test Loss: 1.088 | Test Acc: 74.755% (5789/7744)\n",
      "150 234 Test Loss: 1.086 | Test Acc: 74.648% (7214/9664)\n",
      "180 234 Test Loss: 1.078 | Test Acc: 74.853% (8671/11584)\n",
      "210 234 Test Loss: 1.089 | Test Acc: 74.674% (10084/13504)\n",
      "234 Epoch: 23 | Test Loss: 1.075 | Test Acc: 74.846% (11209/14976)\n",
      "\n",
      "Epoch: 24\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.025 | Train Acc: 99.345% (1971/1984)\n",
      "60 234 Train Loss: 0.028 | Train Acc: 99.232% (3874/3904)\n",
      "90 234 Train Loss: 0.030 | Train Acc: 99.227% (5779/5824)\n",
      "120 234 Train Loss: 0.028 | Train Acc: 99.238% (7685/7744)\n",
      "150 234 Train Loss: 0.029 | Train Acc: 99.214% (9588/9664)\n",
      "180 234 Train Loss: 0.029 | Train Acc: 99.189% (11490/11584)\n",
      "210 234 Train Loss: 0.030 | Train Acc: 99.148% (13389/13504)\n",
      "234 Epoch: 24 | Train Loss: 0.031 | Train Acc: 99.105% (14842/14976)\n",
      "0 234 Test Loss: 0.725 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 1.017 | Test Acc: 74.345% (1475/1984)\n",
      "60 234 Test Loss: 1.047 | Test Acc: 74.693% (2916/3904)\n",
      "90 234 Test Loss: 1.037 | Test Acc: 74.966% (4366/5824)\n",
      "120 234 Test Loss: 1.039 | Test Acc: 75.155% (5820/7744)\n",
      "150 234 Test Loss: 1.037 | Test Acc: 75.166% (7264/9664)\n",
      "180 234 Test Loss: 1.036 | Test Acc: 75.224% (8714/11584)\n",
      "210 234 Test Loss: 1.025 | Test Acc: 75.289% (10167/13504)\n",
      "234 Epoch: 24 | Test Loss: 1.021 | Test Acc: 75.374% (11288/14976)\n",
      "\n",
      "Epoch: 25\n",
      "0 234 Train Loss: 0.023 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.024 | Train Acc: 99.345% (1971/1984)\n",
      "60 234 Train Loss: 0.028 | Train Acc: 99.155% (3871/3904)\n",
      "90 234 Train Loss: 0.031 | Train Acc: 99.056% (5769/5824)\n",
      "120 234 Train Loss: 0.031 | Train Acc: 99.070% (7672/7744)\n",
      "150 234 Train Loss: 0.031 | Train Acc: 99.069% (9574/9664)\n",
      "180 234 Train Loss: 0.032 | Train Acc: 99.042% (11473/11584)\n",
      "210 234 Train Loss: 0.032 | Train Acc: 99.023% (13372/13504)\n",
      "234 Epoch: 25 | Train Loss: 0.033 | Train Acc: 98.965% (14821/14976)\n",
      "0 234 Test Loss: 1.517 | Test Acc: 67.188% (43/64)\n",
      "30 234 Test Loss: 1.237 | Test Acc: 71.925% (1427/1984)\n",
      "60 234 Test Loss: 1.206 | Test Acc: 71.901% (2807/3904)\n",
      "90 234 Test Loss: 1.212 | Test Acc: 71.652% (4173/5824)\n",
      "120 234 Test Loss: 1.206 | Test Acc: 71.849% (5564/7744)\n",
      "150 234 Test Loss: 1.190 | Test Acc: 72.041% (6962/9664)\n",
      "180 234 Test Loss: 1.185 | Test Acc: 72.307% (8376/11584)\n",
      "210 234 Test Loss: 1.173 | Test Acc: 72.490% (9789/13504)\n",
      "234 Epoch: 25 | Test Loss: 1.172 | Test Acc: 72.449% (10850/14976)\n",
      "\n",
      "Epoch: 26\n",
      "0 234 Train Loss: 0.012 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.040 | Train Acc: 98.690% (1958/1984)\n",
      "60 234 Train Loss: 0.036 | Train Acc: 98.975% (3864/3904)\n",
      "90 234 Train Loss: 0.034 | Train Acc: 99.021% (5767/5824)\n",
      "120 234 Train Loss: 0.035 | Train Acc: 98.993% (7666/7744)\n",
      "150 234 Train Loss: 0.038 | Train Acc: 98.820% (9550/9664)\n",
      "180 234 Train Loss: 0.040 | Train Acc: 98.731% (11437/11584)\n",
      "210 234 Train Loss: 0.039 | Train Acc: 98.793% (13341/13504)\n",
      "234 Epoch: 26 | Train Loss: 0.040 | Train Acc: 98.738% (14787/14976)\n",
      "0 234 Test Loss: 0.868 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 1.065 | Test Acc: 74.899% (1486/1984)\n",
      "60 234 Test Loss: 1.074 | Test Acc: 74.590% (2912/3904)\n",
      "90 234 Test Loss: 1.110 | Test Acc: 73.867% (4302/5824)\n",
      "120 234 Test Loss: 1.127 | Test Acc: 73.631% (5702/7744)\n",
      "150 234 Test Loss: 1.110 | Test Acc: 73.851% (7137/9664)\n",
      "180 234 Test Loss: 1.105 | Test Acc: 73.748% (8543/11584)\n",
      "210 234 Test Loss: 1.098 | Test Acc: 73.593% (9938/13504)\n",
      "234 Epoch: 26 | Test Loss: 1.088 | Test Acc: 73.858% (11061/14976)\n",
      "\n",
      "Epoch: 27\n",
      "0 234 Train Loss: 0.058 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.049 | Train Acc: 98.337% (1951/1984)\n",
      "60 234 Train Loss: 0.046 | Train Acc: 98.591% (3849/3904)\n",
      "90 234 Train Loss: 0.045 | Train Acc: 98.575% (5741/5824)\n",
      "120 234 Train Loss: 0.044 | Train Acc: 98.644% (7639/7744)\n",
      "150 234 Train Loss: 0.046 | Train Acc: 98.531% (9522/9664)\n",
      "180 234 Train Loss: 0.046 | Train Acc: 98.550% (11416/11584)\n",
      "210 234 Train Loss: 0.046 | Train Acc: 98.534% (13306/13504)\n",
      "234 Epoch: 27 | Train Loss: 0.046 | Train Acc: 98.491% (14750/14976)\n",
      "0 234 Test Loss: 0.799 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 1.142 | Test Acc: 73.337% (1455/1984)\n",
      "60 234 Test Loss: 1.153 | Test Acc: 72.797% (2842/3904)\n",
      "90 234 Test Loss: 1.113 | Test Acc: 73.935% (4306/5824)\n",
      "120 234 Test Loss: 1.123 | Test Acc: 73.786% (5714/7744)\n",
      "150 234 Test Loss: 1.131 | Test Acc: 73.634% (7116/9664)\n",
      "180 234 Test Loss: 1.127 | Test Acc: 73.602% (8526/11584)\n",
      "210 234 Test Loss: 1.112 | Test Acc: 73.889% (9978/13504)\n",
      "234 Epoch: 27 | Test Loss: 1.115 | Test Acc: 73.731% (11042/14976)\n",
      "\n",
      "Epoch: 28\n",
      "0 234 Train Loss: 0.017 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.033 | Train Acc: 99.042% (1965/1984)\n",
      "60 234 Train Loss: 0.033 | Train Acc: 99.129% (3870/3904)\n",
      "90 234 Train Loss: 0.034 | Train Acc: 99.004% (5766/5824)\n",
      "120 234 Train Loss: 0.034 | Train Acc: 99.070% (7672/7744)\n",
      "150 234 Train Loss: 0.034 | Train Acc: 99.058% (9573/9664)\n",
      "180 234 Train Loss: 0.035 | Train Acc: 99.016% (11470/11584)\n",
      "210 234 Train Loss: 0.035 | Train Acc: 98.985% (13367/13504)\n",
      "234 Epoch: 28 | Train Loss: 0.035 | Train Acc: 98.978% (14823/14976)\n",
      "0 234 Test Loss: 1.215 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 1.254 | Test Acc: 71.018% (1409/1984)\n",
      "60 234 Test Loss: 1.248 | Test Acc: 71.209% (2780/3904)\n",
      "90 234 Test Loss: 1.257 | Test Acc: 71.188% (4146/5824)\n",
      "120 234 Test Loss: 1.293 | Test Acc: 70.700% (5475/7744)\n",
      "150 234 Test Loss: 1.280 | Test Acc: 70.954% (6857/9664)\n",
      "180 234 Test Loss: 1.274 | Test Acc: 71.072% (8233/11584)\n",
      "210 234 Test Loss: 1.275 | Test Acc: 71.068% (9597/13504)\n",
      "234 Epoch: 28 | Test Loss: 1.277 | Test Acc: 70.880% (10615/14976)\n",
      "\n",
      "Epoch: 29\n",
      "0 234 Train Loss: 0.085 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.033 | Train Acc: 98.690% (1958/1984)\n",
      "60 234 Train Loss: 0.033 | Train Acc: 98.719% (3854/3904)\n",
      "90 234 Train Loss: 0.032 | Train Acc: 98.850% (5757/5824)\n",
      "120 234 Train Loss: 0.034 | Train Acc: 98.773% (7649/7744)\n",
      "150 234 Train Loss: 0.035 | Train Acc: 98.789% (9547/9664)\n",
      "180 234 Train Loss: 0.036 | Train Acc: 98.791% (11444/11584)\n",
      "210 234 Train Loss: 0.036 | Train Acc: 98.771% (13338/13504)\n",
      "234 Epoch: 29 | Train Loss: 0.036 | Train Acc: 98.798% (14796/14976)\n",
      "0 234 Test Loss: 0.932 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 1.040 | Test Acc: 74.798% (1484/1984)\n",
      "60 234 Test Loss: 1.055 | Test Acc: 74.590% (2912/3904)\n",
      "90 234 Test Loss: 1.062 | Test Acc: 74.433% (4335/5824)\n",
      "120 234 Test Loss: 1.051 | Test Acc: 74.780% (5791/7744)\n",
      "150 234 Test Loss: 1.051 | Test Acc: 74.886% (7237/9664)\n",
      "180 234 Test Loss: 1.060 | Test Acc: 74.689% (8652/11584)\n",
      "210 234 Test Loss: 1.075 | Test Acc: 74.467% (10056/13504)\n",
      "234 Epoch: 29 | Test Loss: 1.072 | Test Acc: 74.493% (11156/14976)\n",
      "\n",
      "Epoch: 30\n",
      "0 234 Train Loss: 0.049 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.027 | Train Acc: 98.992% (1964/1984)\n",
      "60 234 Train Loss: 0.026 | Train Acc: 99.078% (3868/3904)\n",
      "90 234 Train Loss: 0.023 | Train Acc: 99.296% (5783/5824)\n",
      "120 234 Train Loss: 0.021 | Train Acc: 99.406% (7698/7744)\n",
      "150 234 Train Loss: 0.020 | Train Acc: 99.431% (9609/9664)\n",
      "180 234 Train Loss: 0.019 | Train Acc: 99.422% (11517/11584)\n",
      "210 234 Train Loss: 0.019 | Train Acc: 99.415% (13425/13504)\n",
      "234 Epoch: 30 | Train Loss: 0.020 | Train Acc: 99.399% (14886/14976)\n",
      "0 234 Test Loss: 0.867 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 1.107 | Test Acc: 73.992% (1468/1984)\n",
      "60 234 Test Loss: 1.105 | Test Acc: 73.796% (2881/3904)\n",
      "90 234 Test Loss: 1.063 | Test Acc: 74.536% (4341/5824)\n",
      "120 234 Test Loss: 1.067 | Test Acc: 74.471% (5767/7744)\n",
      "150 234 Test Loss: 1.081 | Test Acc: 74.172% (7168/9664)\n",
      "180 234 Test Loss: 1.083 | Test Acc: 74.258% (8602/11584)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 234 Test Loss: 1.074 | Test Acc: 74.415% (10049/13504)\n",
      "234 Epoch: 30 | Test Loss: 1.078 | Test Acc: 74.459% (11151/14976)\n",
      "\n",
      "Epoch: 31\n",
      "0 234 Train Loss: 0.089 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.022 | Train Acc: 99.446% (1973/1984)\n",
      "60 234 Train Loss: 0.021 | Train Acc: 99.436% (3882/3904)\n",
      "90 234 Train Loss: 0.021 | Train Acc: 99.416% (5790/5824)\n",
      "120 234 Train Loss: 0.022 | Train Acc: 99.354% (7694/7744)\n",
      "150 234 Train Loss: 0.021 | Train Acc: 99.379% (9604/9664)\n",
      "180 234 Train Loss: 0.020 | Train Acc: 99.430% (11518/11584)\n",
      "210 234 Train Loss: 0.021 | Train Acc: 99.430% (13427/13504)\n",
      "234 Epoch: 31 | Train Loss: 0.020 | Train Acc: 99.432% (14891/14976)\n",
      "0 234 Test Loss: 0.500 | Test Acc: 89.062% (57/64)\n",
      "30 234 Test Loss: 1.029 | Test Acc: 75.101% (1490/1984)\n",
      "60 234 Test Loss: 1.042 | Test Acc: 74.744% (2918/3904)\n",
      "90 234 Test Loss: 1.034 | Test Acc: 74.914% (4363/5824)\n",
      "120 234 Test Loss: 1.038 | Test Acc: 74.858% (5797/7744)\n",
      "150 234 Test Loss: 1.037 | Test Acc: 74.783% (7227/9664)\n",
      "180 234 Test Loss: 1.046 | Test Acc: 74.655% (8648/11584)\n",
      "210 234 Test Loss: 1.046 | Test Acc: 74.711% (10089/13504)\n",
      "234 Epoch: 31 | Test Loss: 1.050 | Test Acc: 74.653% (11180/14976)\n",
      "\n",
      "Epoch: 32\n",
      "0 234 Train Loss: 0.008 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.013 | Train Acc: 99.698% (1978/1984)\n",
      "60 234 Train Loss: 0.011 | Train Acc: 99.795% (3896/3904)\n",
      "90 234 Train Loss: 0.010 | Train Acc: 99.811% (5813/5824)\n",
      "120 234 Train Loss: 0.010 | Train Acc: 99.819% (7730/7744)\n",
      "150 234 Train Loss: 0.010 | Train Acc: 99.814% (9646/9664)\n",
      "180 234 Train Loss: 0.010 | Train Acc: 99.801% (11561/11584)\n",
      "210 234 Train Loss: 0.010 | Train Acc: 99.785% (13475/13504)\n",
      "234 Epoch: 32 | Train Loss: 0.010 | Train Acc: 99.773% (14942/14976)\n",
      "0 234 Test Loss: 0.970 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 1.034 | Test Acc: 74.849% (1485/1984)\n",
      "60 234 Test Loss: 1.048 | Test Acc: 73.899% (2885/3904)\n",
      "90 234 Test Loss: 1.039 | Test Acc: 74.519% (4340/5824)\n",
      "120 234 Test Loss: 1.036 | Test Acc: 74.716% (5786/7744)\n",
      "150 234 Test Loss: 1.047 | Test Acc: 74.741% (7223/9664)\n",
      "180 234 Test Loss: 1.062 | Test Acc: 74.586% (8640/11584)\n",
      "210 234 Test Loss: 1.056 | Test Acc: 74.645% (10080/13504)\n",
      "234 Epoch: 32 | Test Loss: 1.053 | Test Acc: 74.679% (11184/14976)\n",
      "\n",
      "Epoch: 33\n",
      "0 234 Train Loss: 0.010 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.014 | Train Acc: 99.597% (1976/1984)\n",
      "60 234 Train Loss: 0.013 | Train Acc: 99.667% (3891/3904)\n",
      "90 234 Train Loss: 0.013 | Train Acc: 99.639% (5803/5824)\n",
      "120 234 Train Loss: 0.014 | Train Acc: 99.613% (7714/7744)\n",
      "150 234 Train Loss: 0.014 | Train Acc: 99.638% (9629/9664)\n",
      "180 234 Train Loss: 0.014 | Train Acc: 99.603% (11538/11584)\n",
      "210 234 Train Loss: 0.014 | Train Acc: 99.608% (13451/13504)\n",
      "234 Epoch: 33 | Train Loss: 0.014 | Train Acc: 99.586% (14914/14976)\n",
      "0 234 Test Loss: 0.753 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 1.036 | Test Acc: 75.706% (1502/1984)\n",
      "60 234 Test Loss: 1.027 | Test Acc: 75.384% (2943/3904)\n",
      "90 234 Test Loss: 1.041 | Test Acc: 75.258% (4383/5824)\n",
      "120 234 Test Loss: 1.048 | Test Acc: 75.039% (5811/7744)\n",
      "150 234 Test Loss: 1.043 | Test Acc: 74.897% (7238/9664)\n",
      "180 234 Test Loss: 1.046 | Test Acc: 74.957% (8683/11584)\n",
      "210 234 Test Loss: 1.042 | Test Acc: 75.089% (10140/13504)\n",
      "234 Epoch: 33 | Test Loss: 1.045 | Test Acc: 75.047% (11239/14976)\n",
      "\n",
      "Epoch: 34\n",
      "0 234 Train Loss: 0.005 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.013 | Train Acc: 99.798% (1980/1984)\n",
      "60 234 Train Loss: 0.011 | Train Acc: 99.821% (3897/3904)\n",
      "90 234 Train Loss: 0.011 | Train Acc: 99.760% (5810/5824)\n",
      "120 234 Train Loss: 0.010 | Train Acc: 99.768% (7726/7744)\n",
      "150 234 Train Loss: 0.010 | Train Acc: 99.783% (9643/9664)\n",
      "180 234 Train Loss: 0.011 | Train Acc: 99.750% (11555/11584)\n",
      "210 234 Train Loss: 0.011 | Train Acc: 99.711% (13465/13504)\n",
      "234 Epoch: 34 | Train Loss: 0.012 | Train Acc: 99.686% (14929/14976)\n",
      "0 234 Test Loss: 0.999 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 0.965 | Test Acc: 75.907% (1506/1984)\n",
      "60 234 Test Loss: 1.005 | Test Acc: 75.487% (2947/3904)\n",
      "90 234 Test Loss: 1.025 | Test Acc: 75.343% (4388/5824)\n",
      "120 234 Test Loss: 1.024 | Test Acc: 75.594% (5854/7744)\n",
      "150 234 Test Loss: 1.017 | Test Acc: 75.786% (7324/9664)\n",
      "180 234 Test Loss: 1.023 | Test Acc: 75.544% (8751/11584)\n",
      "210 234 Test Loss: 1.016 | Test Acc: 75.526% (10199/13504)\n",
      "234 Epoch: 34 | Test Loss: 1.004 | Test Acc: 75.708% (11338/14976)\n",
      "\n",
      "Epoch: 35\n",
      "0 234 Train Loss: 0.005 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.010 | Train Acc: 99.899% (1982/1984)\n",
      "60 234 Train Loss: 0.010 | Train Acc: 99.846% (3898/3904)\n",
      "90 234 Train Loss: 0.008 | Train Acc: 99.863% (5816/5824)\n",
      "120 234 Train Loss: 0.008 | Train Acc: 99.858% (7733/7744)\n",
      "150 234 Train Loss: 0.009 | Train Acc: 99.824% (9647/9664)\n",
      "180 234 Train Loss: 0.009 | Train Acc: 99.836% (11565/11584)\n",
      "210 234 Train Loss: 0.009 | Train Acc: 99.822% (13480/13504)\n",
      "234 Epoch: 35 | Train Loss: 0.008 | Train Acc: 99.840% (14952/14976)\n",
      "0 234 Test Loss: 1.015 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 0.965 | Test Acc: 76.562% (1519/1984)\n",
      "60 234 Test Loss: 0.910 | Test Acc: 77.741% (3035/3904)\n",
      "90 234 Test Loss: 0.931 | Test Acc: 77.404% (4508/5824)\n",
      "120 234 Test Loss: 0.939 | Test Acc: 77.079% (5969/7744)\n",
      "150 234 Test Loss: 0.934 | Test Acc: 77.183% (7459/9664)\n",
      "180 234 Test Loss: 0.929 | Test Acc: 77.158% (8938/11584)\n",
      "210 234 Test Loss: 0.929 | Test Acc: 77.222% (10428/13504)\n",
      "234 Epoch: 35 | Test Loss: 0.933 | Test Acc: 77.130% (11551/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 36\n",
      "0 234 Train Loss: 0.007 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.004 | Train Acc: 99.950% (1983/1984)\n",
      "60 234 Train Loss: 0.004 | Train Acc: 99.974% (3903/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 99.983% (5823/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 99.961% (7741/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 99.959% (9660/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 99.965% (11580/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 99.970% (13500/13504)\n",
      "234 Epoch: 36 | Train Loss: 0.003 | Train Acc: 99.973% (14972/14976)\n",
      "0 234 Test Loss: 0.844 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 0.941 | Test Acc: 76.663% (1521/1984)\n",
      "60 234 Test Loss: 0.923 | Test Acc: 76.972% (3005/3904)\n",
      "90 234 Test Loss: 0.915 | Test Acc: 77.370% (4506/5824)\n",
      "120 234 Test Loss: 0.894 | Test Acc: 77.815% (6026/7744)\n",
      "150 234 Test Loss: 0.887 | Test Acc: 77.908% (7529/9664)\n",
      "180 234 Test Loss: 0.877 | Test Acc: 78.047% (9041/11584)\n",
      "210 234 Test Loss: 0.886 | Test Acc: 77.932% (10524/13504)\n",
      "234 Epoch: 36 | Test Loss: 0.891 | Test Acc: 77.825% (11655/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 37\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 99.987% (7743/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 99.990% (9663/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 99.991% (11583/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 99.993% (13503/13504)\n",
      "234 Epoch: 37 | Train Loss: 0.002 | Train Acc: 99.993% (14975/14976)\n",
      "0 234 Test Loss: 0.680 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 0.878 | Test Acc: 78.427% (1556/1984)\n",
      "60 234 Test Loss: 0.861 | Test Acc: 78.893% (3080/3904)\n",
      "90 234 Test Loss: 0.891 | Test Acc: 78.073% (4547/5824)\n",
      "120 234 Test Loss: 0.896 | Test Acc: 78.035% (6043/7744)\n",
      "150 234 Test Loss: 0.893 | Test Acc: 77.990% (7537/9664)\n",
      "180 234 Test Loss: 0.885 | Test Acc: 78.073% (9044/11584)\n",
      "210 234 Test Loss: 0.880 | Test Acc: 78.088% (10545/13504)\n",
      "234 Epoch: 37 | Test Loss: 0.873 | Test Acc: 78.305% (11727/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 38\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.001 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.001 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 38 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.811 | Test Acc: 73.438% (47/64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 234 Test Loss: 0.916 | Test Acc: 77.419% (1536/1984)\n",
      "60 234 Test Loss: 0.886 | Test Acc: 77.587% (3029/3904)\n",
      "90 234 Test Loss: 0.873 | Test Acc: 77.850% (4534/5824)\n",
      "120 234 Test Loss: 0.864 | Test Acc: 77.970% (6038/7744)\n",
      "150 234 Test Loss: 0.860 | Test Acc: 78.135% (7551/9664)\n",
      "180 234 Test Loss: 0.856 | Test Acc: 78.401% (9082/11584)\n",
      "210 234 Test Loss: 0.853 | Test Acc: 78.451% (10594/13504)\n",
      "234 Epoch: 38 | Test Loss: 0.852 | Test Acc: 78.439% (11747/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 39\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 39 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.729 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.804 | Test Acc: 79.083% (1569/1984)\n",
      "60 234 Test Loss: 0.824 | Test Acc: 78.714% (3073/3904)\n",
      "90 234 Test Loss: 0.844 | Test Acc: 78.365% (4564/5824)\n",
      "120 234 Test Loss: 0.847 | Test Acc: 78.383% (6070/7744)\n",
      "150 234 Test Loss: 0.864 | Test Acc: 78.063% (7544/9664)\n",
      "180 234 Test Loss: 0.855 | Test Acc: 78.263% (9066/11584)\n",
      "210 234 Test Loss: 0.836 | Test Acc: 78.510% (10602/13504)\n",
      "234 Epoch: 39 | Test Loss: 0.839 | Test Acc: 78.399% (11741/14976)\n",
      "\n",
      "Epoch: 40\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.001 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 40 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.740 | Test Acc: 82.812% (53/64)\n",
      "30 234 Test Loss: 0.803 | Test Acc: 78.780% (1563/1984)\n",
      "60 234 Test Loss: 0.814 | Test Acc: 78.765% (3075/3904)\n",
      "90 234 Test Loss: 0.840 | Test Acc: 78.039% (4545/5824)\n",
      "120 234 Test Loss: 0.839 | Test Acc: 77.828% (6027/7744)\n",
      "150 234 Test Loss: 0.826 | Test Acc: 78.260% (7563/9664)\n",
      "180 234 Test Loss: 0.825 | Test Acc: 78.332% (9074/11584)\n",
      "210 234 Test Loss: 0.829 | Test Acc: 78.325% (10577/13504)\n",
      "234 Epoch: 40 | Test Loss: 0.826 | Test Acc: 78.519% (11759/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 41\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.001 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.001 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 41 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 1.216 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 0.810 | Test Acc: 79.133% (1570/1984)\n",
      "60 234 Test Loss: 0.795 | Test Acc: 79.380% (3099/3904)\n",
      "90 234 Test Loss: 0.826 | Test Acc: 78.503% (4572/5824)\n",
      "120 234 Test Loss: 0.820 | Test Acc: 78.525% (6081/7744)\n",
      "150 234 Test Loss: 0.817 | Test Acc: 78.694% (7605/9664)\n",
      "180 234 Test Loss: 0.811 | Test Acc: 78.781% (9126/11584)\n",
      "210 234 Test Loss: 0.809 | Test Acc: 78.791% (10640/13504)\n",
      "234 Epoch: 41 | Test Loss: 0.814 | Test Acc: 78.813% (11803/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 42\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 42 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 1.183 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 0.785 | Test Acc: 78.931% (1566/1984)\n",
      "60 234 Test Loss: 0.800 | Test Acc: 78.689% (3072/3904)\n",
      "90 234 Test Loss: 0.798 | Test Acc: 78.863% (4593/5824)\n",
      "120 234 Test Loss: 0.810 | Test Acc: 78.577% (6085/7744)\n",
      "150 234 Test Loss: 0.820 | Test Acc: 78.435% (7580/9664)\n",
      "180 234 Test Loss: 0.814 | Test Acc: 78.574% (9102/11584)\n",
      "210 234 Test Loss: 0.804 | Test Acc: 78.747% (10634/13504)\n",
      "234 Epoch: 42 | Test Loss: 0.809 | Test Acc: 78.653% (11779/14976)\n",
      "\n",
      "Epoch: 43\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 43 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.848 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 0.835 | Test Acc: 78.831% (1564/1984)\n",
      "60 234 Test Loss: 0.818 | Test Acc: 78.893% (3080/3904)\n",
      "90 234 Test Loss: 0.810 | Test Acc: 79.052% (4604/5824)\n",
      "120 234 Test Loss: 0.806 | Test Acc: 78.771% (6100/7744)\n",
      "150 234 Test Loss: 0.795 | Test Acc: 78.787% (7614/9664)\n",
      "180 234 Test Loss: 0.788 | Test Acc: 78.911% (9141/11584)\n",
      "210 234 Test Loss: 0.783 | Test Acc: 79.028% (10672/13504)\n",
      "234 Epoch: 43 | Test Loss: 0.790 | Test Acc: 78.933% (11821/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 44\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 44 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.709 | Test Acc: 84.375% (54/64)\n",
      "30 234 Test Loss: 0.770 | Test Acc: 80.040% (1588/1984)\n",
      "60 234 Test Loss: 0.784 | Test Acc: 78.996% (3084/3904)\n",
      "90 234 Test Loss: 0.780 | Test Acc: 79.069% (4605/5824)\n",
      "120 234 Test Loss: 0.778 | Test Acc: 79.055% (6122/7744)\n",
      "150 234 Test Loss: 0.791 | Test Acc: 78.725% (7608/9664)\n",
      "180 234 Test Loss: 0.788 | Test Acc: 78.885% (9138/11584)\n",
      "210 234 Test Loss: 0.782 | Test Acc: 79.006% (10669/13504)\n",
      "234 Epoch: 44 | Test Loss: 0.782 | Test Acc: 79.087% (11844/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 45\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 45 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.858 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 0.779 | Test Acc: 78.579% (1559/1984)\n",
      "60 234 Test Loss: 0.777 | Test Acc: 79.150% (3090/3904)\n",
      "90 234 Test Loss: 0.774 | Test Acc: 79.121% (4608/5824)\n",
      "120 234 Test Loss: 0.762 | Test Acc: 79.378% (6147/7744)\n",
      "150 234 Test Loss: 0.761 | Test Acc: 79.346% (7668/9664)\n",
      "180 234 Test Loss: 0.767 | Test Acc: 79.213% (9176/11584)\n",
      "210 234 Test Loss: 0.767 | Test Acc: 79.199% (10695/13504)\n",
      "234 Epoch: 45 | Test Loss: 0.773 | Test Acc: 79.147% (11853/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 46\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 46 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.776 | Test Acc: 82.812% (53/64)\n",
      "30 234 Test Loss: 0.859 | Test Acc: 77.319% (1534/1984)\n",
      "60 234 Test Loss: 0.809 | Test Acc: 78.689% (3072/3904)\n",
      "90 234 Test Loss: 0.782 | Test Acc: 78.880% (4594/5824)\n",
      "120 234 Test Loss: 0.785 | Test Acc: 78.848% (6106/7744)\n",
      "150 234 Test Loss: 0.772 | Test Acc: 78.974% (7632/9664)\n",
      "180 234 Test Loss: 0.767 | Test Acc: 79.152% (9169/11584)\n",
      "210 234 Test Loss: 0.765 | Test Acc: 79.221% (10698/13504)\n",
      "234 Epoch: 46 | Test Loss: 0.767 | Test Acc: 79.173% (11857/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 47\n",
      "0 234 Train Loss: 0.004 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 47 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.922 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 0.760 | Test Acc: 79.889% (1585/1984)\n",
      "60 234 Test Loss: 0.755 | Test Acc: 79.559% (3106/3904)\n",
      "90 234 Test Loss: 0.772 | Test Acc: 79.035% (4603/5824)\n",
      "120 234 Test Loss: 0.766 | Test Acc: 78.926% (6112/7744)\n",
      "150 234 Test Loss: 0.765 | Test Acc: 78.953% (7630/9664)\n",
      "180 234 Test Loss: 0.759 | Test Acc: 79.100% (9163/11584)\n",
      "210 234 Test Loss: 0.757 | Test Acc: 79.251% (10702/13504)\n",
      "234 Epoch: 47 | Test Loss: 0.766 | Test Acc: 79.200% (11861/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 48\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 48 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.441 | Test Acc: 89.062% (57/64)\n",
      "30 234 Test Loss: 0.770 | Test Acc: 78.528% (1558/1984)\n",
      "60 234 Test Loss: 0.756 | Test Acc: 79.175% (3091/3904)\n",
      "90 234 Test Loss: 0.767 | Test Acc: 79.052% (4604/5824)\n",
      "120 234 Test Loss: 0.758 | Test Acc: 79.326% (6143/7744)\n",
      "150 234 Test Loss: 0.758 | Test Acc: 79.160% (7650/9664)\n",
      "180 234 Test Loss: 0.761 | Test Acc: 79.057% (9158/11584)\n",
      "210 234 Test Loss: 0.756 | Test Acc: 79.110% (10683/13504)\n",
      "234 Epoch: 48 | Test Loss: 0.760 | Test Acc: 79.133% (11851/14976)\n",
      "\n",
      "Epoch: 49\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 49 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.913 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 0.729 | Test Acc: 78.679% (1561/1984)\n",
      "60 234 Test Loss: 0.753 | Test Acc: 79.022% (3085/3904)\n",
      "90 234 Test Loss: 0.755 | Test Acc: 79.001% (4601/5824)\n",
      "120 234 Test Loss: 0.758 | Test Acc: 79.158% (6130/7744)\n",
      "150 234 Test Loss: 0.750 | Test Acc: 79.284% (7662/9664)\n",
      "180 234 Test Loss: 0.748 | Test Acc: 79.282% (9184/11584)\n",
      "210 234 Test Loss: 0.754 | Test Acc: 79.117% (10684/13504)\n",
      "234 Epoch: 49 | Test Loss: 0.755 | Test Acc: 79.120% (11849/14976)\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.2.3: Start Target model training\n",
    "\n",
    "max_epoch = 50  #@param {type:\"integer\"}\n",
    "train_result_summary = 'target_train_DCA-BiLSTM.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(target_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(target_testloader, epoch, batch_size=batch_size, logfile = train_result_summary, save_modelpath = save_model_folder)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea5c825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_training_summary(filepath = 'target_train_DCA-BiLSTM.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23706efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DLA-BiLSTM'  #@param {type:\"string\"}\n",
    "save_model_folder = './Shadow-DLA-BiLSTM_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DLA-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76652da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 234 Train Loss: 0.707 | Train Acc: 82.812% (53/64)\n",
      "30 234 Train Loss: 0.890 | Train Acc: 73.438% (1457/1984)\n",
      "60 234 Train Loss: 0.833 | Train Acc: 73.770% (2880/3904)\n",
      "90 234 Train Loss: 0.797 | Train Acc: 74.365% (4331/5824)\n",
      "120 234 Train Loss: 0.756 | Train Acc: 75.517% (5848/7744)\n",
      "150 234 Train Loss: 0.735 | Train Acc: 76.190% (7363/9664)\n",
      "180 234 Train Loss: 0.721 | Train Acc: 76.424% (8853/11584)\n",
      "210 234 Train Loss: 0.710 | Train Acc: 76.607% (10345/13504)\n",
      "234 Epoch: 0 | Train Loss: 0.699 | Train Acc: 77.017% (11534/14976)\n",
      "0 234 Test Loss: 0.649 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 0.653 | Test Acc: 78.125% (1550/1984)\n",
      "60 234 Test Loss: 0.651 | Test Acc: 78.125% (3050/3904)\n",
      "90 234 Test Loss: 0.634 | Test Acc: 78.966% (4599/5824)\n",
      "120 234 Test Loss: 0.626 | Test Acc: 78.861% (6107/7744)\n",
      "150 234 Test Loss: 0.627 | Test Acc: 78.808% (7616/9664)\n",
      "180 234 Test Loss: 0.623 | Test Acc: 78.928% (9143/11584)\n",
      "210 234 Test Loss: 0.624 | Test Acc: 78.821% (10644/13504)\n",
      "234 Epoch: 0 | Test Loss: 0.627 | Test Acc: 78.552% (11764/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "0 234 Train Loss: 0.310 | Train Acc: 93.750% (60/64)\n",
      "30 234 Train Loss: 0.379 | Train Acc: 87.601% (1738/1984)\n",
      "60 234 Train Loss: 0.376 | Train Acc: 87.859% (3430/3904)\n",
      "90 234 Train Loss: 0.390 | Train Acc: 87.431% (5092/5824)\n",
      "120 234 Train Loss: 0.390 | Train Acc: 87.306% (6761/7744)\n",
      "150 234 Train Loss: 0.401 | Train Acc: 86.848% (8393/9664)\n",
      "180 234 Train Loss: 0.407 | Train Acc: 86.689% (10042/11584)\n",
      "210 234 Train Loss: 0.410 | Train Acc: 86.478% (11678/13504)\n",
      "234 Epoch: 1 | Train Loss: 0.413 | Train Acc: 86.265% (12919/14976)\n",
      "0 234 Test Loss: 0.741 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 0.738 | Test Acc: 74.849% (1485/1984)\n",
      "60 234 Test Loss: 0.755 | Test Acc: 75.000% (2928/3904)\n",
      "90 234 Test Loss: 0.747 | Test Acc: 75.223% (4381/5824)\n",
      "120 234 Test Loss: 0.745 | Test Acc: 75.413% (5840/7744)\n",
      "150 234 Test Loss: 0.741 | Test Acc: 75.507% (7297/9664)\n",
      "180 234 Test Loss: 0.742 | Test Acc: 75.596% (8757/11584)\n",
      "210 234 Test Loss: 0.753 | Test Acc: 75.259% (10163/13504)\n",
      "234 Epoch: 1 | Test Loss: 0.754 | Test Acc: 75.180% (11259/14976)\n",
      "\n",
      "Epoch: 2\n",
      "0 234 Train Loss: 0.312 | Train Acc: 87.500% (56/64)\n",
      "30 234 Train Loss: 0.243 | Train Acc: 92.137% (1828/1984)\n",
      "60 234 Train Loss: 0.244 | Train Acc: 92.188% (3599/3904)\n",
      "90 234 Train Loss: 0.247 | Train Acc: 91.981% (5357/5824)\n",
      "120 234 Train Loss: 0.252 | Train Acc: 91.890% (7116/7744)\n",
      "150 234 Train Loss: 0.246 | Train Acc: 92.105% (8901/9664)\n",
      "180 234 Train Loss: 0.248 | Train Acc: 91.963% (10653/11584)\n",
      "210 234 Train Loss: 0.259 | Train Acc: 91.551% (12363/13504)\n",
      "234 Epoch: 2 | Train Loss: 0.262 | Train Acc: 91.466% (13698/14976)\n",
      "0 234 Test Loss: 0.849 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 0.940 | Test Acc: 71.774% (1424/1984)\n",
      "60 234 Test Loss: 0.906 | Test Acc: 72.541% (2832/3904)\n",
      "90 234 Test Loss: 0.917 | Test Acc: 72.356% (4214/5824)\n",
      "120 234 Test Loss: 0.903 | Test Acc: 72.508% (5615/7744)\n",
      "150 234 Test Loss: 0.910 | Test Acc: 72.496% (7006/9664)\n",
      "180 234 Test Loss: 0.908 | Test Acc: 72.522% (8401/11584)\n",
      "210 234 Test Loss: 0.908 | Test Acc: 72.438% (9782/13504)\n",
      "234 Epoch: 2 | Test Loss: 0.909 | Test Acc: 72.443% (10849/14976)\n",
      "\n",
      "Epoch: 3\n",
      "0 234 Train Loss: 0.153 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.159 | Train Acc: 95.161% (1888/1984)\n",
      "60 234 Train Loss: 0.145 | Train Acc: 95.594% (3732/3904)\n",
      "90 234 Train Loss: 0.138 | Train Acc: 95.862% (5583/5824)\n",
      "120 234 Train Loss: 0.139 | Train Acc: 95.648% (7407/7744)\n",
      "150 234 Train Loss: 0.147 | Train Acc: 95.385% (9218/9664)\n",
      "180 234 Train Loss: 0.153 | Train Acc: 95.123% (11019/11584)\n",
      "210 234 Train Loss: 0.159 | Train Acc: 94.935% (12820/13504)\n",
      "234 Epoch: 3 | Train Loss: 0.167 | Train Acc: 94.625% (14171/14976)\n",
      "0 234 Test Loss: 0.796 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 1.019 | Test Acc: 72.883% (1446/1984)\n",
      "60 234 Test Loss: 1.007 | Test Acc: 73.566% (2872/3904)\n",
      "90 234 Test Loss: 1.010 | Test Acc: 73.884% (4303/5824)\n",
      "120 234 Test Loss: 1.009 | Test Acc: 73.605% (5700/7744)\n",
      "150 234 Test Loss: 1.008 | Test Acc: 73.665% (7119/9664)\n",
      "180 234 Test Loss: 1.013 | Test Acc: 73.593% (8525/11584)\n",
      "210 234 Test Loss: 1.017 | Test Acc: 73.645% (9945/13504)\n",
      "234 Epoch: 3 | Test Loss: 1.015 | Test Acc: 73.538% (11013/14976)\n",
      "\n",
      "Epoch: 4\n",
      "0 234 Train Loss: 0.263 | Train Acc: 89.062% (57/64)\n",
      "30 234 Train Loss: 0.148 | Train Acc: 95.212% (1889/1984)\n",
      "60 234 Train Loss: 0.138 | Train Acc: 95.850% (3742/3904)\n",
      "90 234 Train Loss: 0.134 | Train Acc: 95.931% (5587/5824)\n",
      "120 234 Train Loss: 0.133 | Train Acc: 95.919% (7428/7744)\n",
      "150 234 Train Loss: 0.137 | Train Acc: 95.706% (9249/9664)\n",
      "180 234 Train Loss: 0.142 | Train Acc: 95.502% (11063/11584)\n",
      "210 234 Train Loss: 0.148 | Train Acc: 95.372% (12879/13504)\n",
      "234 Epoch: 4 | Train Loss: 0.153 | Train Acc: 95.126% (14246/14976)\n",
      "0 234 Test Loss: 0.500 | Test Acc: 87.500% (56/64)\n",
      "30 234 Test Loss: 0.726 | Test Acc: 78.931% (1566/1984)\n",
      "60 234 Test Loss: 0.754 | Test Acc: 77.510% (3026/3904)\n",
      "90 234 Test Loss: 0.760 | Test Acc: 77.232% (4498/5824)\n",
      "120 234 Test Loss: 0.761 | Test Acc: 77.118% (5972/7744)\n",
      "150 234 Test Loss: 0.764 | Test Acc: 77.101% (7451/9664)\n",
      "180 234 Test Loss: 0.768 | Test Acc: 77.201% (8943/11584)\n",
      "210 234 Test Loss: 0.769 | Test Acc: 77.096% (10411/13504)\n",
      "234 Epoch: 4 | Test Loss: 0.767 | Test Acc: 77.217% (11564/14976)\n",
      "\n",
      "Epoch: 5\n",
      "0 234 Train Loss: 0.116 | Train Acc: 93.750% (60/64)\n",
      "30 234 Train Loss: 0.096 | Train Acc: 97.127% (1927/1984)\n",
      "60 234 Train Loss: 0.097 | Train Acc: 97.080% (3790/3904)\n",
      "90 234 Train Loss: 0.101 | Train Acc: 96.789% (5637/5824)\n",
      "120 234 Train Loss: 0.102 | Train Acc: 96.875% (7502/7744)\n",
      "150 234 Train Loss: 0.104 | Train Acc: 96.699% (9345/9664)\n",
      "180 234 Train Loss: 0.102 | Train Acc: 96.780% (11211/11584)\n",
      "210 234 Train Loss: 0.103 | Train Acc: 96.794% (13071/13504)\n",
      "234 Epoch: 5 | Train Loss: 0.104 | Train Acc: 96.768% (14492/14976)\n",
      "0 234 Test Loss: 0.986 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 0.813 | Test Acc: 77.268% (1533/1984)\n",
      "60 234 Test Loss: 0.761 | Test Acc: 78.432% (3062/3904)\n",
      "90 234 Test Loss: 0.739 | Test Acc: 78.915% (4596/5824)\n",
      "120 234 Test Loss: 0.740 | Test Acc: 79.029% (6120/7744)\n",
      "150 234 Test Loss: 0.744 | Test Acc: 79.015% (7636/9664)\n",
      "180 234 Test Loss: 0.749 | Test Acc: 78.867% (9136/11584)\n",
      "210 234 Test Loss: 0.752 | Test Acc: 78.880% (10652/13504)\n",
      "234 Epoch: 5 | Test Loss: 0.754 | Test Acc: 78.759% (11795/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      "0 234 Train Loss: 0.031 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.083 | Train Acc: 97.379% (1932/1984)\n",
      "60 234 Train Loss: 0.070 | Train Acc: 98.002% (3826/3904)\n",
      "90 234 Train Loss: 0.075 | Train Acc: 97.751% (5693/5824)\n",
      "120 234 Train Loss: 0.073 | Train Acc: 97.766% (7571/7744)\n",
      "150 234 Train Loss: 0.075 | Train Acc: 97.713% (9443/9664)\n",
      "180 234 Train Loss: 0.079 | Train Acc: 97.592% (11305/11584)\n",
      "210 234 Train Loss: 0.085 | Train Acc: 97.275% (13136/13504)\n",
      "234 Epoch: 6 | Train Loss: 0.087 | Train Acc: 97.216% (14559/14976)\n",
      "0 234 Test Loss: 1.145 | Test Acc: 70.312% (45/64)\n",
      "30 234 Test Loss: 0.846 | Test Acc: 76.411% (1516/1984)\n",
      "60 234 Test Loss: 0.849 | Test Acc: 76.716% (2995/3904)\n",
      "90 234 Test Loss: 0.892 | Test Acc: 75.841% (4417/5824)\n",
      "120 234 Test Loss: 0.899 | Test Acc: 75.439% (5842/7744)\n",
      "150 234 Test Loss: 0.876 | Test Acc: 75.900% (7335/9664)\n",
      "180 234 Test Loss: 0.877 | Test Acc: 75.967% (8800/11584)\n",
      "210 234 Test Loss: 0.866 | Test Acc: 76.177% (10287/13504)\n",
      "234 Epoch: 6 | Test Loss: 0.861 | Test Acc: 76.249% (11419/14976)\n",
      "\n",
      "Epoch: 7\n",
      "0 234 Train Loss: 0.084 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.066 | Train Acc: 98.034% (1945/1984)\n",
      "60 234 Train Loss: 0.060 | Train Acc: 98.361% (3840/3904)\n",
      "90 234 Train Loss: 0.057 | Train Acc: 98.403% (5731/5824)\n",
      "120 234 Train Loss: 0.059 | Train Acc: 98.334% (7615/7744)\n",
      "150 234 Train Loss: 0.062 | Train Acc: 98.189% (9489/9664)\n",
      "180 234 Train Loss: 0.065 | Train Acc: 98.049% (11358/11584)\n",
      "210 234 Train Loss: 0.065 | Train Acc: 98.060% (13242/13504)\n",
      "234 Epoch: 7 | Train Loss: 0.066 | Train Acc: 97.997% (14676/14976)\n",
      "0 234 Test Loss: 1.005 | Test Acc: 73.438% (47/64)\n",
      "30 234 Test Loss: 0.795 | Test Acc: 79.284% (1573/1984)\n",
      "60 234 Test Loss: 0.821 | Test Acc: 78.458% (3063/3904)\n",
      "90 234 Test Loss: 0.823 | Test Acc: 78.365% (4564/5824)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 234 Test Loss: 0.824 | Test Acc: 78.435% (6074/7744)\n",
      "150 234 Test Loss: 0.824 | Test Acc: 78.342% (7571/9664)\n",
      "180 234 Test Loss: 0.834 | Test Acc: 78.237% (9063/11584)\n",
      "210 234 Test Loss: 0.830 | Test Acc: 78.184% (10558/13504)\n",
      "234 Epoch: 7 | Test Loss: 0.828 | Test Acc: 78.145% (11703/14976)\n",
      "\n",
      "Epoch: 8\n",
      "0 234 Train Loss: 0.126 | Train Acc: 95.312% (61/64)\n",
      "30 234 Train Loss: 0.058 | Train Acc: 98.135% (1947/1984)\n",
      "60 234 Train Loss: 0.055 | Train Acc: 98.361% (3840/3904)\n",
      "90 234 Train Loss: 0.053 | Train Acc: 98.455% (5734/5824)\n",
      "120 234 Train Loss: 0.051 | Train Acc: 98.528% (7630/7744)\n",
      "150 234 Train Loss: 0.050 | Train Acc: 98.551% (9524/9664)\n",
      "180 234 Train Loss: 0.052 | Train Acc: 98.463% (11406/11584)\n",
      "210 234 Train Loss: 0.052 | Train Acc: 98.460% (13296/13504)\n",
      "234 Epoch: 8 | Train Loss: 0.054 | Train Acc: 98.404% (14737/14976)\n",
      "0 234 Test Loss: 0.597 | Test Acc: 82.812% (53/64)\n",
      "30 234 Test Loss: 0.786 | Test Acc: 79.335% (1574/1984)\n",
      "60 234 Test Loss: 0.744 | Test Acc: 79.816% (3116/3904)\n",
      "90 234 Test Loss: 0.760 | Test Acc: 79.619% (4637/5824)\n",
      "120 234 Test Loss: 0.764 | Test Acc: 79.365% (6146/7744)\n",
      "150 234 Test Loss: 0.770 | Test Acc: 79.346% (7668/9664)\n",
      "180 234 Test Loss: 0.770 | Test Acc: 79.334% (9190/11584)\n",
      "210 234 Test Loss: 0.776 | Test Acc: 79.206% (10696/13504)\n",
      "234 Epoch: 8 | Test Loss: 0.778 | Test Acc: 79.160% (11855/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      "0 234 Train Loss: 0.092 | Train Acc: 95.312% (61/64)\n",
      "30 234 Train Loss: 0.056 | Train Acc: 98.337% (1951/1984)\n",
      "60 234 Train Loss: 0.048 | Train Acc: 98.617% (3850/3904)\n",
      "90 234 Train Loss: 0.047 | Train Acc: 98.678% (5747/5824)\n",
      "120 234 Train Loss: 0.046 | Train Acc: 98.735% (7646/7744)\n",
      "150 234 Train Loss: 0.044 | Train Acc: 98.779% (9546/9664)\n",
      "180 234 Train Loss: 0.042 | Train Acc: 98.817% (11447/11584)\n",
      "210 234 Train Loss: 0.042 | Train Acc: 98.823% (13345/13504)\n",
      "234 Epoch: 9 | Train Loss: 0.043 | Train Acc: 98.798% (14796/14976)\n",
      "0 234 Test Loss: 0.610 | Test Acc: 85.938% (55/64)\n",
      "30 234 Test Loss: 0.905 | Test Acc: 76.865% (1525/1984)\n",
      "60 234 Test Loss: 0.881 | Test Acc: 77.587% (3029/3904)\n",
      "90 234 Test Loss: 0.900 | Test Acc: 77.421% (4509/5824)\n",
      "120 234 Test Loss: 0.884 | Test Acc: 77.454% (5998/7744)\n",
      "150 234 Test Loss: 0.886 | Test Acc: 77.546% (7494/9664)\n",
      "180 234 Test Loss: 0.882 | Test Acc: 77.564% (8985/11584)\n",
      "210 234 Test Loss: 0.876 | Test Acc: 77.673% (10489/13504)\n",
      "234 Epoch: 9 | Test Loss: 0.879 | Test Acc: 77.818% (11654/14976)\n",
      "\n",
      "Epoch: 10\n",
      "0 234 Train Loss: 0.074 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.035 | Train Acc: 98.891% (1962/1984)\n",
      "60 234 Train Loss: 0.034 | Train Acc: 98.847% (3859/3904)\n",
      "90 234 Train Loss: 0.036 | Train Acc: 98.850% (5757/5824)\n",
      "120 234 Train Loss: 0.038 | Train Acc: 98.812% (7652/7744)\n",
      "150 234 Train Loss: 0.038 | Train Acc: 98.841% (9552/9664)\n",
      "180 234 Train Loss: 0.038 | Train Acc: 98.835% (11449/11584)\n",
      "210 234 Train Loss: 0.041 | Train Acc: 98.726% (13332/13504)\n",
      "234 Epoch: 10 | Train Loss: 0.041 | Train Acc: 98.718% (14784/14976)\n",
      "0 234 Test Loss: 0.704 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 0.842 | Test Acc: 78.629% (1560/1984)\n",
      "60 234 Test Loss: 0.831 | Test Acc: 78.484% (3064/3904)\n",
      "90 234 Test Loss: 0.825 | Test Acc: 78.915% (4596/5824)\n",
      "120 234 Test Loss: 0.810 | Test Acc: 79.132% (6128/7744)\n",
      "150 234 Test Loss: 0.811 | Test Acc: 78.829% (7618/9664)\n",
      "180 234 Test Loss: 0.817 | Test Acc: 78.652% (9111/11584)\n",
      "210 234 Test Loss: 0.816 | Test Acc: 78.717% (10630/13504)\n",
      "234 Epoch: 10 | Test Loss: 0.816 | Test Acc: 78.839% (11807/14976)\n",
      "\n",
      "Epoch: 11\n",
      "0 234 Train Loss: 0.077 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.046 | Train Acc: 98.639% (1957/1984)\n",
      "60 234 Train Loss: 0.039 | Train Acc: 98.822% (3858/3904)\n",
      "90 234 Train Loss: 0.040 | Train Acc: 98.781% (5753/5824)\n",
      "120 234 Train Loss: 0.043 | Train Acc: 98.605% (7636/7744)\n",
      "150 234 Train Loss: 0.044 | Train Acc: 98.520% (9521/9664)\n",
      "180 234 Train Loss: 0.043 | Train Acc: 98.524% (11413/11584)\n",
      "210 234 Train Loss: 0.044 | Train Acc: 98.489% (13300/13504)\n",
      "234 Epoch: 11 | Train Loss: 0.046 | Train Acc: 98.458% (14745/14976)\n",
      "0 234 Test Loss: 1.099 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 0.850 | Test Acc: 78.679% (1561/1984)\n",
      "60 234 Test Loss: 0.840 | Test Acc: 78.919% (3081/3904)\n",
      "90 234 Test Loss: 0.849 | Test Acc: 78.760% (4587/5824)\n",
      "120 234 Test Loss: 0.851 | Test Acc: 78.577% (6085/7744)\n",
      "150 234 Test Loss: 0.836 | Test Acc: 78.663% (7602/9664)\n",
      "180 234 Test Loss: 0.832 | Test Acc: 78.738% (9121/11584)\n",
      "210 234 Test Loss: 0.839 | Test Acc: 78.703% (10628/13504)\n",
      "234 Epoch: 11 | Test Loss: 0.843 | Test Acc: 78.632% (11776/14976)\n",
      "\n",
      "Epoch: 12\n",
      "0 234 Train Loss: 0.122 | Train Acc: 95.312% (61/64)\n",
      "30 234 Train Loss: 0.051 | Train Acc: 98.236% (1949/1984)\n",
      "60 234 Train Loss: 0.046 | Train Acc: 98.438% (3843/3904)\n",
      "90 234 Train Loss: 0.042 | Train Acc: 98.626% (5744/5824)\n",
      "120 234 Train Loss: 0.040 | Train Acc: 98.696% (7643/7744)\n",
      "150 234 Train Loss: 0.041 | Train Acc: 98.675% (9536/9664)\n",
      "180 234 Train Loss: 0.041 | Train Acc: 98.696% (11433/11584)\n",
      "210 234 Train Loss: 0.042 | Train Acc: 98.652% (13322/13504)\n",
      "234 Epoch: 12 | Train Loss: 0.043 | Train Acc: 98.651% (14774/14976)\n",
      "0 234 Test Loss: 1.046 | Test Acc: 68.750% (44/64)\n",
      "30 234 Test Loss: 0.825 | Test Acc: 78.226% (1552/1984)\n",
      "60 234 Test Loss: 0.819 | Test Acc: 78.151% (3051/3904)\n",
      "90 234 Test Loss: 0.793 | Test Acc: 79.138% (4609/5824)\n",
      "120 234 Test Loss: 0.803 | Test Acc: 78.809% (6103/7744)\n",
      "150 234 Test Loss: 0.812 | Test Acc: 78.746% (7610/9664)\n",
      "180 234 Test Loss: 0.800 | Test Acc: 79.161% (9170/11584)\n",
      "210 234 Test Loss: 0.798 | Test Acc: 79.132% (10686/13504)\n",
      "234 Epoch: 12 | Test Loss: 0.802 | Test Acc: 79.087% (11844/14976)\n",
      "\n",
      "Epoch: 13\n",
      "0 234 Train Loss: 0.070 | Train Acc: 96.875% (62/64)\n",
      "30 234 Train Loss: 0.049 | Train Acc: 98.488% (1954/1984)\n",
      "60 234 Train Loss: 0.044 | Train Acc: 98.463% (3844/3904)\n",
      "90 234 Train Loss: 0.041 | Train Acc: 98.609% (5743/5824)\n",
      "120 234 Train Loss: 0.045 | Train Acc: 98.541% (7631/7744)\n",
      "150 234 Train Loss: 0.044 | Train Acc: 98.572% (9526/9664)\n",
      "180 234 Train Loss: 0.045 | Train Acc: 98.584% (11420/11584)\n",
      "210 234 Train Loss: 0.045 | Train Acc: 98.637% (13320/13504)\n",
      "234 Epoch: 13 | Train Loss: 0.045 | Train Acc: 98.624% (14770/14976)\n",
      "0 234 Test Loss: 0.736 | Test Acc: 82.812% (53/64)\n",
      "30 234 Test Loss: 0.776 | Test Acc: 79.738% (1582/1984)\n",
      "60 234 Test Loss: 0.775 | Test Acc: 79.995% (3123/3904)\n",
      "90 234 Test Loss: 0.770 | Test Acc: 80.031% (4661/5824)\n",
      "120 234 Test Loss: 0.792 | Test Acc: 79.403% (6149/7744)\n",
      "150 234 Test Loss: 0.793 | Test Acc: 79.294% (7663/9664)\n",
      "180 234 Test Loss: 0.798 | Test Acc: 79.420% (9200/11584)\n",
      "210 234 Test Loss: 0.797 | Test Acc: 79.339% (10714/13504)\n",
      "234 Epoch: 13 | Test Loss: 0.801 | Test Acc: 79.227% (11865/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 14\n",
      "0 234 Train Loss: 0.006 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.031 | Train Acc: 99.294% (1970/1984)\n",
      "60 234 Train Loss: 0.027 | Train Acc: 99.334% (3878/3904)\n",
      "90 234 Train Loss: 0.029 | Train Acc: 99.159% (5775/5824)\n",
      "120 234 Train Loss: 0.030 | Train Acc: 99.096% (7674/7744)\n",
      "150 234 Train Loss: 0.032 | Train Acc: 99.048% (9572/9664)\n",
      "180 234 Train Loss: 0.031 | Train Acc: 99.085% (11478/11584)\n",
      "210 234 Train Loss: 0.032 | Train Acc: 99.045% (13375/13504)\n",
      "234 Epoch: 14 | Train Loss: 0.032 | Train Acc: 99.032% (14831/14976)\n",
      "0 234 Test Loss: 0.629 | Test Acc: 89.062% (57/64)\n",
      "30 234 Test Loss: 0.859 | Test Acc: 78.478% (1557/1984)\n",
      "60 234 Test Loss: 0.862 | Test Acc: 78.586% (3068/3904)\n",
      "90 234 Test Loss: 0.844 | Test Acc: 78.795% (4589/5824)\n",
      "120 234 Test Loss: 0.858 | Test Acc: 78.629% (6089/7744)\n",
      "150 234 Test Loss: 0.866 | Test Acc: 78.549% (7591/9664)\n",
      "180 234 Test Loss: 0.864 | Test Acc: 78.591% (9104/11584)\n",
      "210 234 Test Loss: 0.852 | Test Acc: 78.769% (10637/13504)\n",
      "234 Epoch: 14 | Test Loss: 0.841 | Test Acc: 78.966% (11826/14976)\n",
      "\n",
      "Epoch: 15\n",
      "0 234 Train Loss: 0.010 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.025 | Train Acc: 99.446% (1973/1984)\n",
      "60 234 Train Loss: 0.024 | Train Acc: 99.308% (3877/3904)\n",
      "90 234 Train Loss: 0.029 | Train Acc: 99.090% (5771/5824)\n",
      "120 234 Train Loss: 0.029 | Train Acc: 99.070% (7672/7744)\n",
      "150 234 Train Loss: 0.029 | Train Acc: 99.079% (9575/9664)\n",
      "180 234 Train Loss: 0.030 | Train Acc: 99.007% (11469/11584)\n",
      "210 234 Train Loss: 0.030 | Train Acc: 99.067% (13378/13504)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234 Epoch: 15 | Train Loss: 0.029 | Train Acc: 99.079% (14838/14976)\n",
      "0 234 Test Loss: 0.912 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 0.781 | Test Acc: 79.940% (1586/1984)\n",
      "60 234 Test Loss: 0.768 | Test Acc: 80.149% (3129/3904)\n",
      "90 234 Test Loss: 0.771 | Test Acc: 79.876% (4652/5824)\n",
      "120 234 Test Loss: 0.765 | Test Acc: 80.333% (6221/7744)\n",
      "150 234 Test Loss: 0.767 | Test Acc: 80.433% (7773/9664)\n",
      "180 234 Test Loss: 0.756 | Test Acc: 80.551% (9331/11584)\n",
      "210 234 Test Loss: 0.761 | Test Acc: 80.295% (10843/13504)\n",
      "234 Epoch: 15 | Test Loss: 0.758 | Test Acc: 80.402% (12041/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 16\n",
      "0 234 Train Loss: 0.024 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.020 | Train Acc: 99.345% (1971/1984)\n",
      "60 234 Train Loss: 0.019 | Train Acc: 99.462% (3883/3904)\n",
      "90 234 Train Loss: 0.018 | Train Acc: 99.554% (5798/5824)\n",
      "120 234 Train Loss: 0.016 | Train Acc: 99.613% (7714/7744)\n",
      "150 234 Train Loss: 0.017 | Train Acc: 99.534% (9619/9664)\n",
      "180 234 Train Loss: 0.019 | Train Acc: 99.499% (11526/11584)\n",
      "210 234 Train Loss: 0.019 | Train Acc: 99.474% (13433/13504)\n",
      "234 Epoch: 16 | Train Loss: 0.020 | Train Acc: 99.452% (14894/14976)\n",
      "0 234 Test Loss: 0.943 | Test Acc: 76.562% (49/64)\n",
      "30 234 Test Loss: 0.762 | Test Acc: 81.099% (1609/1984)\n",
      "60 234 Test Loss: 0.762 | Test Acc: 80.456% (3141/3904)\n",
      "90 234 Test Loss: 0.777 | Test Acc: 80.237% (4673/5824)\n",
      "120 234 Test Loss: 0.772 | Test Acc: 80.114% (6204/7744)\n",
      "150 234 Test Loss: 0.766 | Test Acc: 80.184% (7749/9664)\n",
      "180 234 Test Loss: 0.769 | Test Acc: 80.128% (9282/11584)\n",
      "210 234 Test Loss: 0.751 | Test Acc: 80.428% (10861/13504)\n",
      "234 Epoch: 16 | Test Loss: 0.751 | Test Acc: 80.449% (12048/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 17\n",
      "0 234 Train Loss: 0.028 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.021 | Train Acc: 99.446% (1973/1984)\n",
      "60 234 Train Loss: 0.018 | Train Acc: 99.616% (3889/3904)\n",
      "90 234 Train Loss: 0.015 | Train Acc: 99.691% (5806/5824)\n",
      "120 234 Train Loss: 0.015 | Train Acc: 99.703% (7721/7744)\n",
      "150 234 Train Loss: 0.015 | Train Acc: 99.679% (9633/9664)\n",
      "180 234 Train Loss: 0.016 | Train Acc: 99.637% (11542/11584)\n",
      "210 234 Train Loss: 0.016 | Train Acc: 99.637% (13455/13504)\n",
      "234 Epoch: 17 | Train Loss: 0.016 | Train Acc: 99.646% (14923/14976)\n",
      "0 234 Test Loss: 0.708 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 0.693 | Test Acc: 82.964% (1646/1984)\n",
      "60 234 Test Loss: 0.740 | Test Acc: 81.993% (3201/3904)\n",
      "90 234 Test Loss: 0.724 | Test Acc: 81.748% (4761/5824)\n",
      "120 234 Test Loss: 0.704 | Test Acc: 81.947% (6346/7744)\n",
      "150 234 Test Loss: 0.699 | Test Acc: 82.026% (7927/9664)\n",
      "180 234 Test Loss: 0.715 | Test Acc: 81.483% (9439/11584)\n",
      "210 234 Test Loss: 0.717 | Test Acc: 81.561% (11014/13504)\n",
      "234 Epoch: 17 | Test Loss: 0.719 | Test Acc: 81.457% (12199/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 18\n",
      "0 234 Train Loss: 0.023 | Train Acc: 98.438% (63/64)\n",
      "30 234 Train Loss: 0.017 | Train Acc: 99.597% (1976/1984)\n",
      "60 234 Train Loss: 0.014 | Train Acc: 99.667% (3891/3904)\n",
      "90 234 Train Loss: 0.012 | Train Acc: 99.742% (5809/5824)\n",
      "120 234 Train Loss: 0.011 | Train Acc: 99.780% (7727/7744)\n",
      "150 234 Train Loss: 0.010 | Train Acc: 99.814% (9646/9664)\n",
      "180 234 Train Loss: 0.009 | Train Acc: 99.845% (11566/11584)\n",
      "210 234 Train Loss: 0.009 | Train Acc: 99.844% (13483/13504)\n",
      "234 Epoch: 18 | Train Loss: 0.009 | Train Acc: 99.833% (14951/14976)\n",
      "0 234 Test Loss: 1.180 | Test Acc: 75.000% (48/64)\n",
      "30 234 Test Loss: 0.656 | Test Acc: 83.014% (1647/1984)\n",
      "60 234 Test Loss: 0.679 | Test Acc: 82.377% (3216/3904)\n",
      "90 234 Test Loss: 0.693 | Test Acc: 82.400% (4799/5824)\n",
      "120 234 Test Loss: 0.682 | Test Acc: 82.464% (6386/7744)\n",
      "150 234 Test Loss: 0.671 | Test Acc: 82.709% (7993/9664)\n",
      "180 234 Test Loss: 0.670 | Test Acc: 82.588% (9567/11584)\n",
      "210 234 Test Loss: 0.670 | Test Acc: 82.553% (11148/13504)\n",
      "234 Epoch: 18 | Test Loss: 0.672 | Test Acc: 82.565% (12365/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 19\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.006 | Train Acc: 99.950% (1983/1984)\n",
      "60 234 Train Loss: 0.005 | Train Acc: 99.974% (3903/3904)\n",
      "90 234 Train Loss: 0.005 | Train Acc: 99.966% (5822/5824)\n",
      "120 234 Train Loss: 0.005 | Train Acc: 99.935% (7739/7744)\n",
      "150 234 Train Loss: 0.005 | Train Acc: 99.928% (9657/9664)\n",
      "180 234 Train Loss: 0.005 | Train Acc: 99.922% (11575/11584)\n",
      "210 234 Train Loss: 0.005 | Train Acc: 99.919% (13493/13504)\n",
      "234 Epoch: 19 | Train Loss: 0.006 | Train Acc: 99.900% (14961/14976)\n",
      "0 234 Test Loss: 1.011 | Test Acc: 78.125% (50/64)\n",
      "30 234 Test Loss: 0.679 | Test Acc: 82.359% (1634/1984)\n",
      "60 234 Test Loss: 0.696 | Test Acc: 82.147% (3207/3904)\n",
      "90 234 Test Loss: 0.684 | Test Acc: 82.366% (4797/5824)\n",
      "120 234 Test Loss: 0.691 | Test Acc: 82.051% (6354/7744)\n",
      "150 234 Test Loss: 0.699 | Test Acc: 81.902% (7915/9664)\n",
      "180 234 Test Loss: 0.696 | Test Acc: 82.001% (9499/11584)\n",
      "210 234 Test Loss: 0.698 | Test Acc: 82.102% (11087/13504)\n",
      "234 Epoch: 19 | Test Loss: 0.705 | Test Acc: 81.991% (12279/14976)\n",
      "\n",
      "Epoch: 20\n",
      "0 234 Train Loss: 0.005 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.004 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.003 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 99.979% (9662/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 99.983% (11582/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 99.985% (13502/13504)\n",
      "234 Epoch: 20 | Train Loss: 0.004 | Train Acc: 99.980% (14973/14976)\n",
      "0 234 Test Loss: 0.896 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.680 | Test Acc: 83.216% (1651/1984)\n",
      "60 234 Test Loss: 0.668 | Test Acc: 83.555% (3262/3904)\n",
      "90 234 Test Loss: 0.659 | Test Acc: 83.482% (4862/5824)\n",
      "120 234 Test Loss: 0.655 | Test Acc: 83.407% (6459/7744)\n",
      "150 234 Test Loss: 0.661 | Test Acc: 83.216% (8042/9664)\n",
      "180 234 Test Loss: 0.661 | Test Acc: 83.106% (9627/11584)\n",
      "210 234 Test Loss: 0.657 | Test Acc: 83.109% (11223/13504)\n",
      "234 Epoch: 20 | Test Loss: 0.657 | Test Acc: 83.073% (12441/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 21\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 21 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.907 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 0.624 | Test Acc: 82.863% (1644/1984)\n",
      "60 234 Test Loss: 0.611 | Test Acc: 83.607% (3264/3904)\n",
      "90 234 Test Loss: 0.614 | Test Acc: 83.757% (4878/5824)\n",
      "120 234 Test Loss: 0.620 | Test Acc: 83.471% (6464/7744)\n",
      "150 234 Test Loss: 0.618 | Test Acc: 83.495% (8069/9664)\n",
      "180 234 Test Loss: 0.628 | Test Acc: 83.270% (9646/11584)\n",
      "210 234 Test Loss: 0.626 | Test Acc: 83.405% (11263/13504)\n",
      "234 Epoch: 21 | Test Loss: 0.631 | Test Acc: 83.307% (12476/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 22\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 22 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.808 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.640 | Test Acc: 83.569% (1658/1984)\n",
      "60 234 Test Loss: 0.602 | Test Acc: 84.273% (3290/3904)\n",
      "90 234 Test Loss: 0.609 | Test Acc: 83.826% (4882/5824)\n",
      "120 234 Test Loss: 0.611 | Test Acc: 83.665% (6479/7744)\n",
      "150 234 Test Loss: 0.623 | Test Acc: 83.351% (8055/9664)\n",
      "180 234 Test Loss: 0.616 | Test Acc: 83.538% (9677/11584)\n",
      "210 234 Test Loss: 0.612 | Test Acc: 83.575% (11286/13504)\n",
      "234 Epoch: 22 | Test Loss: 0.609 | Test Acc: 83.754% (12543/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 23\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 23 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.562 | Test Acc: 82.812% (53/64)\n",
      "30 234 Test Loss: 0.598 | Test Acc: 84.627% (1679/1984)\n",
      "60 234 Test Loss: 0.612 | Test Acc: 83.888% (3275/3904)\n",
      "90 234 Test Loss: 0.618 | Test Acc: 83.723% (4876/5824)\n",
      "120 234 Test Loss: 0.616 | Test Acc: 83.807% (6490/7744)\n",
      "150 234 Test Loss: 0.601 | Test Acc: 84.044% (8122/9664)\n",
      "180 234 Test Loss: 0.601 | Test Acc: 83.987% (9729/11584)\n",
      "210 234 Test Loss: 0.607 | Test Acc: 83.812% (11318/13504)\n",
      "234 Epoch: 23 | Test Loss: 0.607 | Test Acc: 83.727% (12539/14976)\n",
      "\n",
      "Epoch: 24\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 24 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.511 | Test Acc: 84.375% (54/64)\n",
      "30 234 Test Loss: 0.589 | Test Acc: 83.317% (1653/1984)\n",
      "60 234 Test Loss: 0.590 | Test Acc: 83.607% (3264/3904)\n",
      "90 234 Test Loss: 0.605 | Test Acc: 83.551% (4866/5824)\n",
      "120 234 Test Loss: 0.610 | Test Acc: 83.510% (6467/7744)\n",
      "150 234 Test Loss: 0.603 | Test Acc: 83.702% (8089/9664)\n",
      "180 234 Test Loss: 0.598 | Test Acc: 83.814% (9709/11584)\n",
      "210 234 Test Loss: 0.602 | Test Acc: 83.753% (11310/13504)\n",
      "234 Epoch: 24 | Test Loss: 0.601 | Test Acc: 83.801% (12550/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 25\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.001 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 25 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.375 | Test Acc: 84.375% (54/64)\n",
      "30 234 Test Loss: 0.542 | Test Acc: 84.526% (1677/1984)\n",
      "60 234 Test Loss: 0.586 | Test Acc: 83.760% (3270/3904)\n",
      "90 234 Test Loss: 0.598 | Test Acc: 83.551% (4866/5824)\n",
      "120 234 Test Loss: 0.585 | Test Acc: 83.936% (6500/7744)\n",
      "150 234 Test Loss: 0.585 | Test Acc: 84.034% (8121/9664)\n",
      "180 234 Test Loss: 0.588 | Test Acc: 83.961% (9726/11584)\n",
      "210 234 Test Loss: 0.580 | Test Acc: 84.182% (11368/13504)\n",
      "234 Epoch: 25 | Test Loss: 0.586 | Test Acc: 84.081% (12592/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 26\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 26 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.410 | Test Acc: 84.375% (54/64)\n",
      "30 234 Test Loss: 0.597 | Test Acc: 83.921% (1665/1984)\n",
      "60 234 Test Loss: 0.597 | Test Acc: 84.042% (3281/3904)\n",
      "90 234 Test Loss: 0.575 | Test Acc: 84.512% (4922/5824)\n",
      "120 234 Test Loss: 0.560 | Test Acc: 84.917% (6576/7744)\n",
      "150 234 Test Loss: 0.570 | Test Acc: 84.489% (8165/9664)\n",
      "180 234 Test Loss: 0.570 | Test Acc: 84.306% (9766/11584)\n",
      "210 234 Test Loss: 0.568 | Test Acc: 84.360% (11392/13504)\n",
      "234 Epoch: 26 | Test Loss: 0.577 | Test Acc: 84.128% (12599/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 27\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 27 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.861 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 0.544 | Test Acc: 84.325% (1673/1984)\n",
      "60 234 Test Loss: 0.559 | Test Acc: 84.196% (3287/3904)\n",
      "90 234 Test Loss: 0.560 | Test Acc: 84.306% (4910/5824)\n",
      "120 234 Test Loss: 0.559 | Test Acc: 84.259% (6525/7744)\n",
      "150 234 Test Loss: 0.564 | Test Acc: 84.178% (8135/9664)\n",
      "180 234 Test Loss: 0.564 | Test Acc: 84.168% (9750/11584)\n",
      "210 234 Test Loss: 0.571 | Test Acc: 83.983% (11341/13504)\n",
      "234 Epoch: 27 | Test Loss: 0.574 | Test Acc: 83.974% (12576/14976)\n",
      "\n",
      "Epoch: 28\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 28 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.696 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.574 | Test Acc: 84.325% (1673/1984)\n",
      "60 234 Test Loss: 0.569 | Test Acc: 84.606% (3303/3904)\n",
      "90 234 Test Loss: 0.565 | Test Acc: 84.272% (4908/5824)\n",
      "120 234 Test Loss: 0.565 | Test Acc: 84.130% (6515/7744)\n",
      "150 234 Test Loss: 0.570 | Test Acc: 84.023% (8120/9664)\n",
      "180 234 Test Loss: 0.573 | Test Acc: 83.978% (9728/11584)\n",
      "210 234 Test Loss: 0.574 | Test Acc: 83.983% (11341/13504)\n",
      "234 Epoch: 28 | Test Loss: 0.569 | Test Acc: 84.115% (12597/14976)\n",
      "\n",
      "Epoch: 29\n",
      "0 234 Train Loss: 0.001 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 29 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.698 | Test Acc: 84.375% (54/64)\n",
      "30 234 Test Loss: 0.540 | Test Acc: 84.325% (1673/1984)\n",
      "60 234 Test Loss: 0.534 | Test Acc: 84.887% (3314/3904)\n",
      "90 234 Test Loss: 0.554 | Test Acc: 84.427% (4917/5824)\n",
      "120 234 Test Loss: 0.562 | Test Acc: 84.272% (6526/7744)\n",
      "150 234 Test Loss: 0.567 | Test Acc: 84.209% (8138/9664)\n",
      "180 234 Test Loss: 0.563 | Test Acc: 84.142% (9747/11584)\n",
      "210 234 Test Loss: 0.562 | Test Acc: 84.182% (11368/13504)\n",
      "234 Epoch: 29 | Test Loss: 0.561 | Test Acc: 84.308% (12626/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 30\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 99.950% (1983/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 99.974% (3903/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 99.983% (5823/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 99.987% (7743/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 99.990% (9663/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 99.991% (11583/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 99.993% (13503/13504)\n",
      "234 Epoch: 30 | Train Loss: 0.002 | Train Acc: 99.993% (14975/14976)\n",
      "0 234 Test Loss: 0.671 | Test Acc: 82.812% (53/64)\n",
      "30 234 Test Loss: 0.583 | Test Acc: 83.417% (1655/1984)\n",
      "60 234 Test Loss: 0.590 | Test Acc: 83.453% (3258/3904)\n",
      "90 234 Test Loss: 0.573 | Test Acc: 83.705% (4875/5824)\n",
      "120 234 Test Loss: 0.563 | Test Acc: 83.962% (6502/7744)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 234 Test Loss: 0.567 | Test Acc: 84.168% (8134/9664)\n",
      "180 234 Test Loss: 0.567 | Test Acc: 84.107% (9743/11584)\n",
      "210 234 Test Loss: 0.566 | Test Acc: 84.094% (11356/13504)\n",
      "234 Epoch: 30 | Test Loss: 0.562 | Test Acc: 84.168% (12605/14976)\n",
      "\n",
      "Epoch: 31\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 31 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.742 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 0.556 | Test Acc: 84.375% (1674/1984)\n",
      "60 234 Test Loss: 0.563 | Test Acc: 83.965% (3278/3904)\n",
      "90 234 Test Loss: 0.556 | Test Acc: 84.203% (4904/5824)\n",
      "120 234 Test Loss: 0.571 | Test Acc: 83.962% (6502/7744)\n",
      "150 234 Test Loss: 0.570 | Test Acc: 83.982% (8116/9664)\n",
      "180 234 Test Loss: 0.560 | Test Acc: 84.237% (9758/11584)\n",
      "210 234 Test Loss: 0.556 | Test Acc: 84.219% (11373/13504)\n",
      "234 Epoch: 31 | Test Loss: 0.560 | Test Acc: 84.228% (12614/14976)\n",
      "\n",
      "Epoch: 32\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 32 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.658 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.582 | Test Acc: 83.115% (1649/1984)\n",
      "60 234 Test Loss: 0.579 | Test Acc: 83.274% (3251/3904)\n",
      "90 234 Test Loss: 0.574 | Test Acc: 83.534% (4865/5824)\n",
      "120 234 Test Loss: 0.582 | Test Acc: 83.290% (6450/7744)\n",
      "150 234 Test Loss: 0.576 | Test Acc: 83.506% (8070/9664)\n",
      "180 234 Test Loss: 0.573 | Test Acc: 83.728% (9699/11584)\n",
      "210 234 Test Loss: 0.563 | Test Acc: 83.945% (11336/13504)\n",
      "234 Epoch: 32 | Test Loss: 0.560 | Test Acc: 84.034% (12585/14976)\n",
      "\n",
      "Epoch: 33\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 33 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.636 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 0.579 | Test Acc: 83.821% (1663/1984)\n",
      "60 234 Test Loss: 0.554 | Test Acc: 84.298% (3291/3904)\n",
      "90 234 Test Loss: 0.568 | Test Acc: 83.723% (4876/5824)\n",
      "120 234 Test Loss: 0.563 | Test Acc: 83.897% (6497/7744)\n",
      "150 234 Test Loss: 0.567 | Test Acc: 83.868% (8105/9664)\n",
      "180 234 Test Loss: 0.565 | Test Acc: 83.883% (9717/11584)\n",
      "210 234 Test Loss: 0.558 | Test Acc: 84.020% (11346/13504)\n",
      "234 Epoch: 33 | Test Loss: 0.556 | Test Acc: 84.115% (12597/14976)\n",
      "\n",
      "Epoch: 34\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 34 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.386 | Test Acc: 90.625% (58/64)\n",
      "30 234 Test Loss: 0.576 | Test Acc: 83.871% (1664/1984)\n",
      "60 234 Test Loss: 0.535 | Test Acc: 84.964% (3317/3904)\n",
      "90 234 Test Loss: 0.525 | Test Acc: 85.130% (4958/5824)\n",
      "120 234 Test Loss: 0.535 | Test Acc: 84.879% (6573/7744)\n",
      "150 234 Test Loss: 0.541 | Test Acc: 84.572% (8173/9664)\n",
      "180 234 Test Loss: 0.550 | Test Acc: 84.306% (9766/11584)\n",
      "210 234 Test Loss: 0.555 | Test Acc: 84.131% (11361/13504)\n",
      "234 Epoch: 34 | Test Loss: 0.552 | Test Acc: 84.261% (12619/14976)\n",
      "\n",
      "Epoch: 35\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.002 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.002 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 35 | Train Loss: 0.002 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.659 | Test Acc: 84.375% (54/64)\n",
      "30 234 Test Loss: 0.514 | Test Acc: 84.980% (1686/1984)\n",
      "60 234 Test Loss: 0.537 | Test Acc: 84.708% (3307/3904)\n",
      "90 234 Test Loss: 0.537 | Test Acc: 84.753% (4936/5824)\n",
      "120 234 Test Loss: 0.536 | Test Acc: 84.698% (6559/7744)\n",
      "150 234 Test Loss: 0.535 | Test Acc: 84.789% (8194/9664)\n",
      "180 234 Test Loss: 0.538 | Test Acc: 84.781% (9821/11584)\n",
      "210 234 Test Loss: 0.541 | Test Acc: 84.568% (11420/13504)\n",
      "234 Epoch: 35 | Test Loss: 0.551 | Test Acc: 84.342% (12631/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 36\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.002 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.002 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 36 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.346 | Test Acc: 89.062% (57/64)\n",
      "30 234 Test Loss: 0.491 | Test Acc: 85.282% (1692/1984)\n",
      "60 234 Test Loss: 0.510 | Test Acc: 84.887% (3314/3904)\n",
      "90 234 Test Loss: 0.520 | Test Acc: 84.718% (4934/5824)\n",
      "120 234 Test Loss: 0.530 | Test Acc: 84.620% (6553/7744)\n",
      "150 234 Test Loss: 0.550 | Test Acc: 84.147% (8132/9664)\n",
      "180 234 Test Loss: 0.551 | Test Acc: 84.038% (9735/11584)\n",
      "210 234 Test Loss: 0.545 | Test Acc: 84.212% (11372/13504)\n",
      "234 Epoch: 36 | Test Loss: 0.550 | Test Acc: 84.075% (12591/14976)\n",
      "\n",
      "Epoch: 37\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.002 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 37 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.675 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.548 | Test Acc: 84.325% (1673/1984)\n",
      "60 234 Test Loss: 0.556 | Test Acc: 84.170% (3286/3904)\n",
      "90 234 Test Loss: 0.543 | Test Acc: 84.530% (4923/5824)\n",
      "120 234 Test Loss: 0.557 | Test Acc: 84.104% (6513/7744)\n",
      "150 234 Test Loss: 0.556 | Test Acc: 84.106% (8128/9664)\n",
      "180 234 Test Loss: 0.567 | Test Acc: 83.909% (9720/11584)\n",
      "210 234 Test Loss: 0.553 | Test Acc: 84.175% (11367/13504)\n",
      "234 Epoch: 37 | Test Loss: 0.559 | Test Acc: 84.034% (12585/14976)\n",
      "\n",
      "Epoch: 38\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.002 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.002 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234 Epoch: 38 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.702 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.548 | Test Acc: 83.770% (1662/1984)\n",
      "60 234 Test Loss: 0.550 | Test Acc: 83.735% (3269/3904)\n",
      "90 234 Test Loss: 0.576 | Test Acc: 83.310% (4852/5824)\n",
      "120 234 Test Loss: 0.570 | Test Acc: 83.510% (6467/7744)\n",
      "150 234 Test Loss: 0.567 | Test Acc: 83.640% (8083/9664)\n",
      "180 234 Test Loss: 0.555 | Test Acc: 83.952% (9725/11584)\n",
      "210 234 Test Loss: 0.553 | Test Acc: 83.990% (11342/13504)\n",
      "234 Epoch: 38 | Test Loss: 0.551 | Test Acc: 84.115% (12597/14976)\n",
      "\n",
      "Epoch: 39\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.003 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 39 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.384 | Test Acc: 89.062% (57/64)\n",
      "30 234 Test Loss: 0.555 | Test Acc: 83.972% (1666/1984)\n",
      "60 234 Test Loss: 0.567 | Test Acc: 84.119% (3284/3904)\n",
      "90 234 Test Loss: 0.557 | Test Acc: 84.358% (4913/5824)\n",
      "120 234 Test Loss: 0.559 | Test Acc: 84.336% (6531/7744)\n",
      "150 234 Test Loss: 0.547 | Test Acc: 84.468% (8163/9664)\n",
      "180 234 Test Loss: 0.544 | Test Acc: 84.418% (9779/11584)\n",
      "210 234 Test Loss: 0.546 | Test Acc: 84.331% (11388/13504)\n",
      "234 Epoch: 39 | Test Loss: 0.550 | Test Acc: 84.241% (12616/14976)\n",
      "\n",
      "Epoch: 40\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.003 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 40 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.473 | Test Acc: 81.250% (52/64)\n",
      "30 234 Test Loss: 0.557 | Test Acc: 84.123% (1669/1984)\n",
      "60 234 Test Loss: 0.557 | Test Acc: 84.401% (3295/3904)\n",
      "90 234 Test Loss: 0.559 | Test Acc: 84.375% (4914/5824)\n",
      "120 234 Test Loss: 0.566 | Test Acc: 84.065% (6510/7744)\n",
      "150 234 Test Loss: 0.559 | Test Acc: 84.240% (8141/9664)\n",
      "180 234 Test Loss: 0.558 | Test Acc: 84.202% (9754/11584)\n",
      "210 234 Test Loss: 0.556 | Test Acc: 84.331% (11388/13504)\n",
      "234 Epoch: 40 | Test Loss: 0.556 | Test Acc: 84.355% (12633/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 41\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.003 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 41 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.577 | Test Acc: 84.375% (54/64)\n",
      "30 234 Test Loss: 0.552 | Test Acc: 84.627% (1679/1984)\n",
      "60 234 Test Loss: 0.547 | Test Acc: 84.759% (3309/3904)\n",
      "90 234 Test Loss: 0.557 | Test Acc: 84.272% (4908/5824)\n",
      "120 234 Test Loss: 0.555 | Test Acc: 84.259% (6525/7744)\n",
      "150 234 Test Loss: 0.558 | Test Acc: 84.168% (8134/9664)\n",
      "180 234 Test Loss: 0.557 | Test Acc: 84.151% (9748/11584)\n",
      "210 234 Test Loss: 0.557 | Test Acc: 84.212% (11372/13504)\n",
      "234 Epoch: 41 | Test Loss: 0.551 | Test Acc: 84.342% (12631/14976)\n",
      "\n",
      "Epoch: 42\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.003 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 42 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.377 | Test Acc: 89.062% (57/64)\n",
      "30 234 Test Loss: 0.544 | Test Acc: 84.325% (1673/1984)\n",
      "60 234 Test Loss: 0.541 | Test Acc: 84.068% (3282/3904)\n",
      "90 234 Test Loss: 0.553 | Test Acc: 83.877% (4885/5824)\n",
      "120 234 Test Loss: 0.548 | Test Acc: 84.181% (6519/7744)\n",
      "150 234 Test Loss: 0.536 | Test Acc: 84.478% (8164/9664)\n",
      "180 234 Test Loss: 0.537 | Test Acc: 84.444% (9782/11584)\n",
      "210 234 Test Loss: 0.543 | Test Acc: 84.368% (11393/13504)\n",
      "234 Epoch: 42 | Test Loss: 0.548 | Test Acc: 84.288% (12623/14976)\n",
      "\n",
      "Epoch: 43\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.003 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 43 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.624 | Test Acc: 79.688% (51/64)\n",
      "30 234 Test Loss: 0.596 | Test Acc: 83.871% (1664/1984)\n",
      "60 234 Test Loss: 0.565 | Test Acc: 84.580% (3302/3904)\n",
      "90 234 Test Loss: 0.551 | Test Acc: 84.684% (4932/5824)\n",
      "120 234 Test Loss: 0.551 | Test Acc: 84.401% (6536/7744)\n",
      "150 234 Test Loss: 0.559 | Test Acc: 84.292% (8146/9664)\n",
      "180 234 Test Loss: 0.556 | Test Acc: 84.332% (9769/11584)\n",
      "210 234 Test Loss: 0.550 | Test Acc: 84.338% (11389/13504)\n",
      "234 Epoch: 43 | Test Loss: 0.548 | Test Acc: 84.395% (12639/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 44\n",
      "0 234 Train Loss: 0.008 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.003 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 44 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.463 | Test Acc: 85.938% (55/64)\n",
      "30 234 Test Loss: 0.574 | Test Acc: 83.569% (1658/1984)\n",
      "60 234 Test Loss: 0.569 | Test Acc: 83.760% (3270/3904)\n",
      "90 234 Test Loss: 0.554 | Test Acc: 83.997% (4892/5824)\n",
      "120 234 Test Loss: 0.548 | Test Acc: 84.155% (6517/7744)\n",
      "150 234 Test Loss: 0.554 | Test Acc: 84.044% (8122/9664)\n",
      "180 234 Test Loss: 0.564 | Test Acc: 83.926% (9722/11584)\n",
      "210 234 Test Loss: 0.559 | Test Acc: 84.064% (11352/13504)\n",
      "234 Epoch: 44 | Test Loss: 0.557 | Test Acc: 84.095% (12594/14976)\n",
      "\n",
      "Epoch: 45\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.003 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 45 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.503 | Test Acc: 82.812% (53/64)\n",
      "30 234 Test Loss: 0.521 | Test Acc: 85.433% (1695/1984)\n",
      "60 234 Test Loss: 0.547 | Test Acc: 84.477% (3298/3904)\n",
      "90 234 Test Loss: 0.553 | Test Acc: 84.375% (4914/5824)\n",
      "120 234 Test Loss: 0.548 | Test Acc: 84.349% (6532/7744)\n",
      "150 234 Test Loss: 0.549 | Test Acc: 84.427% (8159/9664)\n",
      "180 234 Test Loss: 0.543 | Test Acc: 84.643% (9805/11584)\n",
      "210 234 Test Loss: 0.546 | Test Acc: 84.627% (11428/13504)\n",
      "234 Epoch: 45 | Test Loss: 0.552 | Test Acc: 84.468% (12650/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 46\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 234 Train Loss: 0.003 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 46 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.517 | Test Acc: 82.812% (53/64)\n",
      "30 234 Test Loss: 0.538 | Test Acc: 84.325% (1673/1984)\n",
      "60 234 Test Loss: 0.546 | Test Acc: 84.503% (3299/3904)\n",
      "90 234 Test Loss: 0.543 | Test Acc: 84.530% (4923/5824)\n",
      "120 234 Test Loss: 0.550 | Test Acc: 84.569% (6549/7744)\n",
      "150 234 Test Loss: 0.546 | Test Acc: 84.634% (8179/9664)\n",
      "180 234 Test Loss: 0.544 | Test Acc: 84.712% (9813/11584)\n",
      "210 234 Test Loss: 0.547 | Test Acc: 84.582% (11422/13504)\n",
      "234 Epoch: 46 | Test Loss: 0.553 | Test Acc: 84.422% (12643/14976)\n",
      "\n",
      "Epoch: 47\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.003 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 47 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.345 | Test Acc: 87.500% (56/64)\n",
      "30 234 Test Loss: 0.559 | Test Acc: 84.274% (1672/1984)\n",
      "60 234 Test Loss: 0.541 | Test Acc: 84.836% (3312/3904)\n",
      "90 234 Test Loss: 0.553 | Test Acc: 84.530% (4923/5824)\n",
      "120 234 Test Loss: 0.561 | Test Acc: 84.181% (6519/7744)\n",
      "150 234 Test Loss: 0.572 | Test Acc: 83.889% (8107/9664)\n",
      "180 234 Test Loss: 0.567 | Test Acc: 83.969% (9727/11584)\n",
      "210 234 Test Loss: 0.562 | Test Acc: 84.160% (11365/13504)\n",
      "234 Epoch: 47 | Test Loss: 0.555 | Test Acc: 84.362% (12634/14976)\n",
      "\n",
      "Epoch: 48\n",
      "0 234 Train Loss: 0.003 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.003 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 48 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.551 | Test Acc: 85.938% (55/64)\n",
      "30 234 Test Loss: 0.549 | Test Acc: 84.627% (1679/1984)\n",
      "60 234 Test Loss: 0.558 | Test Acc: 84.401% (3295/3904)\n",
      "90 234 Test Loss: 0.542 | Test Acc: 84.770% (4937/5824)\n",
      "120 234 Test Loss: 0.546 | Test Acc: 84.724% (6561/7744)\n",
      "150 234 Test Loss: 0.556 | Test Acc: 84.303% (8147/9664)\n",
      "180 234 Test Loss: 0.563 | Test Acc: 84.081% (9740/11584)\n",
      "210 234 Test Loss: 0.553 | Test Acc: 84.419% (11400/13504)\n",
      "234 Epoch: 48 | Test Loss: 0.551 | Test Acc: 84.428% (12644/14976)\n",
      "\n",
      "Epoch: 49\n",
      "0 234 Train Loss: 0.002 | Train Acc: 100.000% (64/64)\n",
      "30 234 Train Loss: 0.003 | Train Acc: 100.000% (1984/1984)\n",
      "60 234 Train Loss: 0.003 | Train Acc: 100.000% (3904/3904)\n",
      "90 234 Train Loss: 0.003 | Train Acc: 100.000% (5824/5824)\n",
      "120 234 Train Loss: 0.003 | Train Acc: 100.000% (7744/7744)\n",
      "150 234 Train Loss: 0.003 | Train Acc: 100.000% (9664/9664)\n",
      "180 234 Train Loss: 0.003 | Train Acc: 100.000% (11584/11584)\n",
      "210 234 Train Loss: 0.003 | Train Acc: 100.000% (13504/13504)\n",
      "234 Epoch: 49 | Train Loss: 0.003 | Train Acc: 100.000% (14976/14976)\n",
      "0 234 Test Loss: 0.358 | Test Acc: 87.500% (56/64)\n",
      "30 234 Test Loss: 0.520 | Test Acc: 85.685% (1700/1984)\n",
      "60 234 Test Loss: 0.536 | Test Acc: 84.734% (3308/3904)\n",
      "90 234 Test Loss: 0.538 | Test Acc: 84.547% (4924/5824)\n",
      "120 234 Test Loss: 0.551 | Test Acc: 84.491% (6543/7744)\n",
      "150 234 Test Loss: 0.560 | Test Acc: 84.344% (8151/9664)\n",
      "180 234 Test Loss: 0.556 | Test Acc: 84.487% (9787/11584)\n",
      "210 234 Test Loss: 0.554 | Test Acc: 84.471% (11407/13504)\n",
      "234 Epoch: 49 | Test Loss: 0.553 | Test Acc: 84.468% (12650/14976)\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.2.3: Start Shadow model training\n",
    "\n",
    "max_epoch = 50  #@param {type:\"integer\"}\n",
    "train_result_summary = 'shadow_train_DCA-BiLSTM.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(shadow_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(shadow_testloader, epoch, batch_size=batch_size, logfile = train_result_summary, save_modelpath = save_model_folder)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34726edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_training_summary(filepath = 'shadow_train_DCA-BiLSTM.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b47abae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model for DLA-LSTM\n",
      "==> Training model from scratch..\n",
      "Total trained parameters:  22596154\n"
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DLA-LSTM'  #@param {type:\"string\"}\n",
    "save_model_folder = './Target-DLA-LSTM_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DLA-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b80fac24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 234 Train Loss: 2.297 | Train Acc: 15.625% (10/64)\n",
      "30 234 Train Loss: 2.282 | Train Acc: 13.558% (269/1984)\n",
      "60 234 Train Loss: 2.234 | Train Acc: 15.113% (590/3904)\n",
      "90 234 Train Loss: 2.187 | Train Acc: 16.449% (958/5824)\n",
      "120 234 Train Loss: 2.139 | Train Acc: 17.911% (1387/7744)\n",
      "150 234 Train Loss: 2.098 | Train Acc: 19.029% (1839/9664)\n",
      "180 234 Train Loss: 2.069 | Train Acc: 19.881% (2303/11584)\n",
      "210 234 Train Loss: 2.039 | Train Acc: 20.786% (2807/13504)\n",
      "234 Epoch: 0 | Train Loss: 2.016 | Train Acc: 21.608% (3236/14976)\n",
      "0 234 Test Loss: 2.001 | Test Acc: 26.562% (17/64)\n",
      "30 234 Test Loss: 1.880 | Test Acc: 25.756% (511/1984)\n",
      "60 234 Test Loss: 1.886 | Test Acc: 25.538% (997/3904)\n",
      "90 234 Test Loss: 1.890 | Test Acc: 25.326% (1475/5824)\n",
      "120 234 Test Loss: 1.889 | Test Acc: 25.968% (2011/7744)\n",
      "150 234 Test Loss: 1.886 | Test Acc: 26.231% (2535/9664)\n",
      "180 234 Test Loss: 1.886 | Test Acc: 26.321% (3049/11584)\n",
      "210 234 Test Loss: 1.888 | Test Acc: 26.148% (3531/13504)\n",
      "234 Epoch: 0 | Test Loss: 1.888 | Test Acc: 25.928% (3883/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "0 234 Train Loss: 1.710 | Train Acc: 28.125% (18/64)\n",
      "30 234 Train Loss: 1.731 | Train Acc: 32.157% (638/1984)\n",
      "60 234 Train Loss: 1.746 | Train Acc: 32.070% (1252/3904)\n",
      "90 234 Train Loss: 1.748 | Train Acc: 32.091% (1869/5824)\n",
      "120 234 Train Loss: 1.733 | Train Acc: 32.503% (2517/7744)\n",
      "150 234 Train Loss: 1.715 | Train Acc: 33.382% (3226/9664)\n",
      "180 234 Train Loss: 1.697 | Train Acc: 34.142% (3955/11584)\n",
      "210 234 Train Loss: 1.680 | Train Acc: 34.960% (4721/13504)\n",
      "234 Epoch: 1 | Train Loss: 1.674 | Train Acc: 35.377% (5298/14976)\n",
      "0 234 Test Loss: 1.644 | Test Acc: 40.625% (26/64)\n",
      "30 234 Test Loss: 1.685 | Test Acc: 35.181% (698/1984)\n",
      "60 234 Test Loss: 1.684 | Test Acc: 35.605% (1390/3904)\n",
      "90 234 Test Loss: 1.684 | Test Acc: 35.491% (2067/5824)\n",
      "120 234 Test Loss: 1.685 | Test Acc: 35.511% (2750/7744)\n",
      "150 234 Test Loss: 1.683 | Test Acc: 35.348% (3416/9664)\n",
      "180 234 Test Loss: 1.685 | Test Acc: 35.152% (4072/11584)\n",
      "210 234 Test Loss: 1.685 | Test Acc: 35.041% (4732/13504)\n",
      "234 Epoch: 1 | Test Loss: 1.686 | Test Acc: 35.043% (5248/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "0 234 Train Loss: 1.594 | Train Acc: 40.625% (26/64)\n",
      "30 234 Train Loss: 1.464 | Train Acc: 44.607% (885/1984)\n",
      "60 234 Train Loss: 1.478 | Train Acc: 44.314% (1730/3904)\n",
      "90 234 Train Loss: 1.468 | Train Acc: 45.106% (2627/5824)\n",
      "120 234 Train Loss: 1.452 | Train Acc: 46.010% (3563/7744)\n",
      "150 234 Train Loss: 1.441 | Train Acc: 46.440% (4488/9664)\n",
      "180 234 Train Loss: 1.423 | Train Acc: 47.324% (5482/11584)\n",
      "210 234 Train Loss: 1.403 | Train Acc: 48.164% (6504/13504)\n",
      "234 Epoch: 2 | Train Loss: 1.392 | Train Acc: 48.598% (7278/14976)\n",
      "0 234 Test Loss: 1.358 | Test Acc: 50.000% (32/64)\n",
      "30 234 Test Loss: 1.485 | Test Acc: 46.069% (914/1984)\n",
      "60 234 Test Loss: 1.489 | Test Acc: 46.849% (1829/3904)\n",
      "90 234 Test Loss: 1.487 | Test Acc: 46.755% (2723/5824)\n",
      "120 234 Test Loss: 1.495 | Test Acc: 46.152% (3574/7744)\n",
      "150 234 Test Loss: 1.490 | Test Acc: 46.554% (4499/9664)\n",
      "180 234 Test Loss: 1.486 | Test Acc: 46.564% (5394/11584)\n",
      "210 234 Test Loss: 1.487 | Test Acc: 46.527% (6283/13504)\n",
      "234 Epoch: 2 | Test Loss: 1.488 | Test Acc: 46.588% (6977/14976)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "0 234 Train Loss: 1.154 | Train Acc: 50.000% (32/64)\n",
      "30 234 Train Loss: 1.202 | Train Acc: 56.653% (1124/1984)\n",
      "60 234 Train Loss: 1.208 | Train Acc: 56.378% (2201/3904)\n",
      "90 234 Train Loss: 1.181 | Train Acc: 56.988% (3319/5824)\n",
      "120 234 Train Loss: 1.154 | Train Acc: 58.006% (4492/7744)\n",
      "150 234 Train Loss: 1.168 | Train Acc: 57.626% (5569/9664)\n",
      "180 234 Train Loss: 1.159 | Train Acc: 58.097% (6730/11584)\n",
      "210 234 Train Loss: 1.151 | Train Acc: 58.412% (7888/13504)\n",
      "234 Epoch: 3 | Train Loss: 1.141 | Train Acc: 58.707% (8792/14976)\n",
      "0 234 Test Loss: 1.576 | Test Acc: 45.312% (29/64)\n",
      "30 234 Test Loss: 1.374 | Test Acc: 51.210% (1016/1984)\n",
      "60 234 Test Loss: 1.398 | Test Acc: 51.588% (2014/3904)\n",
      "90 234 Test Loss: 1.390 | Test Acc: 52.095% (3034/5824)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, start_epoch\u001b[38;5;241m+\u001b[39mmax_epoch):\n\u001b[0;32m     16\u001b[0m     train(target_trainloader, epoch, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, logfile \u001b[38;5;241m=\u001b[39m train_result_summary)\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_testloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_result_summary\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 49\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(testloader, epoch, batch_size, logfile, save_modelpath)\u001b[0m\n\u001b[0;32m     44\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(testloader):\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;66;03m#inputs, targets = inputs.to(device), targets.to(device)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m         inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcuda(), targets\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     52\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:517\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 517\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    520\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    556\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    559\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:330\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m--> 330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:330\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m--> 330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:219\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\cifar.py:120\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    117\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:60\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 60\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:97\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\functional.py:142\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    140\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title (Run) Part 5.2.3: Start Target model training\n",
    "\n",
    "max_epoch = 30  #@param {type:\"integer\"}\n",
    "train_result_summary = 'target_train_DCA-LSTM.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(target_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(target_testloader, epoch, batch_size=batch_size, logfile = train_result_summary, save_modelpath = save_model_folder)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fecd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_training_summary(filepath = 'target_train_DCA-LSTM.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7576e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DLA-LSTM'  #@param {type:\"string\"}\n",
    "save_model_folder = './Shadow-DLA-LSTM_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DLA-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.2.3: Start Shadow model training\n",
    "\n",
    "max_epoch = 50  #@param {type:\"integer\"}\n",
    "train_result_summary = 'shadow_train_DCA-LSTM.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(shadow_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(shadow_testloader, epoch, batch_size=batch_size, logfile = train_result_summary, save_modelpath = save_model_folder)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_training_summary(filepath = 'shadow_train_DCA-LSTM.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8daac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DLA'  #@param {type:\"string\"}\n",
    "save_model_folder = './Target-DLA_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DLA-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3504245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.2.3: Start Target model training\n",
    "\n",
    "max_epoch = 50  #@param {type:\"integer\"}\n",
    "train_result_summary = 'target_train_DCA.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(target_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(target_testloader, epoch, batch_size=batch_size, logfile = train_result_summary, save_modelpath = save_model_folder)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c765fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_training_summary(filepath = 'target_train_DCA.summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b220fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.1.1: Select an architecture for Target model training\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "#@markdown Enter the folder of path to the pdb files:\n",
    "#@markdown * Option 1: ReNet\n",
    "#@markdown * Option 2: DLA\n",
    "#@markdown * Option 3: DLA-LSTM\n",
    "#@markdown * Option 4: DLA-BiLSTM\n",
    "\n",
    "\n",
    "method_name = 'DLA'  #@param {type:\"string\"}\n",
    "save_model_folder = './Shadow-DLA_models/'   #@param {type:\"string\"}\n",
    "batch_size = 64  #@param {type:\"integer\"}\n",
    "load_pretrain_weight = False   #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "print('==> Building model for ' + method_name)\n",
    "if method_name == 'DLA-BiLSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='Bi-LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA-LSTM':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='LSTM')\n",
    "  net.cuda()\n",
    "elif method_name == 'DLA':\n",
    "  # Model\n",
    "  \n",
    "  net = DLA(num_classes=10, enable_RNN='None')\n",
    "  net.cuda()\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "if load_pretrain_weight:\n",
    "  try:\n",
    "      # Load checkpoint.\n",
    "      print('==> Resuming from checkpoint..')\n",
    "      checkpoint = torch.load(save_model_folder+'/ckpt.pth')\n",
    "      net.load_state_dict(checkpoint['net'])\n",
    "      best_acc = checkpoint['acc']\n",
    "      start_epoch = checkpoint['epoch']\n",
    "  except:\n",
    "      print('!!! Error: no checkpoint directory found!')\n",
    "else:\n",
    "  print('==> Training model from scratch..')\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total trained parameters: \",pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1954c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Run) Part 5.2.3: Start Shadow model training\n",
    "\n",
    "max_epoch = 50  #@param {type:\"integer\"}\n",
    "train_result_summary = 'shadow_train_DCA.summary'   #@param {type:\"string\"}\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "\n",
    "if not load_pretrain_weight:\n",
    "    f = open(train_result_summary, \"w\")\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epoch):\n",
    "    train(shadow_trainloader, epoch, batch_size=batch_size, logfile = train_result_summary)\n",
    "    test(shadow_testloader, epoch, batch_size=batch_size, logfile = train_result_summary, save_modelpath = save_model_folder)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0050b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_training_summary(filepath = 'shadow_train_DCA.summary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
